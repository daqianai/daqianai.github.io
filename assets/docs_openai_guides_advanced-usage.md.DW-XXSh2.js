import{_ as t,c as o,o as a,a2 as r}from"./chunks/framework.DDIT__tB.js";const u=JSON.parse('{"title":"Advanced usage","description":"","frontmatter":{},"headers":[],"relativePath":"docs/openai/guides/advanced-usage.md","filePath":"docs/openai/guides/advanced-usage.md","lastUpdated":1747747486000}'),s={name:"docs/openai/guides/advanced-usage.md"};function n(i,e,l,c,p,d){return a(),o("div",null,e[0]||(e[0]=[r('<h1 id="advanced-usage" tabindex="-1">Advanced usage <a class="header-anchor" href="#advanced-usage" aria-label="Permalink to &quot;Advanced usage&quot;">​</a></h1><p>Use advanced techniques for reproducibility and parameter tuning.</p><p>OpenAI&#39;s text generation models (often called generative pre-trained transformers or large language models) have been trained to understand natural language, code, and images. The models provide text outputs in response to their inputs. The text inputs to these models are also referred to as &quot;prompts&quot;. Designing a prompt is essentially how you “program” a large language model model, usually by providing instructions or some examples of how to successfully complete a task.</p><h2 id="reproducible-outputs" tabindex="-1">Reproducible outputs <a class="header-anchor" href="#reproducible-outputs" aria-label="Permalink to &quot;Reproducible outputs&quot;">​</a></h2><p>Chat Completions are non-deterministic by default (which means model outputs may differ from request to request). That being said, we offer some control towards deterministic outputs by giving you access to the <a href="/docs/api-reference/chat/create#chat-create-seed">seed</a> parameter and the <a href="/docs/api-reference/completions/object#completions/object-system_fingerprint">system_fingerprint</a> response field.</p><p>To receive (mostly) deterministic outputs across API calls, you can:</p><ul><li>Set the <a href="/docs/api-reference/chat/create#chat-create-seed">seed</a> parameter to any integer of your choice and use the same value across requests you&#39;d like deterministic outputs for.</li><li>Ensure all other parameters (like <code>prompt</code> or <code>temperature</code>) are the exact same across requests.</li></ul><p>Sometimes, determinism may be impacted due to necessary changes OpenAI makes to model configurations on our end. To help you keep track of these changes, we expose the <a href="/docs/api-reference/chat/object#chat/object-system_fingerprint">system_fingerprint</a> field. If this value is different, you may see different outputs due to changes we&#39;ve made on our systems.</p><p>[</p><p>Deterministic outputs</p><p>Explore the new seed parameter in the OpenAI cookbook</p><p>](<a href="https://cookbook.openai.com/examples/reproducible_outputs_with_the_seed_parameter" target="_blank" rel="noreferrer">https://cookbook.openai.com/examples/reproducible_outputs_with_the_seed_parameter</a>)</p><h2 id="managing-tokens" tabindex="-1">Managing tokens <a class="header-anchor" href="#managing-tokens" aria-label="Permalink to &quot;Managing tokens&quot;">​</a></h2><p>Language models read and write text in chunks called tokens. In English, a token can be as short as one character or as long as one word (e.g., <code>a</code> or <code>apple</code>), and in some languages tokens can be even shorter than one character or even longer than one word.</p><p>As a rough rule of thumb, 1 token is approximately 4 characters or 0.75 words for English text.</p><p>Check out our <a href="https://platform.openai.com/tokenizer" target="_blank" rel="noreferrer">Tokenizer tool</a> to test specific strings and see how they are translated into tokens.</p><p>For example, the string <code>&quot;ChatGPT is great!&quot;</code> is encoded into six tokens: <code>[&quot;Chat&quot;, &quot;G&quot;, &quot;PT&quot;, &quot; is&quot;, &quot; great&quot;, &quot;!&quot;]</code>.</p><p>The total number of tokens in an API call affects:</p><ul><li>How much your API call costs, as you pay per token</li><li>How long your API call takes, as writing more tokens takes more time</li><li>Whether your API call works at all, as total tokens must be below the model&#39;s maximum limit (4097 tokens for <code>gpt-3.5-turbo</code>)</li></ul><p>Both input and output tokens count toward these quantities. For example, if your API call used 10 tokens in the message input and you received 20 tokens in the message output, you would be billed for 30 tokens. Note however that for some models the price per token is different for tokens in the input vs. the output (see the <a href="https://openai.com/api/pricing" target="_blank" rel="noreferrer">pricing</a> page for more information).</p><p>To see how many tokens are used by an API call, check the <code>usage</code> field in the API response (e.g., <code>response[&#39;usage&#39;][&#39;total_tokens&#39;]</code>).</p><p>Chat models like <code>gpt-3.5-turbo</code> and <code>gpt-4-turbo-preview</code> use tokens in the same way as the models available in the completions API, but because of their message-based formatting, it&#39;s more difficult to count how many tokens will be used by a conversation.</p><p>Deep dive</p><p>Counting tokens for chat API calls</p><p>To see how many tokens are in a text string without making an API call, use OpenAI’s <a href="https://github.com/openai/tiktoken" target="_blank" rel="noreferrer">tiktoken</a> Python library. Example code can be found in the OpenAI Cookbook’s guide on <a href="https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken" target="_blank" rel="noreferrer">how to count tokens with tiktoken</a>.</p><p>Each message passed to the API consumes the number of tokens in the content, role, and other fields, plus a few extra for behind-the-scenes formatting. This may change slightly in the future.</p><p>If a conversation has too many tokens to fit within a model’s maximum limit (e.g., more than 4097 tokens for <code>gpt-3.5-turbo</code> or more than 128k tokens for <code>gpt-4o</code>), you will have to truncate, omit, or otherwise shrink your text until it fits. Beware that if a message is removed from the messages input, the model will lose all knowledge of it.</p><p>Note that very long conversations are more likely to receive incomplete replies. For example, a <code>gpt-3.5-turbo</code> conversation that is 4090 tokens long will have its reply cut off after just 6 tokens.</p><h2 id="parameter-details" tabindex="-1">Parameter details <a class="header-anchor" href="#parameter-details" aria-label="Permalink to &quot;Parameter details&quot;">​</a></h2><h3 id="frequency-and-presence-penalties" tabindex="-1">Frequency and presence penalties <a class="header-anchor" href="#frequency-and-presence-penalties" aria-label="Permalink to &quot;Frequency and presence penalties&quot;">​</a></h3><p>The frequency and presence penalties found in the <a href="/docs/api-reference/chat/create">Chat Completions API</a> and <a href="/docs/api-reference/completions">Legacy Completions API</a> can be used to reduce the likelihood of sampling repetitive sequences of tokens.</p><p>Deep dive</p><p>Penalties behind the scenes</p><p>Reasonable values for the penalty coefficients are around 0.1 to 1 if the aim is to just reduce repetitive samples somewhat. If the aim is to strongly suppress repetition, then one can increase the coefficients up to 2, but this can noticeably degrade the quality of samples. Negative values can be used to increase the likelihood of repetition.</p><h3 id="token-log-probabilities" tabindex="-1">Token log probabilities <a class="header-anchor" href="#token-log-probabilities" aria-label="Permalink to &quot;Token log probabilities&quot;">​</a></h3><p>The <a href="/docs/api-reference/chat/create#chat-create-logprobs">logprobs</a> parameter found in the <a href="/docs/api-reference/chat/create">Chat Completions API</a> and <a href="/docs/api-reference/completions">Legacy Completions API</a>, when requested, provides the log probabilities of each output token, and a limited number of the most likely tokens at each token position alongside their log probabilities. This can be useful in some cases to assess the confidence of the model in its output, or to examine alternative responses the model might have given.</p><h3 id="other-parameters" tabindex="-1">Other parameters <a class="header-anchor" href="#other-parameters" aria-label="Permalink to &quot;Other parameters&quot;">​</a></h3><p>See the full <a href="https://platform.openai.com/docs/api-reference/chat" target="_blank" rel="noreferrer">API reference documentation</a> to learn more.</p>',38)]))}const m=t(s,[["render",n]]);export{u as __pageData,m as default};
