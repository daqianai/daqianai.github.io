import{_ as i,c as a,o as t,ag as e}from"./chunks/framework.BDwTZuFy.js";const c=JSON.parse('{"title":"Rate limits","description":"","frontmatter":{},"headers":[],"relativePath":"docs/openai/guides/rate-limits.md","filePath":"docs/openai/guides/rate-limits.md","lastUpdated":1748071461000}'),n={name:"docs/openai/guides/rate-limits.md"};function l(h,s,r,p,o,k){return t(),a("div",null,s[0]||(s[0]=[e(`<h1 id="rate-limits" tabindex="-1">Rate limits <a class="header-anchor" href="#rate-limits" aria-label="Permalink to &quot;Rate limits&quot;">​</a></h1><p>Understand API rate limits and restrictions.</p><p>Rate limits are restrictions that our API imposes on the number of times a user or client can access our services within a specified period of time.</p><h2 id="why-do-we-have-rate-limits" tabindex="-1">Why do we have rate limits? <a class="header-anchor" href="#why-do-we-have-rate-limits" aria-label="Permalink to &quot;Why do we have rate limits?&quot;">​</a></h2><p>Rate limits are a common practice for APIs, and they&#39;re put in place for a few different reasons:</p><ul><li><strong>They help protect against abuse or misuse of the API.</strong> For example, a malicious actor could flood the API with requests in an attempt to overload it or cause disruptions in service. By setting rate limits, OpenAI can prevent this kind of activity.</li><li><strong>Rate limits help ensure that everyone has fair access to the API.</strong> If one person or organization makes an excessive number of requests, it could bog down the API for everyone else. By throttling the number of requests that a single user can make, OpenAI ensures that the most number of people have an opportunity to use the API without experiencing slowdowns.</li><li><strong>Rate limits can help OpenAI manage the aggregate load on its infrastructure.</strong> If requests to the API increase dramatically, it could tax the servers and cause performance issues. By setting rate limits, OpenAI can help maintain a smooth and consistent experience for all users.</li></ul><p>Please work through this document in its entirety to better understand how OpenAI’s rate limit system works. We include code examples and possible solutions to handle common issues. We also include details around how your rate limits are automatically increased in the usage tiers section below.</p><h2 id="how-do-these-rate-limits-work" tabindex="-1">How do these rate limits work? <a class="header-anchor" href="#how-do-these-rate-limits-work" aria-label="Permalink to &quot;How do these rate limits work?&quot;">​</a></h2><p>Rate limits are measured in five ways: <strong>RPM</strong> (requests per minute), <strong>RPD</strong> (requests per day), <strong>TPM</strong> (tokens per minute), <strong>TPD</strong> (tokens per day), and <strong>IPM</strong> (images per minute). Rate limits can be hit across any of the options depending on what occurs first. For example, you might send 20 requests with only 100 tokens to the ChatCompletions endpoint and that would fill your limit (if your RPM was 20), even if you did not send 150k tokens (if your TPM limit was 150k) within those 20 requests.</p><p><a href="/docs/api-reference/batch/create.html">Batch API</a> queue limits are calculated based on the total number of input tokens queued for a given model. Tokens from pending batch jobs are counted against your queue limit. Once a batch job is completed, its tokens are no longer counted against that model&#39;s limit.</p><p>Other important things worth noting:</p><ul><li>Rate limits are defined at the <a href="/docs/openai/guides/production-best-practices.html">organization level</a> and at the project level, not user level.</li><li>Rate limits vary by the <a href="/docs/models.html">model</a> being used.</li><li>For long context models like GPT-4.1, there is a separate rate limit for long context requests. You can view these rate limits in <a href="/settings/organization/limits.html">developer console</a>.</li><li>Limits are also placed on the total amount an organization can spend on the API each month. These are also known as &quot;usage limits&quot;.</li><li>Some model families have shared rate limits. Any models listed under a &quot;shared limit&quot; in your <a href="https://platform.openai.com/settings/organization/limits" target="_blank" rel="noreferrer">organizations limit page</a> share a rate limit between them. For example, if the listed shared TPM is 3.5M, all calls to any model in the given &quot;shared limit&quot; list will count towards that 3.5M.</li></ul><h2 id="usage-tiers" tabindex="-1">Usage tiers <a class="header-anchor" href="#usage-tiers" aria-label="Permalink to &quot;Usage tiers&quot;">​</a></h2><p>You can view the rate and usage limits for your organization under the <a href="/settings/organization/limits.html">limits</a> section of your account settings. As your spend on our API goes up, we automatically graduate you to the next usage tier. This usually results in an increase in rate limits across most models.</p><table tabindex="0"><thead><tr><th>Tier</th><th>Qualification</th><th>Usage limits</th></tr></thead><tbody><tr><td>Free</td><td>User must be in an allowed geography</td><td>$100 / month</td></tr><tr><td>Tier 1</td><td>$5 paid</td><td>$100 / month</td></tr><tr><td>Tier 2</td><td>$50 paid and 7+ days since first successful payment</td><td>$500 / month</td></tr><tr><td>Tier 3</td><td>$100 paid and 7+ days since first successful payment</td><td>$1,000 / month</td></tr><tr><td>Tier 4</td><td>$250 paid and 14+ days since first successful payment</td><td>$5,000 / month</td></tr><tr><td>Tier 5</td><td>$1,000 paid and 30+ days since first successful payment</td><td>$200,000 / month</td></tr></tbody></table><p>To view a high-level summary of rate limits per model, visit the <a href="/docs/models.html">models page</a>.</p><h3 id="rate-limits-in-headers" tabindex="-1">Rate limits in headers <a class="header-anchor" href="#rate-limits-in-headers" aria-label="Permalink to &quot;Rate limits in headers&quot;">​</a></h3><p>In addition to seeing your rate limit on your <a href="/settings/organization/limits.html">account page</a>, you can also view important information about your rate limits such as the remaining requests, tokens, and other metadata in the headers of the HTTP response.</p><p>You can expect to see the following header fields:</p><table tabindex="0"><thead><tr><th>Field</th><th>Sample Value</th><th>Description</th></tr></thead><tbody><tr><td>x-ratelimit-limit-requests</td><td>60</td><td>The maximum number of requests that are permitted before exhausting the rate limit.</td></tr><tr><td>x-ratelimit-limit-tokens</td><td>150000</td><td>The maximum number of tokens that are permitted before exhausting the rate limit.</td></tr><tr><td>x-ratelimit-remaining-requests</td><td>59</td><td>The remaining number of requests that are permitted before exhausting the rate limit.</td></tr><tr><td>x-ratelimit-remaining-tokens</td><td>149984</td><td>The remaining number of tokens that are permitted before exhausting the rate limit.</td></tr><tr><td>x-ratelimit-reset-requests</td><td>1s</td><td>The time until the rate limit (based on requests) resets to its initial state.</td></tr><tr><td>x-ratelimit-reset-tokens</td><td>6m0s</td><td>The time until the rate limit (based on tokens) resets to its initial state.</td></tr></tbody></table><h3 id="fine-tuning-rate-limits" tabindex="-1">Fine-tuning rate limits <a class="header-anchor" href="#fine-tuning-rate-limits" aria-label="Permalink to &quot;Fine-tuning rate limits&quot;">​</a></h3><p>The fine-tuning rate limits for your organization can be <a href="/settings/organization/limits.html">found in the dashboard as well</a>, and can also be retrieved via API:</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">curl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://api.openai.com/v1/fine_tuning/model_limits</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  -H</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;Authorization: Bearer </span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">$OPENAI_API_KEY</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span></span></code></pre></div><h2 id="error-mitigation" tabindex="-1">Error mitigation <a class="header-anchor" href="#error-mitigation" aria-label="Permalink to &quot;Error mitigation&quot;">​</a></h2><h3 id="what-are-some-steps-i-can-take-to-mitigate-this" tabindex="-1">What are some steps I can take to mitigate this? <a class="header-anchor" href="#what-are-some-steps-i-can-take-to-mitigate-this" aria-label="Permalink to &quot;What are some steps I can take to mitigate this?&quot;">​</a></h3><p>The OpenAI Cookbook has a <a href="https://cookbook.openai.com/examples/how_to_handle_rate_limits" target="_blank" rel="noreferrer">Python notebook</a> that explains how to avoid rate limit errors, as well an example <a href="https://github.com/openai/openai-cookbook/blob/main/examples/api_request_parallel_processor.py" target="_blank" rel="noreferrer">Python script</a> for staying under rate limits while batch processing API requests.</p><p>You should also exercise caution when providing programmatic access, bulk processing features, and automated social media posting - consider only enabling these for trusted customers.</p><p>To protect against automated and high-volume misuse, set a usage limit for individual users within a specified time frame (daily, weekly, or monthly). Consider implementing a hard cap or a manual review process for users who exceed the limit.</p><h4 id="retrying-with-exponential-backoff" tabindex="-1">Retrying with exponential backoff <a class="header-anchor" href="#retrying-with-exponential-backoff" aria-label="Permalink to &quot;Retrying with exponential backoff&quot;">​</a></h4><p>One easy way to avoid rate limit errors is to automatically retry requests with a random exponential backoff. Retrying with exponential backoff means performing a short sleep when a rate limit error is hit, then retrying the unsuccessful request. If the request is still unsuccessful, the sleep length is increased and the process is repeated. This continues until the request is successful or until a maximum number of retries is reached. This approach has many benefits:</p><ul><li>Automatic retries means you can recover from rate limit errors without crashes or missing data</li><li>Exponential backoff means that your first retries can be tried quickly, while still benefiting from longer delays if your first few retries fail</li><li>Adding random jitter to the delay helps retries from all hitting at the same time.</li></ul><p>Note that unsuccessful requests contribute to your per-minute limit, so continuously resending a request won’t work.</p><p>Below are a few example solutions <strong>for Python</strong> that use exponential backoff.</p><p>Example 1: Using the Tenacity library</p><p>Tenacity is an Apache 2.0 licensed general-purpose retrying library, written in Python, to simplify the task of adding retry behavior to just about anything. To add exponential backoff to your requests, you can use the <code>tenacity.retry</code> decorator. The below example uses the <code>tenacity.wait_random_exponential</code> function to add random exponential backoff to a request.</p><p>Using the Tenacity library</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> openai </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">client </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tenacity </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    retry,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    stop_after_attempt,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    wait_random_exponential,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># for exponential backoff</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@retry</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">wait</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">wait_random_exponential(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">min</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">max</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">60</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">stop</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">stop_after_attempt(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">6</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> completion_with_backoff</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> client.completions.create(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">kwargs)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">completion_with_backoff(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;gpt-4o-mini&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">prompt</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Once upon a time,&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>Note that the Tenacity library is a third-party tool, and OpenAI makes no guarantees about its reliability or security.</p><p>Example 2: Using the backoff library</p><p>Another python library that provides function decorators for backoff and retry is <a href="https://pypi.org/project/backoff/" target="_blank" rel="noreferrer">backoff</a>:</p><p>Using the Tenacity library</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> backoff </span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> openai</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> openai </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">client </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@backoff.on_exception</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(backoff.expo, openai.RateLimitError)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> completions_with_backoff</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> client.completions.create(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">kwargs)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">completions_with_backoff(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;gpt-4o-mini&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">prompt</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Once upon a time,&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>Like Tenacity, the backoff library is a third-party tool, and OpenAI makes no guarantees about its reliability or security.</p><p>Example 3: Manual backoff implementation</p><p>If you don&#39;t want to use third-party libraries, you can implement your own backoff logic following this example:</p><p>Using manual backoff implementation</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># imports</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> random</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> time</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> openai</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> openai </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">client </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># define a retry decorator</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> retry_with_exponential_backoff</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    func,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    initial_delay: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">float</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    exponential_base: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">float</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    jitter: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">bool</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    max_retries: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">int</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    errors: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">tuple</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (openai.RateLimitError,),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;Retry a function with exponential backoff.&quot;&quot;&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> wrapper</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">args, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # Initialize variables</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        num_retries </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        delay </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> initial_delay</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # Loop until a successful response or max_retries is hit or an exception is raised</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        while</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">            try</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">                return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> func(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">args, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">kwargs)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # Retry on specific errors</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">            except</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> errors </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> e:</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">                # Increment retries</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">                num_retries </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">                # Check if max retries has been reached</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">                if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> num_retries </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> max_retries:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">                    raise</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> Exception</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">                        f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Maximum number of retries (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">max_retries</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">) exceeded.&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">                    )</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">                # Increment the delay</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">                delay </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> exponential_base </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> +</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> jitter </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> random.random())</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">                # Sleep for the delay</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">                time.sleep(delay)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # Raise exceptions for any errors not specified</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">            except</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> Exception</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> e:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">                raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> e</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> wrapper</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@retry_with_exponential_backoff</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> completions_with_backoff</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> client.completions.create(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">kwargs)</span></span></code></pre></div><p>Again, OpenAI makes no guarantees on the security or efficiency of this solution but it can be a good starting place for your own solution.</p><h4 id="reduce-the-max-tokens-to-match-the-size-of-your-completions" tabindex="-1">Reduce the <code>max_tokens</code> to match the size of your completions <a class="header-anchor" href="#reduce-the-max-tokens-to-match-the-size-of-your-completions" aria-label="Permalink to &quot;Reduce the \`max_tokens\` to match the size of your completions&quot;">​</a></h4><p>Your rate limit is calculated as the maximum of <code>max_tokens</code> and the estimated number of tokens based on the character count of your request. Try to set the <code>max_tokens</code> value as close to your expected response size as possible.</p><h4 id="batching-requests" tabindex="-1">Batching requests <a class="header-anchor" href="#batching-requests" aria-label="Permalink to &quot;Batching requests&quot;">​</a></h4><p>If your use case does not require immediate responses, you can use the <a href="/docs/openai/guides/batch.html">Batch API</a> to more easily submit and execute large collections of requests without impacting your synchronous request rate limits.</p><p>For use cases that <em>do</em> requires synchronous respones, the OpenAI API has separate limits for <strong>requests per minute</strong> and <strong>tokens per minute</strong>.</p><p>If you&#39;re hitting the limit on requests per minute but have available capacity on tokens per minute, you can increase your throughput by batching multiple tasks into each request. This will allow you to process more tokens per minute, especially with our smaller models.</p><p>Sending in a batch of prompts works exactly the same as a normal API call, except you pass in a list of strings to the prompt parameter instead of a single string. <a href="/docs/openai/guides/batch.html">Learn more in the Batch API guide</a>.</p>`,55)]))}const g=i(n,[["render",l]]);export{c as __pageData,g as default};
