import{_ as t,c as o,o as a,ag as i}from"./chunks/framework.BDwTZuFy.js";const c=JSON.parse('{"title":"Model optimization","description":"","frontmatter":{},"headers":[],"relativePath":"docs/openai/guides/model-optimization.md","filePath":"docs/openai/guides/model-optimization.md","lastUpdated":1748071461000}'),n={name:"docs/openai/guides/model-optimization.md"};function s(r,e,p,l,d,u){return a(),o("div",null,e[0]||(e[0]=[i('<h1 id="model-optimization" tabindex="-1">Model optimization <a class="header-anchor" href="#model-optimization" aria-label="Permalink to &quot;Model optimization&quot;">​</a></h1><p>Ensure quality model outputs with evals and fine-tuning in the OpenAI platform.</p><p>LLM output is non-deterministic, and model behavior changes between model snapshots and families. Developers must constantly measure and tune the performance of LLM applications to ensure they&#39;re getting the best results. In this guide, we explore the techniques and OpenAI platform tools you can use to ensure high quality outputs from the model.</p><p>[</p><p><img src="https://cdn.openai.com/API/docs/images/blue_card.png" alt="Evals"></p><p>Evals</p><p>Systematically measure performance.</p><p>](/docs/openai/guides/evals)[</p><p><img src="https://cdn.openai.com/API/docs/images/orange_card.png" alt="Prompt engineering"></p><p>Prompt engineering</p><p>Give context, instructions, and goals.</p><p>](/docs/openai/guides/text?api-mode=responses#prompt-engineering)[</p><p><img src="https://cdn.openai.com/API/docs/images/purple_card.png" alt="Fine-tuning"></p><p>Fine-tuning</p><p>Train models to excel at a task.</p><p>](/docs/openai/guides/fine-tuning)</p><h2 id="model-optimization-workflow" tabindex="-1">Model optimization workflow <a class="header-anchor" href="#model-optimization-workflow" aria-label="Permalink to &quot;Model optimization workflow&quot;">​</a></h2><p>Optimizing model output requires a combination of <strong>evals</strong>, <strong>prompt engineering</strong>, and <strong>fine-tuning</strong>, creating a flywheel of feedback that leads to better prompts and better training data for fine-tuning. The optimization process usually goes something like this.</p><ol><li>Write <a href="/docs/openai/guides/evals.html">evals</a> that measure model output, establishing a baseline for performance and accuracy.</li><li><a href="/docs/openai/guides/text.html">Prompt the model</a> for output, providing relevant context data and instructions.</li><li>For some use cases, it may be desirable to <a href="/docs/openai/guides/fine-tuning.html">fine-tune</a> a model for a specific task.</li><li>Run evals using test data that is representative of real world inputs. Measure the performance of your prompt and fine-tuned model.</li><li>Tweak your prompt or fine-tuning dataset based on eval feedback.</li><li>Repeat the loop continuously to improve your model results.</li></ol><p>Here&#39;s an overview of the major steps, and how to do them using the OpenAI platform.</p><h2 id="build-evals" tabindex="-1">Build evals <a class="header-anchor" href="#build-evals" aria-label="Permalink to &quot;Build evals&quot;">​</a></h2><p>In the OpenAI platform, you can <a href="/docs/openai/guides/evals.html">build and run evals</a> either via API or in the <a href="/evaluations.html">dashboard</a>. You might even consider writing evals <em>before</em> you start writing prompts, taking an approach akin to behavior-driven development (BDD).</p><p>Run your evals against test inputs like you expect to see in production. Using one of several available <a href="/docs/openai/guides/graders.html">graders</a>, measure the results of a prompt against your test data set.</p><p>[</p><p>Learn about evals</p><p>Run tests on your model outputs to ensure you&#39;re getting the right results.</p><p>](/docs/openai/guides/evals)</p><h2 id="write-effective-prompts" tabindex="-1">Write effective prompts <a class="header-anchor" href="#write-effective-prompts" aria-label="Permalink to &quot;Write effective prompts&quot;">​</a></h2><p>With evals in place, you can effectively iterate on <a href="/docs/openai/guides/text.html">prompts</a>. The prompt engineering process may be all you need in order to get great results for your use case. Different models may require different prompting techniques, but there are several best practices you can apply across the board to get better results.</p><ul><li><strong>Include relevant context</strong> - in your instructions, include text or image content that the model will need to generate a response from outside its training data. This could include data from private databases or current, up-to-the-minute information.</li><li><strong>Provide clear instructions</strong> - your prompt should contain clear goals about what kind of output you want. GPT models like <code>gpt-4.1</code> are great at following very explicit instructions, while <a href="/docs/openai/guides/reasoning.html">reasoning models</a> like <code>o4-mini</code> tend to do better with high level guidance on outcomes.</li><li><strong>Provide example outputs</strong> - give the model a few examples of correct output for a given prompt (a process called few-shot learning). The model can extrapolate from these examples how it should respond for other prompts.</li></ul><p>[</p><p>Learn about prompt engineering</p><p>Learn the basics of writing good prompts for the model.</p><p>](/docs/openai/guides/text)</p><h2 id="fine-tune-a-model" tabindex="-1">Fine-tune a model <a class="header-anchor" href="#fine-tune-a-model" aria-label="Permalink to &quot;Fine-tune a model&quot;">​</a></h2><p>Using the latest base <a href="/docs/models.html">models</a> and iterating on prompts might be all you need to achieve good performance for your use case, but sometimes it&#39;s useful to <a href="/docs/openai/guides/fine-tuning.html">fine-tune a model</a> for a specific task. Fine-tuning exposes a model to additional training data that it can use to update its weights, and adjust how it responds to prompts.</p><p>Fine-tuning can be a time-consuming process, but it can also enable a model to consistently format responses in a certain way or handle novel inputs.</p><p>[</p><p>Learn about fine-tuning</p><p>Learn how to fine-tune a model for a specific use case.</p><p>](/docs/openai/guides/fine-tuning)</p><h2 id="learn-from-experts" tabindex="-1">Learn from experts <a class="header-anchor" href="#learn-from-experts" aria-label="Permalink to &quot;Learn from experts&quot;">​</a></h2><p>Model optimization is a complex topic, and sometimes more art than science. Check out the videos below from members of the OpenAI team on model optimization techniques.</p><p>Cost/accuracy/latency</p><p>Distillation</p><p>Optimizing LLM Performance</p>',46)]))}const h=t(n,[["render",s]]);export{c as __pageData,h as default};
