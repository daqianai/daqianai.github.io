import{_ as a,c as t,o as n,ag as i}from"./chunks/framework.BDwTZuFy.js";const u=JSON.parse('{"title":"Voice agents","description":"","frontmatter":{},"headers":[],"relativePath":"docs/openai/guides/voice-agents-chained.md","filePath":"docs/openai/guides/voice-agents-chained.md","lastUpdated":1748071461000}'),o={name:"docs/openai/guides/voice-agents-chained.md"};function s(r,e,c,p,d,l){return n(),t("div",null,e[0]||(e[0]=[i(`<h1 id="voice-agents" tabindex="-1">Voice agents <a class="header-anchor" href="#voice-agents" aria-label="Permalink to &quot;Voice agents&quot;">​</a></h1><p>Learn how to build voice agents that can understand audio and respond back in natural language.</p><p>Use the OpenAI API and Agents SDK to create powerful, context-aware voice agents for applications like customer support and language tutoring. This guide helps you design and build a voice agent.</p><h2 id="choose-the-right-architecture" tabindex="-1">Choose the right architecture <a class="header-anchor" href="#choose-the-right-architecture" aria-label="Permalink to &quot;Choose the right architecture&quot;">​</a></h2><p>OpenAI provides two primary architectures for building voice agents:</p><p>[</p><p><img src="https://cdn.openai.com/API/docs/images/blue_card.png" alt="Speech-to-Speech"></p><p>Speech-to-Speech</p><p>Native audio handling by the model using the Realtime API</p><p>](/docs/openai/guides/voice-agents?voice-agent-architecture=speech-to-speech)[</p><p><img src="https://cdn.openai.com/API/docs/images/orange_card.png" alt="Chained"></p><p>Chained</p><p>Transforming audio to text and back to use existing models</p><p>](/docs/openai/guides/voice-agents?voice-agent-architecture=chained)</p><h3 id="speech-to-speech-realtime-architecture" tabindex="-1">Speech-to-speech (realtime) architecture <a class="header-anchor" href="#speech-to-speech-realtime-architecture" aria-label="Permalink to &quot;Speech-to-speech (realtime) architecture&quot;">​</a></h3><p><img src="https://cdn.openai.com/API/docs/images/diagram-speech-to-speech.png" alt="Diagram of a speech-to-speech agent"></p><p>The multimodal speech-to-speech (S2S) architecture directly processes audio inputs and outputs, handling speech in real time in a single multimodal model, <code>gpt-4o-realtime-preview</code>. The model thinks and responds in speech. It doesn&#39;t rely on a transcript of the user&#39;s input—it hears emotion and intent, filters out noise, and responds directly in speech. Use this approach for highly interactive, low-latency, conversational use cases.</p><table tabindex="0"><thead><tr><th>Strengths</th><th>Best for</th></tr></thead><tbody><tr><td>Low latency interactions</td><td>Interactive and unstructured conversations</td></tr><tr><td>Rich multimodal understanding (audio and text simultaneously)</td><td>Language tutoring and interactive learning experiences</td></tr><tr><td>Natural, fluid conversational flow</td><td>Conversational search and discovery</td></tr><tr><td>Enhanced user experience through vocal context understanding</td><td>Interactive customer service scenarios</td></tr></tbody></table><h3 id="chained-architecture" tabindex="-1">Chained architecture <a class="header-anchor" href="#chained-architecture" aria-label="Permalink to &quot;Chained architecture&quot;">​</a></h3><p><img src="https://cdn.openai.com/API/docs/images/diagram-chained-agent.png" alt="Diagram of a chained agent architecture"></p><p>A chained architecture processes audio sequentially, converting audio to text, generating intelligent responses using large language models (LLMs), and synthesizing audio from text. We recommend this predictable architecture if you&#39;re new to building voice agents. Both the user input and model&#39;s response are in text, so you have a transcript and can control what happens in your application. It&#39;s also a reliable way to convert an existing LLM-based application into a voice agent.</p><p>You&#39;re chaining these models: <code>gpt-4o-transcribe</code> → <code>gpt-4.1</code> → <code>gpt-4o-mini-tts</code></p><table tabindex="0"><thead><tr><th>Strengths</th><th>Best for</th></tr></thead><tbody><tr><td>High control and transparency</td><td>Structured workflows focused on specific user objectives</td></tr><tr><td>Robust function calling and structured interactions</td><td>Customer support</td></tr><tr><td>Reliable, predictable responses</td><td>Sales and inbound triage</td></tr><tr><td>Support for extended conversational context</td><td>Scenarios that involve transcripts and scripted responses</td></tr></tbody></table><p>The following guide below is for building agents using the <strong>chained architecture</strong>.</p><p>To learn more about our recommended speech-to-speech architecture, see <a href="/docs/openai/guides/voice-agents?voice-agent-architecture=speech-to-speech">the speech-to-speech architecture guide</a>.</p><h2 id="build-a-voice-agent" tabindex="-1">Build a voice agent <a class="header-anchor" href="#build-a-voice-agent" aria-label="Permalink to &quot;Build a voice agent&quot;">​</a></h2><p>Use OpenAI&#39;s APIs and SDKs to create powerful, context-aware voice agents.</p><p>One of the benefits of using a chained architecture is that you can use existing agents for part of your flow and extend them with voice capabilities.</p><p>If you are new to building agents, or have already been building with the <a href="https://openai.github.io/openai-agents-python/" target="_blank" rel="noreferrer">OpenAI Agents SDK for Python</a>, you can use its built-in <a href="https://openai.github.io/openai-agents-python/voice/quickstart/" target="_blank" rel="noreferrer"><code>VoicePipeline</code> support</a> to extend your existing agents with voice capabilities.</p><div class="language-text vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">text</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>pip install openai-agents[voice]</span></span></code></pre></div><p>See the <a href="https://openai.github.io/openai-agents-python/voice/quickstart/" target="_blank" rel="noreferrer">Agents SDK voice agents quickstart in GitHub</a> to follow a complete example.</p><p>In the example, you&#39;ll:</p><ul><li>Run a speech-to-text model to turn audio into text.</li><li>Run your code, which is usually an agentic workflow, to produce a result.</li><li>Run a text-to-speech model to turn the result text back into audio.</li></ul><h3 id="transcribe-audio-and-handle-turn-detection" tabindex="-1">Transcribe audio and handle turn detection <a class="header-anchor" href="#transcribe-audio-and-handle-turn-detection" aria-label="Permalink to &quot;Transcribe audio and handle turn detection&quot;">​</a></h3><p>When you build your voice agent, you&#39;ll need to decide how you want to capture the audio input and transcribe it through a speech-to-text model. Part of this decision is choosing how you want to handle turn detection, or how you&#39;ll signal to the model that the user has finished speaking.</p><p>There are generally two options:</p><ol><li><strong>Manual turn detection</strong>: You determine when the user has finished speaking and pass the completed audio to the speech-to-text model. This works well for &quot;push-to-talk&quot; use cases where there is a clear &quot;start speaking&quot; and &quot;stop speaking&quot; signal or for situations where you want to use your own <a href="/docs/openai/guides/realtime-vad">Voice Activity Detection (VAD) model</a>.</li><li><strong>Automatic turn detection</strong>: You pass the raw audio data to our speech-to-text model and use one of <a href="/docs/openai/guides/realtime-vad">our Voice Activity Detection (VAD) models</a> to determine when the user has finished speaking. This is a good option in more conversational use cases where you don&#39;t have a clear &quot;start speaking&quot; and &quot;stop speaking&quot; signal such as a phone call.</li></ol><p>If you want to leverage our VAD model and automatic turn detection, you can use our <a href="/docs/models/gpt-4o-transcribe"><code>gpt-4o-transcribe</code></a> and <a href="/docs/models/gpt-4o-mini-transcribe"><code>gpt-4o-mini-transcribe</code></a> models using the <a href="/docs/openai/guides/realtime-transcription">Realtime Transcription API</a>.</p><p>While you could use the Realtime Transcription API also for manual turn detection, you can use the both <code>gpt-4o-transcribe</code> and <code>gpt-4o-mini-transcribe</code> with the <a href="/docs/openai/guides/speech-to-text">Audio Transcription API</a>, without the need to establish and manage a WebSocket connection.</p><p>If you are using the <a href="https://openai.github.io/openai-agents-python/voice/pipeline/#running-a-pipeline" target="_blank" rel="noreferrer">OpenAI Agents SDK</a>, this decision is handled for you depending on whether you use the <code>AudioInput()</code> or the <code>StreamedAudioInput()</code> class for your input into the <code>VoicePipeline</code>.</p><h3 id="design-your-text-based-agent" tabindex="-1">Design your text-based agent <a class="header-anchor" href="#design-your-text-based-agent" aria-label="Permalink to &quot;Design your text-based agent&quot;">​</a></h3><p>While you can largely re-use your existing text-based agent inside a chained voice agent architecuture, there are some changes that you should consider when using your agent as a voice agent.</p><h4 id="modifying-the-style-of-responses" tabindex="-1">Modifying the style of responses <a class="header-anchor" href="#modifying-the-style-of-responses" aria-label="Permalink to &quot;Modifying the style of responses&quot;">​</a></h4><p>By default most models will have a more chat like style of responses or maybe you&#39;ve even used some prompting to adhere their response style to adhere to your brand policies. However, when turning that text into speech, certain stylistic choices might not translate well into audio and might actually confuse the model.</p><p>For that reason you should add some additional prompting when your agent is used as a voice agent to ensure that the responses are natural sounding and easy to understand.</p><p>The actual prompt that works best for you will depend on your use case and brand but here are some examples of things you might want to add to your prompt:</p><ul><li>Use a concise conversational tone with short sentences</li><li>Avoid the use of any complex punctuation or emojis</li><li>Don&#39;t use any special formatting like bolding, italicizing or markdown formatting</li><li>Don&#39;t use any lists or enumerations</li></ul><h4 id="streaming-text" tabindex="-1">Streaming text <a class="header-anchor" href="#streaming-text" aria-label="Permalink to &quot;Streaming text&quot;">​</a></h4><p>To decreate latency, you also want to make sure that your agent&#39;s response tokens are streamed out as soon as they are available. This way you can start generating audio and start playing the first bits of audio back to the user and let the model catch up in the meantime.</p><h3 id="generate-audio-output" tabindex="-1">Generate audio output <a class="header-anchor" href="#generate-audio-output" aria-label="Permalink to &quot;Generate audio output&quot;">​</a></h3><p>To turn your agent&#39;s text responses into natural-sounding speech, use OpenAI&#39;s <a href="/docs/openai/guides/text-to-speech">Speech API</a>. The latest model, <code>gpt-4o-mini-tts</code>, delivers high-quality, expressive audio output.</p><p>The Speech API is a synchronous HTTP-based service, so you&#39;ll need to have the text you want to turn into audio ready before making a request. This means you&#39;ll typically wait for your agent to finish generating a response before sending it to the API.</p><h4 id="reducing-latency-with-chunking" tabindex="-1">Reducing latency with chunking <a class="header-anchor" href="#reducing-latency-with-chunking" aria-label="Permalink to &quot;Reducing latency with chunking&quot;">​</a></h4><p>To minimize perceived latency, you can implement your own chunking logic: gather tokens from your agent as they stream in, and send them to the Speech API as soon as you have a complete sentence (or another suitable chunk). This allows you to start generating and playing audio before the entire response is ready.</p><ul><li>The trade-off: Larger chunks (e.g., full paragraphs) sound more natural and fluid, but increase wait time before playback begins. Smaller chunks (e.g., single sentences) reduce wait time but may sound less natural when you transition from one chunk to the next.</li><li>For sentence splitting, you can use a simple native implementation (see <a href="https://github.com/openai/openai-agents-python/blob/main/src/agents/voice/utils.py" target="_blank" rel="noreferrer">this example</a>), or leverage more advanced NLP models for higher accuracy—though these may introduce additional latency.</li></ul><h4 id="audio-streaming-and-formats" tabindex="-1">Audio streaming and formats <a class="header-anchor" href="#audio-streaming-and-formats" aria-label="Permalink to &quot;Audio streaming and formats&quot;">​</a></h4><p>The Speech API streams audio as soon as it&#39;s ready. For the lowest latency, use the <code>wav</code> or <code>pcm</code> output formats, as these are faster to generate and transmit than formats like <code>mp3</code> or <code>opus</code>.</p><p>If you are using the <a href="https://openai.github.io/openai-agents-python/voice/pipeline/#running-a-pipeline" target="_blank" rel="noreferrer">OpenAI Agents SDK</a>, both basic sentence chunking and using <code>pcm</code> are automatically handled for you.</p><h4 id="customizing-voice-and-tone" tabindex="-1">Customizing voice and tone <a class="header-anchor" href="#customizing-voice-and-tone" aria-label="Permalink to &quot;Customizing voice and tone&quot;">​</a></h4><p>You can use the <code>instructions</code> field in the Speech API to guide the model&#39;s voice, tone, and delivery. This allows you to match the agent&#39;s personality to your use case. For inspiration, see <a href="https://openai.fm" target="_blank" rel="noreferrer">openai.fm</a>.</p><p>Here are two example instruction prompts:</p><p>Patient teacher</p><div class="language-plaintext vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">plaintext</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>Accent/Affect: Warm, refined, and gently instructive, reminiscent </span></span>
<span class="line"><span>of a friendly art instructor.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Tone: Calm, encouraging, and articulate, clearly describing each </span></span>
<span class="line"><span>step with patience.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Pacing: Slow and deliberate, pausing often to allow the listener </span></span>
<span class="line"><span>to follow instructions comfortably.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Emotion: Cheerful, supportive, and pleasantly enthusiastic; </span></span>
<span class="line"><span>convey genuine enjoyment and appreciation of art.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Pronunciation: Clearly articulate artistic terminology (e.g., </span></span>
<span class="line"><span>&quot;brushstrokes,&quot; &quot;landscape,&quot; &quot;palette&quot;) with gentle emphasis.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Personality Affect: Friendly and approachable with a hint of </span></span>
<span class="line"><span>sophistication; speak confidently and reassuringly, guiding </span></span>
<span class="line"><span>users through each painting step patiently and warmly.</span></span></code></pre></div><p>Fitness instructor</p><div class="language-plaintext vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">plaintext</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>Voice: High-energy, upbeat, and encouraging, projecting </span></span>
<span class="line"><span>enthusiasm and motivation.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Punctuation: Short, punchy sentences with strategic pauses </span></span>
<span class="line"><span>to maintain excitement and clarity.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Delivery: Fast-paced and dynamic, with rising intonation to </span></span>
<span class="line"><span>build momentum and keep engagement high.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Phrasing: Action-oriented and direct, using motivational </span></span>
<span class="line"><span>cues to push participants forward.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Tone: Positive, energetic, and empowering, creating an </span></span>
<span class="line"><span>atmosphere of encouragement and achievement.</span></span></code></pre></div><p>For more details and advanced options, see the <a href="/docs/openai/guides/text-to-speech">Text-to-Speech guide</a>.</p>`,66)]))}const g=a(o,[["render",s]]);export{u as __pageData,g as default};
