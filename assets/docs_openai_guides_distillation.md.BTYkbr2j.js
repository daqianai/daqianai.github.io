import{_ as s,c as a,o as t,a2 as i}from"./chunks/framework.DDIT__tB.js";const c=JSON.parse('{"title":"Model distillation","description":"","frontmatter":{},"headers":[],"relativePath":"docs/openai/guides/distillation.md","filePath":"docs/openai/guides/distillation.md","lastUpdated":1748071461000}'),n={name:"docs/openai/guides/distillation.md"};function o(l,e,h,r,p,d){return t(),a("div",null,e[0]||(e[0]=[i(`<h1 id="model-distillation" tabindex="-1">Model distillation <a class="header-anchor" href="#model-distillation" aria-label="Permalink to &quot;Model distillation&quot;">​</a></h1><p>Improve smaller models with distillation techniques.</p><p>Model Distillation allows you to leverage the outputs of a large model to <a href="/docs/openai/guides/fine-tuning">fine-tune</a> a smaller model, enabling it to achieve similar performance on a specific task. This process can significantly reduce both cost and latency, as smaller models are typically more efficient.</p><p>Here&#39;s how it works:</p><ol><li>Store high-quality outputs of a large model using the <a href="/docs/api-reference/chat/create#chat-create-store"><code>store</code></a> parameter in the Chat Completions API to store them.</li><li><a href="/docs/openai/guides/evals">Evaluate</a> the stored completions with both the large and the small model to establish a baseline.</li><li>Select the stored completions that you&#39;d like to use to for distillation and use them to <a href="/docs/openai/guides/fine-tuning">fine-tune</a> the smaller model.</li><li><a href="/docs/openai/guides/evals">Evaluate</a> the performance of the fine-tuned model to see how it compares to the large model.</li></ol><p>Let&#39;s go through these steps to see how it&#39;s done.</p><h2 id="store-high-quality-outputs-of-a-large-model" tabindex="-1">Store high-quality outputs of a large model <a class="header-anchor" href="#store-high-quality-outputs-of-a-large-model" aria-label="Permalink to &quot;Store high-quality outputs of a large model&quot;">​</a></h2><p>The first step in the distillation process is to generate good results with a large model like <code>o1-preview</code> or <code>gpt-4o</code> that meet your bar. As you generate these results, you can store them using the <code>store: true</code> option in the <a href="/docs/api-reference/chat/create#chat-create-store">Chat Completions API</a>. We also recommend you use the <a href="/docs/api-reference/chat/create#chat-create-metadata">metadata</a> property to tag these completions for easy filtering later.</p><p>These stored completion can then be viewed and filtered in the <a href="/chat-completions">dashboard</a>.</p><p>Store high-quality outputs of a large model</p><div class="language-javascript vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">javascript</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;openai&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> openai</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> new</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> OpenAI</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">();</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> response</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> openai.chat.completions.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">create</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">({</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  model: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;gpt-4.1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  messages: [</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    { role: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;system&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, content: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;You are a corporate IT support expert.&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    { role: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;user&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, content: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;How can I hide the dock on my Mac?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">},</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  ],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  store: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">true</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  metadata: {</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    role: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;manager&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    department: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;accounting&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    source: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;homepage&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">});</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">console.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">log</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(response.choices[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]);</span></span></code></pre></div><p>When using the <code>store: true</code> option, completions are stored for 30 days. Your completions may contain sensitive information and so, you may want to consider creating a new <a href="https://help.openai.com/en/articles/9186755-managing-your-work-in-the-api-platform-with-projects" target="_blank" rel="noreferrer">Project</a> with limited access to store these completions.</p><h2 id="evaluate-to-establish-a-baseline" tabindex="-1">Evaluate to establish a baseline <a class="header-anchor" href="#evaluate-to-establish-a-baseline" aria-label="Permalink to &quot;Evaluate to establish a baseline&quot;">​</a></h2><p>You can use your stored completions to evaluate the performance of both the larger model and a smaller model on your task to establish a baseline. This can be done using the <a href="/docs/openai/guides/evals">evals</a> product.</p><p>Typically, the large model will outperform the smaller model on your evaluations. Establishing this baseline allows you to measure the improvements gained through the distillation / fine-tuning process.</p><h2 id="create-training-dataset-to-fine-tune-smaller-model" tabindex="-1">Create training dataset to fine-tune smaller model <a class="header-anchor" href="#create-training-dataset-to-fine-tune-smaller-model" aria-label="Permalink to &quot;Create training dataset to fine-tune smaller model&quot;">​</a></h2><p>Next you can select a subset of your stored completions to use as training data for fine-tuning a smaller model like <code>gpt-4o-mini</code>. <a href="/chat-completions">Filter your stored completions</a> to those that you would like to use to train the small model, and click the &quot;Distill&quot; button. A few hundred samples might be sufficient, but sometimes a more diverse range of thousands of samples can yield better results.</p><p><img src="https://openaidevs.retool.com/api/file/7c0009a4-e9f9-4b66-af50-c4e58e0d267d" alt="distill results"></p><p>This action will open a dialog to begin a <a href="/docs/openai/guides/fine-tuning">fine-tuning job</a>, with your selected completions as the training dataset. Configure the parameters as needed, choosing the base model you wish to fine-tune. In this example, we&#39;re going to choose the <a href="/docs/models#gpt-4o-mini">latest snapshot of GPT-4o-mini</a>.</p><p><img src="https://openaidevs.retool.com/api/file/ab8d0ccf-df5d-4099-80e1-2f257d82a92f" alt="fine tune job"></p><p>After configuring, click &quot;Run&quot; to start the fine-tuning job. The process may take 15 minutes or longer, depending on the size of your training dataset.</p><h2 id="evaluate-the-fine-tuned-small-model" tabindex="-1">Evaluate the fine-tuned small model <a class="header-anchor" href="#evaluate-the-fine-tuned-small-model" aria-label="Permalink to &quot;Evaluate the fine-tuned small model&quot;">​</a></h2><p>When your fine-tuning job is complete, you can run evals against it to see how it stacks up against the base small and large models. You can select fine-tuned models in the <a href="/evaluations">Evals</a> product to generate new completions with the fine-tuned small model.</p><p><img src="https://openaidevs.retool.com/api/file/8fcfdb03-1385-47d8-81d6-735af29594cc" alt="eval using ft model"></p><p>Alternately, you could also store <a href="./(/docs/openai/guides/distillation#send-fine-tuned)">new Chat Completions</a> generated by the fine-tuned model, and use them to evaluate performance. By continually tweaking and improving:</p><ul><li>The diversity of the training data</li><li>Your prompts and outputs on the large model</li><li>The accuracy of your eval graders</li></ul><p>You can bring the performance of the smaller model up to the same levels as the large model, for a specific subset of tasks.</p><h2 id="next-steps" tabindex="-1">Next steps <a class="header-anchor" href="#next-steps" aria-label="Permalink to &quot;Next steps&quot;">​</a></h2><p>Distilling large model results to a small model is one powerful way to improve the results you generate from your models, but not the only one. Check out these resources to learn more about optimizing your outputs.</p><p>[</p><p>Fine-tuning</p><p>Improve a model&#39;s ability to generate responses tailored to your use case.</p><p>](/docs/openai/guides/fine-tuning)[</p><p>Evals</p><p>Run tests on your model outputs to ensure you&#39;re getting the right results.</p><p>](/docs/openai/guides/evals)</p>`,36)]))}const k=s(n,[["render",o]]);export{c as __pageData,k as default};
