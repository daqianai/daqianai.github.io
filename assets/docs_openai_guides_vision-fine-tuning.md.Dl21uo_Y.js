import{_ as s,c as a,o as e,ag as n}from"./chunks/framework.BDwTZuFy.js";const k=JSON.parse('{"title":"Vision fine-tuning","description":"","frontmatter":{},"headers":[],"relativePath":"docs/openai/guides/vision-fine-tuning.md","filePath":"docs/openai/guides/vision-fine-tuning.md","lastUpdated":1748071461000}'),t={name:"docs/openai/guides/vision-fine-tuning.md"};function l(o,i,p,h,r,d){return e(),a("div",null,i[0]||(i[0]=[n(`<h1 id="vision-fine-tuning" tabindex="-1">Vision fine-tuning <a class="header-anchor" href="#vision-fine-tuning" aria-label="Permalink to &quot;Vision fine-tuning&quot;">​</a></h1><p>Fine-tune models for better image understanding.</p><p>Vision fine-tuning uses image inputs for <a href="/docs/openai/guides/supervised-fine-tuning">supervised fine-tuning</a> to improve the model&#39;s understanding of image inputs. This guide will take you through this subset of SFT, and outline some of the important considerations for fine-tuning with image inputs.</p><h2 id="data-format" tabindex="-1">Data format <a class="header-anchor" href="#data-format" aria-label="Permalink to &quot;Data format&quot;">​</a></h2><p>Just as you can <a href="/docs/openai/guides/vision">send one or many image inputs and create model responses based on them</a>, you can include those same message types within your JSONL training data files. Images can be provided either as HTTP URLs or data URLs containing Base64-encoded images.</p><p>Here&#39;s an example of an image message on a line of your JSONL file. Below, the JSON object is expanded for readability, but typically this JSON would appear on a single line in your data file:</p><div class="language-json vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">{</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  &quot;messages&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      &quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;system&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      &quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;You are an assistant that identifies uncommon cheeses.&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      &quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;user&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      &quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;What is this cheese?&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      &quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;user&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      &quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">          &quot;type&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;image_url&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">          &quot;image_url&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">            &quot;url&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;https://upload.wikimedia.org/wikipedia/commons/3/36/Danbo_Cheese.jpg&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">          }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">      ]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      &quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;assistant&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">      &quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Danbo&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  ]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div><p>Uploading training data for vision fine-tuning follows the <a href="/docs/openai/guides/supervised-fine-tuning">same process described here</a>.</p><h2 id="image-data-requirements" tabindex="-1">Image data requirements <a class="header-anchor" href="#image-data-requirements" aria-label="Permalink to &quot;Image data requirements&quot;">​</a></h2><h4 id="size" tabindex="-1">Size <a class="header-anchor" href="#size" aria-label="Permalink to &quot;Size&quot;">​</a></h4><ul><li>Your training file can contain a maximum of 50,000 examples that contain images (not including text examples).</li><li>Each example can have at most 10 images.</li><li>Each image can be at most 10 MB.</li></ul><h4 id="format" tabindex="-1">Format <a class="header-anchor" href="#format" aria-label="Permalink to &quot;Format&quot;">​</a></h4><ul><li>Images must be JPEG, PNG, or WEBP format.</li><li>Your images must be in the RGB or RGBA image mode.</li><li>You cannot include images as output from messages with the <code>assistant</code> role.</li></ul><h4 id="content-moderation-policy" tabindex="-1">Content moderation policy <a class="header-anchor" href="#content-moderation-policy" aria-label="Permalink to &quot;Content moderation policy&quot;">​</a></h4><p>We scan your images before training to ensure that they comply with our usage policy. This may introduce latency in file validation before fine-tuning begins.</p><p>Images containing the following will be excluded from your dataset and not used for training:</p><ul><li>People</li><li>Faces</li><li>Children</li><li>CAPTCHAs</li></ul><h4 id="what-to-do-if-your-images-get-skipped" tabindex="-1">What to do if your images get skipped <a class="header-anchor" href="#what-to-do-if-your-images-get-skipped" aria-label="Permalink to &quot;What to do if your images get skipped&quot;">​</a></h4><p>Your images can get skipped during training for the following reasons:</p><ul><li><strong>contains CAPTCHAs</strong>, <strong>contains people</strong>, <strong>contains faces</strong>, <strong>contains children</strong><ul><li>Remove the image. For now, we cannot fine-tune models with images containing these entities.</li></ul></li><li><strong>inaccessible URL</strong><ul><li>Ensure that the image URL is publicly accessible.</li></ul></li><li><strong>image too large</strong><ul><li>Please ensure that your images fall within our <a href="#size">dataset size limits</a>.</li></ul></li><li><strong>invalid image format</strong><ul><li>Please ensure that your images fall within our <a href="#format">dataset format</a>.</li></ul></li></ul><h2 id="best-practices" tabindex="-1">Best practices <a class="header-anchor" href="#best-practices" aria-label="Permalink to &quot;Best practices&quot;">​</a></h2><h4 id="reducing-training-cost" tabindex="-1">Reducing training cost <a class="header-anchor" href="#reducing-training-cost" aria-label="Permalink to &quot;Reducing training cost&quot;">​</a></h4><p>If you set the <code>detail</code> parameter for an image to <code>low</code>, the image is resized to 512 by 512 pixels and is only represented by 85 tokens regardless of its size. This will reduce the cost of training. <a href="/docs/openai/guides/vision#low-or-high-fidelity-image-understanding">See here for more information.</a></p><div class="language-json vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">{</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  &quot;type&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;image_url&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  &quot;image_url&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    &quot;url&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;https://upload.wikimedia.org/wikipedia/commons/3/36/Danbo_Cheese.jpg&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    &quot;detail&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;low&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div><h4 id="control-image-quality" tabindex="-1">Control image quality <a class="header-anchor" href="#control-image-quality" aria-label="Permalink to &quot;Control image quality&quot;">​</a></h4><p>To control the fidelity of image understanding, set the <code>detail</code> parameter of <code>image_url</code> to <code>low</code>, <code>high</code>, or <code>auto</code> for each image. This will also affect the number of tokens per image that the model sees during training time, and will affect the cost of training. <a href="/docs/openai/guides/vision#low-or-high-fidelity-image-understanding">See here for more information</a>.</p><h2 id="next-steps" tabindex="-1">Next steps <a class="header-anchor" href="#next-steps" aria-label="Permalink to &quot;Next steps&quot;">​</a></h2><p>Now that you know the basics of vision fine-tuning, explore these other methods as well.</p><p>[</p><p>Supervised fine-tuning</p><p>Fine-tune a model by providing correct outputs for sample inputs.</p><p>](/docs/openai/guides/supervised-fine-tuning)[</p><p>Direct preference optimization</p><p>Fine-tune a model using direct preference optimization (DPO).</p><p>](/docs/openai/guides/direct-preference-optimization)[</p><p>Reinforcement fine-tuning</p><p>Fine-tune a reasoning model by grading its outputs.</p><p>](/docs/openai/guides/reinforcement-fine-tuning)</p>`,38)]))}const g=s(t,[["render",l]]);export{k as __pageData,g as default};
