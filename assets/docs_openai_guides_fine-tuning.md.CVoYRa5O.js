import{_ as t,c as n,o as i,ag as a}from"./chunks/framework.BDwTZuFy.js";const h=JSON.parse('{"title":"Fine-tuning","description":"","frontmatter":{},"headers":[],"relativePath":"docs/openai/guides/fine-tuning.md","filePath":"docs/openai/guides/fine-tuning.md","lastUpdated":1748071461000}'),o={name:"docs/openai/guides/fine-tuning.md"};function r(s,e,p,d,l,u){return i(),n("div",null,e[0]||(e[0]=[a('<h1 id="fine-tuning" tabindex="-1">Fine-tuning <a class="header-anchor" href="#fine-tuning" aria-label="Permalink to &quot;Fine-tuning&quot;">​</a></h1><p>Fine-tune models for better results and efficiency.</p><p>Fine-tuning lets you customize a pre-trained model to excel at a particular task. Fine-tuning can be used with <a href="/docs/openai/guides/text.html">prompt engineering</a> to realize a few more benefits over prompting alone:</p><ul><li>You can provide more example inputs and outputs than could fit within the context window of a single request, enabling the model handle a wider variety of prompts.</li><li>You can use shorter prompts with fewer examples and context data, which saves on token costs at scale and can be lower latency.</li><li>You can train on proprietary or sensitive data without having to include it via examples in every request.</li><li>You can train a smaller, cheaper, faster model to excel at a particular task where a larger model is not cost-effective.</li></ul><p>Visit our <a href="https://openai.com/api/pricing" target="_blank" rel="noreferrer">pricing page</a> to learn more about how fine-tuned model training and usage are billed.</p><h2 id="fine-tuning-methods" tabindex="-1">Fine-tuning methods <a class="header-anchor" href="#fine-tuning-methods" aria-label="Permalink to &quot;Fine-tuning methods&quot;">​</a></h2><p>There are three fine-tuning methods supported in the OpenAI platform today.</p><p>|| |Supervised fine-tuning (SFT)+Vision fine-tuning|Provide examples of correct responses to prompts to guide the model&#39;s behavior. Often uses human-generated &quot;ground truth&quot; responses to show the model how it should respond.|ClassificationNuanced translationGenerating content in a specific formatCorrecting failures in instruction following for complex prompts| |Direct preference optimization (DPO)|For a prompt, provide both a correct and incorrect example response. Indicating the correct response helps the model perform better when the correct output is more subjective.|Summarizing text, focusing on the right thingsGenerating chat messages with the right tone and style| |Reinforcement fine-tuning (RFT)|Reasoning models only: Generate a response for a prompt, provide an expert grade for the result, and use the resulting score to reinforce the model&#39;s chain-of-thought for higher-scored responses. Works when expert graders can agree on the ideal output from the model.|Complex domain-specific tasks that require advanced reasoningMedical diagnosis based on history and diagnostic guidelinesDetermining relevant passages from legal case law|</p><h2 id="how-it-works" tabindex="-1">How it works <a class="header-anchor" href="#how-it-works" aria-label="Permalink to &quot;How it works&quot;">​</a></h2><p>In the OpenAI platform, you can create fine-tuned models either in the <a href="/finetune.html">dashboard</a> or <a href="/docs/api-reference/fine-tuning.html">with the API</a>.</p><p><img src="https://cdn.openai.com/API/docs/images/fine-tuning-cycle.png" alt="Provide example data and create a fine-tuning job to optimize model performance for your use case"></p><p>The general idea of fine-tuning is much like training a human in a particular subject, where you come up with the curriculum, then teach and test until the student excels. This is the general shape of the fine-tuning process:</p><ol><li>Collect a dataset of examples to use as training data</li><li>Upload that dataset to OpenAI, formatted in JSONL</li><li>Create a fine-tuning job using one of the methods above, depending on your goals—this begins the fine-tuning training process</li><li>In the case of RFT, you&#39;ll also define a grader to score the model&#39;s behavior</li><li>Evaluate the results</li></ol><p>Fine-tuning is the process of adjusting a pre-trained model&#39;s weights using a smaller, task-specific dataset. OpenAI models are already pre-trained to perform across a broad range of subjects and tasks. Fine-tuning lets you take an OpenAI base model, provide the kinds of inputs and outputs you expect in your application, and get a model that excels in the tasks you&#39;ll use it for.</p><h2 id="get-started" tabindex="-1">Get started <a class="header-anchor" href="#get-started" aria-label="Permalink to &quot;Get started&quot;">​</a></h2><p>Follow the guides linked below for examples and more information about how to fine-tune models using each of these methods.</p><p>[</p><p>Supervised fine-tuning</p><p>Fine-tune a model by providing correct outputs for sample inputs.</p><p>](/docs/openai/guides/supervised-fine-tuning)[</p><p>Vision fine-tuning</p><p>Learn to fine-tune for computer vision with image inputs.</p><p>](/docs/openai/guides/vision-fine-tuning)[</p><p>Direct preference optimization</p><p>Fine-tune a model using direct preference optimization (DPO).</p><p>](/docs/openai/guides/direct-preference-optimization)[</p><p>Reinforcement fine-tuning</p><p>Fine-tune a reasoning model by grading its outputs.</p><p>](/docs/openai/guides/reinforcement-fine-tuning)</p>',29)]))}const g=t(o,[["render",r]]);export{h as __pageData,g as default};
