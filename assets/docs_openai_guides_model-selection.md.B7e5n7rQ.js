import{_ as t,c as a,o,a2 as r}from"./chunks/framework.DDIT__tB.js";const p=JSON.parse('{"title":"Model selection","description":"","frontmatter":{},"headers":[],"relativePath":"docs/openai/guides/model-selection.md","filePath":"docs/openai/guides/model-selection.md","lastUpdated":1747747486000}'),i={name:"docs/openai/guides/model-selection.md"};function s(n,e,c,l,d,h){return o(),a("div",null,e[0]||(e[0]=[r('<h1 id="model-selection" tabindex="-1">Model selection <a class="header-anchor" href="#model-selection" aria-label="Permalink to &quot;Model selection&quot;">​</a></h1><p>Choose the best model for performance and cost.</p><p>Choosing the right model, whether GPT-4o or a smaller option like GPT-4o-mini, requires balancing <strong>accuracy</strong>, <strong>latency</strong>, and <strong>cost</strong>. This guide explains key principles to help you make informed decisions, along with a practical example.</p><h2 id="core-principles" tabindex="-1">Core principles <a class="header-anchor" href="#core-principles" aria-label="Permalink to &quot;Core principles&quot;">​</a></h2><p>The principles for model selection are simple:</p><ul><li><strong>Optimize for accuracy first:</strong> Optimize for accuracy until you hit your accuracy target.</li><li><strong>Optimize for cost and latency second:</strong> Then aim to maintain accuracy with the cheapest, fastest model possible.</li></ul><h3 id="_1-focus-on-accuracy-first" tabindex="-1">1. Focus on accuracy first <a class="header-anchor" href="#_1-focus-on-accuracy-first" aria-label="Permalink to &quot;1\\. Focus on accuracy first&quot;">​</a></h3><p>Begin by setting a clear accuracy goal for your use case, where you&#39;re clear on the accuracy that would be &quot;good enough&quot; for this use case to go to production. You can accomplish this through:</p><ul><li><strong>Setting a clear accuracy target:</strong> Identify what your target accuracy statistic is going to be. <ul><li>For example, 90% of customer service calls need to be triaged correctly at the first interaction.</li></ul></li><li><strong>Developing an evaluation dataset:</strong> Create a dataset that allows you to measure the model&#39;s performance against these goals. <ul><li>To extend the example above, capture 100 interaction examples where we have what the user asked for, what the LLM triaged them to, what the correct triage should be, and whether this was correct or not.</li></ul></li><li><strong>Using the most powerful model to optimize:</strong> Start with the most capable model available to achieve your accuracy targets. Log all responses so we can use them for distillation of a smaller model. <ul><li>Use retrieval-augmented generation to optimize for accuracy</li><li>Use fine-tuning to optimize for consistency and behavior</li></ul></li></ul><p>During this process, collect prompt and completion pairs for use in evaluations, few-shot learning, or fine-tuning. This practice, known as <strong>prompt baking</strong>, helps you produce high-quality examples for future use.</p><p>For more methods and tools here, see our <a href="https://platform.openai.com/docs/guides/optimizing-llm-accuracy" target="_blank" rel="noreferrer">Accuracy Optimization Guide</a>.</p><h4 id="setting-a-realistic-accuracy-target" tabindex="-1">Setting a realistic accuracy target <a class="header-anchor" href="#setting-a-realistic-accuracy-target" aria-label="Permalink to &quot;Setting a realistic accuracy target&quot;">​</a></h4><p>Calculate a realistic accuracy target by evaluating the financial impact of model decisions. For example, in a fake news classification scenario:</p><ul><li><strong>Correctly classified news:</strong> If the model classifies it correctly, it saves you the cost of a human reviewing it - let&#39;s assume <strong>$50</strong>.</li><li><strong>Incorrectly classified news:</strong> If it falsely classifies a safe article or misses a fake news article, it may trigger a review process and possible complaint, which might cost us <strong>$300</strong>.</li></ul><p>Our news classification example would need <strong>85.8%</strong> accuracy to cover costs, so targeting 90% or more ensures an overall return on investment. Use these calculations to set an effective accuracy target based on your specific cost structures.</p><h3 id="_2-optimize-cost-and-latency" tabindex="-1">2. Optimize cost and latency <a class="header-anchor" href="#_2-optimize-cost-and-latency" aria-label="Permalink to &quot;2\\. Optimize cost and latency&quot;">​</a></h3><p>Cost and latency are considered secondary because if the model can’t hit your accuracy target then these concerns are moot. However, once you’ve got a model that works for your use case, you can take one of two approaches:</p><ul><li><strong>Compare with a smaller model zero- or few-shot:</strong> Swap out the model for a smaller, cheaper one and test whether it maintains accuracy at the lower cost and latency point.</li><li><strong>Model distillation:</strong> Fine-tune a smaller model using the data gathered during accuracy optimization.</li></ul><p>Cost and latency are typically interconnected; reducing tokens and requests generally leads to faster processing.</p><p>The main strategies to consider here are:</p><ul><li><strong>Reduce requests:</strong> Limit the number of necessary requests to complete tasks.</li><li><strong>Minimize tokens:</strong> Lower the number of input tokens and optimize for shorter model outputs.</li><li><strong>Select a smaller model:</strong> Use models that balance reduced costs and latency with maintained accuracy.</li></ul><p>To dive deeper into these, please refer to our guide on <a href="https://platform.openai.com/docs/guides/latency-optimization" target="_blank" rel="noreferrer">latency optimization</a>.</p><h4 id="exceptions-to-the-rule" tabindex="-1">Exceptions to the rule <a class="header-anchor" href="#exceptions-to-the-rule" aria-label="Permalink to &quot;Exceptions to the rule&quot;">​</a></h4><p>Clear exceptions exist for these principles. If your use case is extremely cost or latency sensitive, establish thresholds for these metrics before beginning your testing, then remove the models that exceed those from consideration. Once benchmarks are set, these guidelines will help you refine model accuracy within your constraints.</p><h2 id="practical-example" tabindex="-1">Practical example <a class="header-anchor" href="#practical-example" aria-label="Permalink to &quot;Practical example&quot;">​</a></h2><p>To demonstrate these principles, we&#39;ll develop a fake news classifier with the following target metrics:</p><ul><li><strong>Accuracy:</strong> Achieve 90% correct classification</li><li><strong>Cost:</strong> Spend less than $5 per 1,000 articles</li><li><strong>Latency:</strong> Maintain processing time under 2 seconds per article</li></ul><h3 id="experiments" tabindex="-1">Experiments <a class="header-anchor" href="#experiments" aria-label="Permalink to &quot;Experiments&quot;">​</a></h3><p>We ran three experiments to reach our goal:</p><ol><li><strong>Zero-shot:</strong> Used <code>GPT-4o</code> with a basic prompt for 1,000 records, but missed the accuracy target.</li><li><strong>Few-shot learning:</strong> Included 5 few-shot examples, meeting the accuracy target but exceeding cost due to more prompt tokens.</li><li><strong>Fine-tuned model:</strong> Fine-tuned <code>GPT-4o-mini</code> with 1,000 labeled examples, meeting all targets with similar latency and accuracy but significantly lower costs.</li></ol><table tabindex="0"><thead><tr><th>ID</th><th>Method</th><th>Accuracy</th><th>Accuracy target</th><th>Cost</th><th>Cost target</th><th>Avg. latency</th><th>Latency target</th></tr></thead><tbody><tr><td>1</td><td>gpt-4o zero-shot</td><td>84.5%</td><td></td><td>$1.72</td><td></td><td>&lt; 1s</td><td></td></tr><tr><td>2</td><td>gpt-4o few-shot (n=5)</td><td>91.5%</td><td>✓</td><td>$11.92</td><td></td><td>&lt; 1s</td><td>✓</td></tr><tr><td>3</td><td>gpt-4o-mini fine-tuned w/ 1000 examples</td><td>91.5%</td><td>✓</td><td>$0.21</td><td>✓</td><td>&lt; 1s</td><td>✓</td></tr></tbody></table><h2 id="conclusion" tabindex="-1">Conclusion <a class="header-anchor" href="#conclusion" aria-label="Permalink to &quot;Conclusion&quot;">​</a></h2><p>By switching from <code>gpt-4o</code> to <code>gpt-4o-mini</code> with fine-tuning, we achieved <strong>equivalent performance for less than 2%</strong> of the cost, using only 1,000 labeled examples.</p><p>This process is important - you often can’t jump right to fine-tuning because you don’t know whether fine-tuning is the right tool for the optimization you need, or you don’t have enough labeled examples. Use <code>gpt-4o</code> to achieve your accuracy targets, and curate a good training set - then go for a smaller, more efficient model with fine-tuning.</p>',34)]))}const g=t(i,[["render",s]]);export{p as __pageData,g as default};
