import{_ as a,c as i,o as e,ag as n}from"./chunks/framework.BDwTZuFy.js";const c=JSON.parse('{"title":"Text generation and prompting","description":"","frontmatter":{},"headers":[],"relativePath":"docs/openai/guides/text-chat.md","filePath":"docs/openai/guides/text-chat.md","lastUpdated":1748071461000}'),t={name:"docs/openai/guides/text-chat.md"};function l(p,s,o,h,r,d){return e(),i("div",null,s[0]||(s[0]=[n(`<h1 id="text-generation-and-prompting" tabindex="-1">Text generation and prompting <a class="header-anchor" href="#text-generation-and-prompting" aria-label="Permalink to &quot;Text generation and prompting&quot;">​</a></h1><p>Learn how to prompt a model to generate text.</p><p>With the OpenAI API, you can use a <a href="/docs/models.html">large language model</a> to generate text from a prompt, as you might using <a href="https://chatgpt.com" target="_blank" rel="noreferrer">ChatGPT</a>. Models can generate almost any kind of text response—like code, mathematical equations, structured JSON data, or human-like prose.</p><p>Here&#39;s a simple example using the <a href="/docs/api-reference/chat.html">Chat Completions API</a>.</p><p>Generate text from a simple prompt</p><div class="language-javascript vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">javascript</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;openai&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> client</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> new</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> OpenAI</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">();</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> completion</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> client.chat.completions.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">create</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">({</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    model: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;gpt-4.1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    messages: [</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        {</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            role: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;user&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            content: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Write a one-sentence bedtime story about a unicorn.&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    ],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">});</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">console.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">log</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(completion.choices[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">].message.content);</span></span></code></pre></div><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> openai </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">client </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">completion </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> client.chat.completions.create(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;gpt-4.1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    messages</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;user&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Write a one-sentence bedtime story about a unicorn.&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    ]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(completion.choices[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">].message.content)</span></span></code></pre></div><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">curl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;https://api.openai.com/v1/chat/completions&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    -H</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;Content-Type: application/json&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    -H</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;Authorization: Bearer </span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">$OPENAI_API_KEY</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    -d</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;model&quot;: &quot;gpt-4.1&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;messages&quot;: [</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">                &quot;role&quot;: &quot;user&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">                &quot;content&quot;: &quot;Write a one-sentence bedtime story about a unicorn.&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            }</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        ]</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    }&#39;</span></span></code></pre></div><p>An array of content generated by the model is in the <code>choices</code> property of the response. In this simple example, we have just one output which looks like this:</p><div class="language-json vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        &quot;index&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        &quot;message&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: {</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">            &quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;assistant&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">            &quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Under the soft glow of the moon, Luna the unicorn danced through fields of twinkling stardust, leaving trails of dreams for every child asleep.&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">            &quot;refusal&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">null</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        },</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        &quot;logprobs&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">null</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        &quot;finish_reason&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;stop&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre></div><p>In addition to plain text, you can also have the model return structured data in JSON format - this feature is called <a href="/docs/openai/guides/structured-outputs.html"><strong>Structured Outputs</strong></a>.</p><h2 id="choosing-a-model" tabindex="-1">Choosing a model <a class="header-anchor" href="#choosing-a-model" aria-label="Permalink to &quot;Choosing a model&quot;">​</a></h2><p>A key choice to make when generating content through the API is which model you want to use - the <code>model</code> parameter of the code samples above. <a href="/docs/models.html">You can find a full listing of available models here</a>. Here are a few factors to consider when choosing a model for text generation.</p><ul><li><strong><a href="/docs/openai/guides/reasoning.html">Reasoning models</a></strong> generate an internal chain of thought to analyze the input prompt, and excel at understanding complex tasks and multi-step planning. They are also generally slower and more expensive to use than GPT models.</li><li><strong>GPT models</strong> are fast, cost-efficient, and highly intelligent, but benefit from more explicit instructions around how to accomplish tasks.</li><li><strong>Large and small (mini or nano) models</strong> offer trade-offs for speed, cost, and intelligence. Large models are more effective at understanding prompts and solving problems across domains, while small models are generally faster and cheaper to use.</li></ul><p>When in doubt, <a href="/docs/models/gpt-4.1.html"><code>gpt-4.1</code></a> offers a solid combination of intelligence, speed, and cost effectiveness.</p><h2 id="prompt-engineering" tabindex="-1">Prompt engineering <a class="header-anchor" href="#prompt-engineering" aria-label="Permalink to &quot;Prompt engineering&quot;">​</a></h2><p><strong>Prompt engineering</strong> is the process of writing effective instructions for a model, such that it consistently generates content that meets your requirements.</p><p>Because the content generated from a model is non-deterministic, it is a combination of art and science to build a prompt that will generate content in the format you want. However, there are a number of techniques and best practices you can apply to consistently get good results from a model.</p><p>Some prompt engineering techniques will work with every model, like using message roles. But different model types (like reasoning versus GPT models) might need to be prompted differently to produce the best results. Even different snapshots of models within the same family could produce different results. So as you are building more complex applications, we strongly recommend that you:</p><ul><li>Pin your production applications to specific <a href="/docs/models.html">model snapshots</a> (like <code>gpt-4.1-2025-04-14</code> for example) to ensure consistent behavior.</li><li>Build <a href="/docs/openai/guides/evals.html">evals</a> that will measure the behavior of your prompts, so that you can monitor the performance of your prompts as you iterate on them, or when you change and upgrade model versions.</li></ul><p>Now, let&#39;s examine some tools and techniques available to you to construct prompts.</p><h2 id="message-roles-and-instruction-following" tabindex="-1">Message roles and instruction following <a class="header-anchor" href="#message-roles-and-instruction-following" aria-label="Permalink to &quot;Message roles and instruction following&quot;">​</a></h2><p>You can provide instructions (prompts) to the model with <a href="https://model-spec.openai.com/2025-02-12.html#chain_of_command" target="_blank" rel="noreferrer">differing levels of authority</a> using <strong>message roles</strong>.</p><p>Generate text with messages using different roles</p><div class="language-javascript vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">javascript</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;openai&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> client</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> new</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> OpenAI</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">();</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> completion</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> client.chat.completions.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">create</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">({</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    model: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;gpt-4.1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    messages: [</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        {</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            role: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;developer&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            content: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Talk like a pirate.&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        {</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            role: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;user&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            content: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Are semicolons optional in JavaScript?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    ],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">});</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">console.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">log</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(completion.choices[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">].message);</span></span></code></pre></div><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> openai </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">client </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">completion </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> client.chat.completions.create(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;gpt-4.1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    messages</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;developer&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Talk like a pirate.&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;user&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Are semicolons optional in JavaScript?&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    ]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(completion.choices[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">].message.content)</span></span></code></pre></div><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">curl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;https://api.openai.com/v1/chat/completions&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    -H</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;Content-Type: application/json&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    -H</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;Authorization: Bearer </span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">$OPENAI_API_KEY</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    -d</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;model&quot;: &quot;gpt-4.1&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;messages&quot;: [</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">                &quot;role&quot;: &quot;developer&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">                &quot;content&quot;: &quot;Talk like a pirate.&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            },</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">                &quot;role&quot;: &quot;user&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">                &quot;content&quot;: &quot;Are semicolons optional in JavaScript?&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            }</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        ]</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    }&#39;</span></span></code></pre></div><p>The <a href="https://model-spec.openai.com/2025-02-12.html#chain_of_command" target="_blank" rel="noreferrer">OpenAI model spec</a> describes how our models give different levels of priority to messages with different roles.</p><table tabindex="0"><thead><tr><th>developer</th><th>user</th><th>assistant</th></tr></thead><tbody><tr><td>developer messages are instructions provided by the application developer, prioritized ahead of user messages.</td><td>user messages are instructions provided by an end user, prioritized behind developer messages.</td><td>Messages generated by the model have the assistant role.</td></tr></tbody></table><p>A multi-turn conversation may consist of several messages of these types, along with other content types provided by both you and the model. Learn more about <a href="/docs/openai/guides/conversation-state.html">managing conversation state here</a>.</p><p>You could think about <code>developer</code> and <code>user</code> messages like a function and its arguments in a programming language.</p><ul><li><code>developer</code> messages provide the system&#39;s rules and business logic, like a function definition.</li><li><code>user</code> messages provide inputs and configuration to which the <code>developer</code> message instructions are applied, like arguments to a function.</li></ul><h2 id="message-formatting-with-markdown-and-xml" tabindex="-1">Message formatting with Markdown and XML <a class="header-anchor" href="#message-formatting-with-markdown-and-xml" aria-label="Permalink to &quot;Message formatting with Markdown and XML&quot;">​</a></h2><p>When writing <code>developer</code> and <code>user</code> messages, you can help the model understand logical boundaries of your prompt and context data using a combination of <a href="https://commonmark.org/help/" target="_blank" rel="noreferrer">Markdown</a> formatting and <a href="https://www.w3.org/TR/xml/" target="_blank" rel="noreferrer">XML tags</a>.</p><p>Markdown headers and lists can be helpful to mark distinct sections of a prompt, and to communicate hierarchy to the model. They can also potentially make your prompts more readable during development. XML tags can help delineate where one piece of content (like a supporting document used for reference) begins and ends. XML attributes can also be used to define metadata about content in the prompt that can be referenced by your instructions.</p><p>In general, a developer message will contain the following sections, usually in this order (though the exact optimal content and order may vary by which model you are using):</p><ul><li><strong>Identity:</strong> Describe the purpose, communication style, and high-level goals of the assistant.</li><li><strong>Instructions:</strong> Provide guidance to the model on how to generate the response you want. What rules should it follow? What should the model do, and what should the model never do? This section could contain many subsections as relevant for your use case, like how the model should <a href="/docs/openai/guides/function-calling.html">call custom functions</a>.</li><li><strong>Examples:</strong> Provide examples of possible inputs, along with the desired output from the model.</li><li><strong>Context:</strong> Give the model any additional information it might need to generate a response, like private/proprietary data outside its training data, or any other data you know will be particularly relevant. This content is usually best positioned near the end of your prompt, as you may include different context for different generation requests.</li></ul><p>Below is an example of using Markdown and XML tags to construct a <code>developer</code> message with distinct sections and supporting examples.</p><p>Example prompt</p><p>A developer message for code generation</p><div class="language-text vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">text</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># Identity</span></span>
<span class="line"><span></span></span>
<span class="line"><span>You are coding assistant that helps enforce the use of snake case </span></span>
<span class="line"><span>variables in JavaScript code, and writing code that will run in </span></span>
<span class="line"><span>Internet Explorer version 6.</span></span>
<span class="line"><span></span></span>
<span class="line"><span># Instructions</span></span>
<span class="line"><span></span></span>
<span class="line"><span>* When defining variables, use snake case names (e.g. my_variable) </span></span>
<span class="line"><span>  instead of camel case names (e.g. myVariable).</span></span>
<span class="line"><span>* To support old browsers, declare variables using the older </span></span>
<span class="line"><span>  &quot;var&quot; keyword.</span></span>
<span class="line"><span>* Do not give responses with Markdown formatting, just return </span></span>
<span class="line"><span>  the code as requested.</span></span>
<span class="line"><span></span></span>
<span class="line"><span># Examples</span></span>
<span class="line"><span></span></span>
<span class="line"><span>&lt;user_query&gt;</span></span>
<span class="line"><span>How do I declare a string variable for a first name?</span></span>
<span class="line"><span>&lt;/user_query&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>&lt;assistant_response&gt;</span></span>
<span class="line"><span>var first_name = &quot;Anna&quot;;</span></span>
<span class="line"><span>&lt;/assistant_response&gt;</span></span></code></pre></div><p>API request</p><p>Send a prompt to generate code through the API</p><div class="language-javascript vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">javascript</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> fs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;fs/promises&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;openai&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> client</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> new</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> OpenAI</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">();</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> instructions</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> fs.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">readFile</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;prompt.txt&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;utf-8&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">);</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> response</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> client.responses.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">create</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">({</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    model: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;gpt-4.1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    instructions,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    input: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;How would I declare a variable for a last name?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">});</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">console.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">log</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(response.output_text);</span></span></code></pre></div><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> openai </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">client </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">with</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> open</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;prompt.txt&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;r&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">encoding</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;utf-8&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> f:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    instructions </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> f.read()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">response </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> client.responses.create(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;gpt-4.1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    instructions</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">instructions,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;How would I declare a variable for a last name?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(response.output_text)</span></span></code></pre></div><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">curl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://api.openai.com/v1/responses</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  -H</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;Authorization: Bearer </span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">$OPENAI_API_KEY</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  -H</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;Content-Type: application/json&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  -d</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;model&quot;: &quot;gpt-4.1&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;instructions&quot;: &quot;&#39;&quot;$(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> prompt.txt)&quot;&#39;&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;input&quot;: &quot;How would I declare a variable for a last name?&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  }&#39;</span></span></code></pre></div><h4 id="save-on-cost-and-latency-with-prompt-caching" tabindex="-1">Save on cost and latency with prompt caching <a class="header-anchor" href="#save-on-cost-and-latency-with-prompt-caching" aria-label="Permalink to &quot;Save on cost and latency with prompt caching&quot;">​</a></h4><p>When constructing a message, you should try and keep content that you expect to use over and over in your API requests at the beginning of your prompt, <strong>and</strong> among the first API parameters you pass in the JSON request body to <a href="/docs/api-reference/chat.html">Chat Completions</a> or <a href="/docs/api-reference/responses.html">Responses</a>. This enables you to maximize cost and latency savings from <a href="/docs/openai/guides/prompt-caching.html">prompt caching</a>.</p><h2 id="few-shot-learning" tabindex="-1">Few-shot learning <a class="header-anchor" href="#few-shot-learning" aria-label="Permalink to &quot;Few-shot learning&quot;">​</a></h2><p>Few-shot learning lets you steer a large language model toward a new task by including a handful of input/output examples in the prompt, rather than <a href="/docs/openai/guides/fine-tuning.html">fine-tuning</a> the model. The model implicitly &quot;picks up&quot; the pattern from those examples and applies it to a prompt. When providing examples, try to show a diverse range of possible inputs with the desired outputs.</p><p>Typically, you will provide examples as part of a <code>developer</code> message in your API request. Here&#39;s an example <code>developer</code> message containing examples that show a model how to classify positive or negative customer service reviews.</p><div class="language-text vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">text</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># Identity</span></span>
<span class="line"><span></span></span>
<span class="line"><span>You are a helpful assistant that labels short product reviews as </span></span>
<span class="line"><span>Positive, Negative, or Neutral.</span></span>
<span class="line"><span></span></span>
<span class="line"><span># Instructions</span></span>
<span class="line"><span></span></span>
<span class="line"><span>* Only output a single word in your response with no additional formatting</span></span>
<span class="line"><span>  or commentary.</span></span>
<span class="line"><span>* Your response should only be one of the words &quot;Positive&quot;, &quot;Negative&quot;, or</span></span>
<span class="line"><span>  &quot;Neutral&quot; depending on the sentiment of the product review you are given.</span></span>
<span class="line"><span></span></span>
<span class="line"><span># Examples</span></span>
<span class="line"><span></span></span>
<span class="line"><span>&lt;product_review id=&quot;example-1&quot;&gt;</span></span>
<span class="line"><span>I absolutely love this headphones — sound quality is amazing!</span></span>
<span class="line"><span>&lt;/product_review&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>&lt;assistant_response id=&quot;example-1&quot;&gt;</span></span>
<span class="line"><span>Positive</span></span>
<span class="line"><span>&lt;/assistant_response&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>&lt;product_review id=&quot;example-2&quot;&gt;</span></span>
<span class="line"><span>Battery life is okay, but the ear pads feel cheap.</span></span>
<span class="line"><span>&lt;/product_review&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>&lt;assistant_response id=&quot;example-2&quot;&gt;</span></span>
<span class="line"><span>Neutral</span></span>
<span class="line"><span>&lt;/assistant_response&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>&lt;product_review id=&quot;example-3&quot;&gt;</span></span>
<span class="line"><span>Terrible customer service, I&#39;ll never buy from them again.</span></span>
<span class="line"><span>&lt;/product_review&gt;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>&lt;assistant_response id=&quot;example-3&quot;&gt;</span></span>
<span class="line"><span>Negative</span></span>
<span class="line"><span>&lt;/assistant_response&gt;</span></span></code></pre></div><h2 id="include-relevant-context-information" tabindex="-1">Include relevant context information <a class="header-anchor" href="#include-relevant-context-information" aria-label="Permalink to &quot;Include relevant context information&quot;">​</a></h2><p>It is often useful to include additional context information the model can use to generate a response within the prompt you give the model. There are a few common reasons why you might do this:</p><ul><li>To give the model access to proprietary data, or any other data outside the data set the model was trained on.</li><li>To constrain the model&#39;s response to a specific set of resources that you have determined will be most beneficial.</li></ul><p>The technique of adding additional relevant context to the model generation request is sometimes called <strong>retrieval-augmented generation (RAG)</strong>. You can add additional context to the prompt in many different ways, from querying a vector database and including the text you get back into a prompt, or by using OpenAI&#39;s built-in <a href="/docs/openai/guides/tools-file-search.html">file search tool</a> to generate content based on uploaded documents.</p><h4 id="planning-for-the-context-window" tabindex="-1">Planning for the context window <a class="header-anchor" href="#planning-for-the-context-window" aria-label="Permalink to &quot;Planning for the context window&quot;">​</a></h4><p>Models can only handle so much data within the context they consider during a generation request. This memory limit is called a <strong>context window</strong>, which is defined in terms of <a href="https://blogs.nvidia.com/blog/ai-tokens-explained" target="_blank" rel="noreferrer">tokens</a> (chunks of data you pass in, from text to images).</p><p>Models have different context window sizes from the low 100k range up to one million tokens for newer GPT-4.1 models. <a href="/docs/models.html">Refer to the model docs</a> for specific context window sizes per model.</p><h2 id="prompting-gpt-4-1-models" tabindex="-1">Prompting GPT-4.1 models <a class="header-anchor" href="#prompting-gpt-4-1-models" aria-label="Permalink to &quot;Prompting GPT-4.1 models&quot;">​</a></h2><p>GPT models like <a href="/docs/models/gpt-4.1.html"><code>gpt-4.1</code></a> benefit from precise instructions that explicitly provide the logic and data required to complete the task in the prompt. GPT-4.1 in particular is highly steerable and responsive to well-specified prompts. To get the most out of GPT-4.1, refer to the prompting guide in the cookbook.</p><p>[</p><p>GPT-4.1 prompting guide</p><p>Get the most out of prompting GPT-4.1 with the tips and tricks in this prompting guide, extracted from real-world use cases and practical experience.</p><p>](<a href="https://cookbook.openai.com/examples/gpt4-1_prompting_guide" target="_blank" rel="noreferrer">https://cookbook.openai.com/examples/gpt4-1_prompting_guide</a>)</p><h4 id="gpt-4-1-prompting-best-practices" tabindex="-1">GPT-4.1 prompting best practices <a class="header-anchor" href="#gpt-4-1-prompting-best-practices" aria-label="Permalink to &quot;GPT-4.1 prompting best practices&quot;">​</a></h4><p>While the <a href="https://cookbook.openai.com/examples/gpt4-1_prompting_guide" target="_blank" rel="noreferrer">cookbook</a> has the best and most comprehensive guidance for prompting this model, here are a few best practices to keep in mind.</p><p>Building agentic workflows</p><h3 id="system-prompt-reminders" tabindex="-1">System Prompt Reminders <a class="header-anchor" href="#system-prompt-reminders" aria-label="Permalink to &quot;System Prompt Reminders&quot;">​</a></h3><p>In order to best utilize the agentic capabilities of GPT-4.1, we recommend including three key types of reminders in all agent prompts for persistence, tool calling, and planning. As a whole, we find that these three instructions transform the model&#39;s behavior from chatbot-like into a much more “eager” agent, driving the interaction forward autonomously and independently. Here are a few examples:</p><div class="language-text vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">text</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>## PERSISTENCE</span></span>
<span class="line"><span>You are an agent - please keep going until the user&#39;s query is completely </span></span>
<span class="line"><span>resolved, before ending your turn and yielding back to the user. Only </span></span>
<span class="line"><span>terminate your turn when you are sure that the problem is solved.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>## TOOL CALLING</span></span>
<span class="line"><span>If you are not sure about file content or codebase structure pertaining to </span></span>
<span class="line"><span>the user&#39;s request, use your tools to read files and gather the relevant </span></span>
<span class="line"><span>information: do NOT guess or make up an answer.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>## PLANNING</span></span>
<span class="line"><span>You MUST plan extensively before each function call, and reflect </span></span>
<span class="line"><span>extensively on the outcomes of the previous function calls. DO NOT do this </span></span>
<span class="line"><span>entire process by making function calls only, as this can impair your </span></span>
<span class="line"><span>ability to solve the problem and think insightfully.</span></span></code></pre></div><h4 id="tool-calls" tabindex="-1">Tool Calls <a class="header-anchor" href="#tool-calls" aria-label="Permalink to &quot;Tool Calls&quot;">​</a></h4><p>Compared to previous models, GPT-4.1 has undergone more training on effectively utilizing tools passed as arguments in an OpenAI API request. We encourage developers to exclusively use the tools field of API requests to pass tools for best understanding and performance, rather than manually injecting tool descriptions into the system prompt and writing a separate parser for tool calls, as some have reported doing in the past.</p><h4 id="diff-generation" tabindex="-1">Diff Generation <a class="header-anchor" href="#diff-generation" aria-label="Permalink to &quot;Diff Generation&quot;">​</a></h4><p>Correct diffs are critical for coding applications, so we&#39;ve significantly improved performance at this task for GPT-4.1. In our cookbook, we open-source a recommended diff format on which GPT-4.1 has been extensively trained. That said, the model should generalize to any well-specified format.</p><p>Using long context</p><p>GPT-4.1 has a performant 1M token input context window, and will be useful for a variety of long context tasks, including structured document parsing, re-ranking, selecting relevant information while ignoring irrelevant context, and performing multi-hop reasoning using context.</p><h4 id="optimal-context-size" tabindex="-1">Optimal Context Size <a class="header-anchor" href="#optimal-context-size" aria-label="Permalink to &quot;Optimal Context Size&quot;">​</a></h4><p>We show perfect performance at needle-in-a-haystack evals up to our full context size, and we&#39;ve observed very strong performance at complex tasks with a mix of relevant and irrelevant code and documents in the range of hundreds of thousands of tokens.</p><h4 id="delimiters" tabindex="-1">Delimiters <a class="header-anchor" href="#delimiters" aria-label="Permalink to &quot;Delimiters&quot;">​</a></h4><p>We tested a variety of delimiters for separating context provided to the model against our long context evals. Briefly, XML and the format demonstrated by Lee et al. (<a href="https://arxiv.org/pdf/2406.13121" target="_blank" rel="noreferrer">ref</a>) tend to perform well, while JSON performed worse for this task. See our cookbook for prompt examples.</p><h4 id="prompt-organization" tabindex="-1">Prompt Organization <a class="header-anchor" href="#prompt-organization" aria-label="Permalink to &quot;Prompt Organization&quot;">​</a></h4><p>Especially in long context usage, placement of instructions and context can substantially impact performance. In our experiments, we found that it was optimal to put critical instructions, including the user query, at both the top and the bottom of the prompt; this elicited marginally better performance from the model than putting them only at the top, and much better performance than only at the bottom.</p><p>Prompting for chain of thought</p><p>As mentioned above, GPT-4.1 isn’t a reasoning model, but prompting the model to think step by step (called “chain of thought”) can be an effective way for a model to break down problems into more manageable pieces. The model has been trained to perform well at agentic reasoning and real-world problem solving, so it shouldn’t require much prompting to do well.</p><p>We recommend starting with this basic chain-of-thought instruction at the end of your prompt:</p><div class="language-text vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">text</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>First, think carefully step by step about what documents are needed to answer the query. Then, print out the TITLE and ID of each document. Then, format the IDs into a list.</span></span></code></pre></div><p>From there, you should improve your CoT prompt by auditing failures in your particular examples and evals, and addressing systematic planning and reasoning errors with more explicit instructions. See our cookbook for a prompt example demonstrating a more opinionated reasoning strategy.</p><p>Instruction following</p><p>GPT-4.1 exhibits outstanding instruction-following performance, which developers can leverage to precisely shape and control the outputs for their particular use cases. However, since the model follows instructions more literally than its predecessors, may need to provide more explicit specification around what to do or not do, and existing prompts optimized for other models may not immediately work with this model.</p><h4 id="recommended-workflow" tabindex="-1">Recommended Workflow <a class="header-anchor" href="#recommended-workflow" aria-label="Permalink to &quot;Recommended Workflow&quot;">​</a></h4><p>Here is our recommended workflow for developing and debugging instructions in prompts:</p><ul><li>Start with an overall “Response Rules” or “Instructions” section with high-level guidance and bullet points.</li><li>If you&#39;d like to change a more specific behavior, add a section containing more details for that category, like <code>## Sample Phrases</code>.</li><li>If there are specific steps you&#39;d like the model to follow in its workflow, add an ordered list and instruct the model to follow these steps.</li><li>If behavior still isn&#39;t working as expected, check for conflicting, underspecified, or incorrect\` instructions and examples. If there are conflicting instructions, GPT-4.1 tends to follow the one closer to the end of the prompt.</li><li>Add examples that demonstrate desired behavior; ensure that any important behavior demonstrated in your examples are also cited in your rules.</li><li>It&#39;s generally not necessary to use all-caps or other incentives like bribes or tips, but developers can experiment with this for extra emphasis if so desired.</li></ul><h4 id="common-failure-modes" tabindex="-1">Common Failure Modes <a class="header-anchor" href="#common-failure-modes" aria-label="Permalink to &quot;Common Failure Modes&quot;">​</a></h4><p>These failure modes are not unique to GPT-4.1, but we share them here for general awareness and ease of debugging.</p><ul><li>Instructing a model to always follow a specific behavior can occasionally induce adverse effects. For instance, if told “you must call a tool before responding to the user,” models may hallucinate tool inputs or call the tool with null values if they do not have enough information. Adding “if you don’t have enough information to call the tool, ask the user for the information you need” should mitigate this.</li><li>When provided sample phrases, models can use those quotes verbatim and start to sound repetitive to users. Ensure you instruct the model to vary them as necessary.</li><li>Without specific instructions, some models can be eager to provide additional prose to explain their decisions, or output more formatting in responses than may be desired. Provide instructions and potentially examples to help mitigate.</li></ul><p>See our cookbook for an example customer service prompt that demonstrates these principles.</p><h2 id="prompting-reasoning-models" tabindex="-1">Prompting reasoning models <a class="header-anchor" href="#prompting-reasoning-models" aria-label="Permalink to &quot;Prompting reasoning models&quot;">​</a></h2><p>There are some differences to consider when prompting a <a href="/docs/openai/guides/reasoning.html">reasoning model</a> versus prompting a GPT model. Generally speaking, reasoning models will provide better results on tasks with only high-level guidance. This differs from GPT models, which benefit from very precise instructions.</p><p>You could think about the difference between reasoning and GPT models like this.</p><ul><li>A reasoning model is like a senior co-worker. You can give them a goal to achieve and trust them to work out the details.</li><li>A GPT model is like a junior coworker. They&#39;ll perform best with explicit instructions to create a specific output.</li></ul><p>For more information on best practices when using reasoning models, <a href="/docs/openai/guides/reasoning-best-practices.html">refer to this guide</a>.</p><h2 id="next-steps" tabindex="-1">Next steps <a class="header-anchor" href="#next-steps" aria-label="Permalink to &quot;Next steps&quot;">​</a></h2><p>Now that you known the basics of text inputs and outputs, you might want to check out one of these resources next.</p><p>[</p><p>Build a prompt in the Playground</p><p>Use the Playground to develop and iterate on prompts.</p><p>](/playground)[</p><p>Generate JSON data with Structured Outputs</p><p>Ensure JSON data emitted from a model conforms to a JSON schema.</p><p>](/docs/openai/guides/structured-outputs)[</p><p>Full API reference</p><p>Check out all the options for text generation in the API reference.</p><p>](/docs/api-reference/responses)</p>`,114)]))}const g=a(t,[["render",l]]);export{c as __pageData,g as default};
