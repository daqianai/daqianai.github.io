<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Images and vision | DaQianAI</title>
    <meta name="description" content="The ultimate AI">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/assets/style.qp7qGaqN.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.4tLL03f4.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.7nXTpekp.js">
    <link rel="modulepreload" href="/assets/chunks/framework.BDwTZuFy.js">
    <link rel="modulepreload" href="/assets/docs_openai_guides_images-vision.md.kT8cY6sA.lean.js">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-d8b57b2d><!--[--><!--]--><!--[--><span tabindex="-1" data-v-fcbfc0e0></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-fcbfc0e0>Skip to content</a><!--]--><!----><header class="VPNav" data-v-d8b57b2d data-v-7ad780c2><div class="VPNavBar" data-v-7ad780c2 data-v-9fd4d1dd><div class="wrapper" data-v-9fd4d1dd><div class="container" data-v-9fd4d1dd><div class="title" data-v-9fd4d1dd><div class="VPNavBarTitle has-sidebar" data-v-9fd4d1dd data-v-9f43907a><a class="title" href="/" data-v-9f43907a><!--[--><!--]--><!----><span data-v-9f43907a>DaQianAI</span><!--[--><!--]--></a></div></div><div class="content" data-v-9fd4d1dd><div class="content-body" data-v-9fd4d1dd><!--[--><!--]--><div class="VPNavBarSearch search" data-v-9fd4d1dd><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-9fd4d1dd data-v-afb2845e><span id="main-nav-aria-label" class="visually-hidden" data-v-afb2845e> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/" tabindex="0" data-v-afb2845e data-v-815115f5><!--[--><span data-v-815115f5>Home</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-afb2845e data-v-bfe7971f><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-bfe7971f><span class="text" data-v-bfe7971f><!----><span data-v-bfe7971f>APP</span><span class="vpi-chevron-down text-icon" data-v-bfe7971f></span></span></button><div class="menu" data-v-bfe7971f><div class="VPMenu" data-v-bfe7971f data-v-20ed86d6><div class="items" data-v-20ed86d6><!--[--><!--[--><div class="VPMenuLink" data-v-20ed86d6 data-v-7eeeb2dc><a class="VPLink link vp-external-link-icon" href="https://deepwiki.com" target="_blank" rel="noreferrer" data-v-7eeeb2dc><!--[--><span data-v-7eeeb2dc>deepwiki</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/tutorial.html" tabindex="0" data-v-afb2845e data-v-815115f5><!--[--><span data-v-815115f5>Prompt</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-afb2845e data-v-bfe7971f><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-bfe7971f><span class="text" data-v-bfe7971f><!----><span data-v-bfe7971f>MCP</span><span class="vpi-chevron-down text-icon" data-v-bfe7971f></span></span></button><div class="menu" data-v-bfe7971f><div class="VPMenu" data-v-bfe7971f data-v-20ed86d6><div class="items" data-v-20ed86d6><!--[--><!--[--><div class="VPMenuLink" data-v-20ed86d6 data-v-7eeeb2dc><a class="VPLink link vp-external-link-icon" href="https://modelcontextprotocol.io/introduction" target="_blank" rel="noreferrer" data-v-7eeeb2dc><!--[--><span data-v-7eeeb2dc>Protocol</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-20ed86d6 data-v-7eeeb2dc><a class="VPLink link vp-external-link-icon" href="https://deepwiki.com/modelcontextprotocol/python-sdk/" target="_blank" rel="noreferrer" data-v-7eeeb2dc><!--[--><span data-v-7eeeb2dc>Python-SDK</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/tutorial.html" tabindex="0" data-v-afb2845e data-v-815115f5><!--[--><span data-v-815115f5>Tutorial</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-afb2845e data-v-bfe7971f><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-bfe7971f><span class="text" data-v-bfe7971f><!----><span data-v-bfe7971f>Docs</span><span class="vpi-chevron-down text-icon" data-v-bfe7971f></span></span></button><div class="menu" data-v-bfe7971f><div class="VPMenu" data-v-bfe7971f data-v-20ed86d6><div class="items" data-v-20ed86d6><!--[--><!--[--><div class="VPMenuLink" data-v-20ed86d6 data-v-7eeeb2dc><a class="VPLink link" href="/docs/openai/guides.html" data-v-7eeeb2dc><!--[--><span data-v-7eeeb2dc>OpenAI</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-20ed86d6 data-v-7eeeb2dc><a class="VPLink link" href="/docs/anthropic.html" data-v-7eeeb2dc><!--[--><span data-v-7eeeb2dc>Anthropic</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-20ed86d6 data-v-7eeeb2dc><a class="VPLink link" href="/docs/examples/vitepress.html" data-v-7eeeb2dc><!--[--><span data-v-7eeeb2dc>Vitepress</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-afb2845e data-v-bfe7971f><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-bfe7971f><span class="text" data-v-bfe7971f><!----><span data-v-bfe7971f>About</span><span class="vpi-chevron-down text-icon" data-v-bfe7971f></span></span></button><div class="menu" data-v-bfe7971f><div class="VPMenu" data-v-bfe7971f data-v-20ed86d6><div class="items" data-v-20ed86d6><!--[--><!--[--><div class="VPMenuLink" data-v-20ed86d6 data-v-7eeeb2dc><a class="VPLink link" href="/about/team.html" data-v-7eeeb2dc><!--[--><span data-v-7eeeb2dc>Team</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-9fd4d1dd data-v-3f90c1a5><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-3f90c1a5 data-v-be9742d9 data-v-b4ccac88><span class="check" data-v-b4ccac88><span class="icon" data-v-b4ccac88><!--[--><span class="vpi-sun sun" data-v-be9742d9></span><span class="vpi-moon moon" data-v-be9742d9></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-9fd4d1dd data-v-ef6192dc data-v-e71e869c><!--[--><a class="VPSocialLink no-icon" href="/about/team" aria-label="wechat" target="_blank" rel="noopener" data-v-e71e869c data-v-60a9a2d3><span class="vpi-social-wechat"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-9fd4d1dd data-v-f953d92f data-v-bfe7971f><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-bfe7971f><span class="vpi-more-horizontal icon" data-v-bfe7971f></span></button><div class="menu" data-v-bfe7971f><div class="VPMenu" data-v-bfe7971f data-v-20ed86d6><!----><!--[--><!--[--><!----><div class="group" data-v-f953d92f><div class="item appearance" data-v-f953d92f><p class="label" data-v-f953d92f>Appearance</p><div class="appearance-action" data-v-f953d92f><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-f953d92f data-v-be9742d9 data-v-b4ccac88><span class="check" data-v-b4ccac88><span class="icon" data-v-b4ccac88><!--[--><span class="vpi-sun sun" data-v-be9742d9></span><span class="vpi-moon moon" data-v-be9742d9></span><!--]--></span></span></button></div></div></div><div class="group" data-v-f953d92f><div class="item social-links" data-v-f953d92f><div class="VPSocialLinks social-links-list" data-v-f953d92f data-v-e71e869c><!--[--><a class="VPSocialLink no-icon" href="/about/team" aria-label="wechat" target="_blank" rel="noopener" data-v-e71e869c data-v-60a9a2d3><span class="vpi-social-wechat"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-9fd4d1dd data-v-6bee1efd><span class="container" data-v-6bee1efd><span class="top" data-v-6bee1efd></span><span class="middle" data-v-6bee1efd></span><span class="bottom" data-v-6bee1efd></span></span></button></div></div></div></div><div class="divider" data-v-9fd4d1dd><div class="divider-line" data-v-9fd4d1dd></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-d8b57b2d data-v-2488c25a><div class="container" data-v-2488c25a><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-2488c25a><span class="vpi-align-left menu-icon" data-v-2488c25a></span><span class="menu-text" data-v-2488c25a>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-2488c25a data-v-6b867909><button data-v-6b867909>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-d8b57b2d data-v-42c4c606><div class="curtain" data-v-42c4c606></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-42c4c606><span class="visually-hidden" id="sidebar-aria-label" data-v-42c4c606> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>GET STARTED</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/quickstart.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Quickstart</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/quickstart-chat.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Quickstart(chat)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/libraries.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Libraries</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0 has-active" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>CORE CONCEPTS</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/text.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Text and prompting</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/text-chat.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Text and prompting(chat)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/images-vision.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Images and vision</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/images-vision-chat.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Images and vision(chat)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/audio.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Audio and speech</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/structured-outputs.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Structured Outputs</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/structured-outputs-chat.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Structured Outputs(chat)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/function-calling.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Function calling</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/function-calling-chat.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Function calling(chat)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/conversation-state.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Conversation state</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/conversation-state-chat.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Conversation state(chat)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/streaming-responses.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Streaming</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/streaming-responses-chat.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Streaming(chat)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/pdf-files.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>File inputs</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/pdf-files-chat.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>File inputs(chat)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/reasoning.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Reasoning</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/reasoning-chat.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Reasoning(chat)</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>BUILT-IN TOOLS</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/tools.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Using built-in tools</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/tools-web-search.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Web search</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/tools-file-search.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>File search</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/tools-computer-use.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Computer use</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>AGENTS</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/agents.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Building agents</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/voice-agents.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Voice agents</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/voice-agents-chained.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Voice agents(chained)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link vp-external-link-icon link" href="https://openai.github.io/openai-agents-python/" target="_blank" rel="noreferrer" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Agents SDK</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>REALTIME API</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/realtime.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Using the Realtime API</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/realtime-conversations.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Realtime conversations</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/realtime-transcription.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Realtime transcription</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/realtime-vad.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Voice activity detection</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>MODEL OPTIMIZATION</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/model-optimization.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Overview</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/evals.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Evals</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/fine-tuning.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Fine-tuning</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/supervised-fine-tuning.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Supervised fine-tuning</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/vision-fine-tuning.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Vision fine-tuning</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/direct-preference-optimization.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Direct preference optimization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/reinforcement-fine-tuning.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Reinforcement fine-tuning</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/graders.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Graders</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/distillation.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Distillation</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>SPECIALIZED MODELS</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/image-generation.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Image generation</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/text-to-speech.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Text to speech</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/speech-to-text.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Speech to text</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/embeddings.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Embeddings</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/moderation.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Moderation</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>OPENAI PLATFORM</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/retrieval.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Retrieval</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/batch.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Batch</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/prompt-generation.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Prompt generation</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>CODEX</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/codex.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Codex</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/tools-local-shell.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Local shell tool</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link vp-external-link-icon link" href="https://github.com/openai/codex" target="_blank" rel="noreferrer" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Codex CLI</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>BEST PRACTICES</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/production-best-practices.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Production best practices</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/safety-best-practices.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Safety best practices</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/prompt-caching.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Prompt Caching</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/predicted-outputs.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Predicted Outputs</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/reasoning-best-practices.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Reasoning best practices</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/evals-design.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Evals design</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/fine-tuning-best-practices.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Fine-tuning best practices</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/rft-use-cases.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Reinforcement fine-tuning use cases</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/model-selection.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Model selection</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/latency-optimization.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Latency optimization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/accuracy-optimization.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Accuracy optimization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/advanced-usage.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Advanced usage</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/responses-vs-chat-completions.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Responses vs. Chat Completions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/flex-processing.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Flex processing</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>ASSISTANTS API</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/assistants/overview.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Overview</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/assistants/quickstart.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Quickstart</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/assistants/deep-dive.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Deep dive</p><!--]--></a><!----></div><!----></div><section class="VPSidebarItem level-1 collapsible collapsed" data-v-0009425e data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h3 class="text" data-v-0009425e>Tools</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-0009425e><span class="vpi-chevron-right caret-icon" data-v-0009425e></span></div></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-2 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/assistants/tools/file-search.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>File Search</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/assistants/tools/code-interpreter.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Code interpreter</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/assistants/tools/function-calling.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Function calling</p><!--]--></a><!----></div><!----></div><!--]--></div></section><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/assistants/whats-new.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>What's new</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>RESOURCES</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link vp-external-link-icon link" href="https://openai.com/policies/" target="_blank" rel="noreferrer" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Terms and policies</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link vp-external-link-icon link" href="https://platform.openai.com/docs/changelog" target="_blank" rel="noreferrer" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Changelog</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/your-data.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Your data</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/rate-limits.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Rate limits</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/deprecations.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Deprecations</p><!--]--></a><!----></div><!----></div><section class="VPSidebarItem level-1 collapsible collapsed" data-v-0009425e data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h3 class="text" data-v-0009425e>ChatGPT Actions</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-0009425e><span class="vpi-chevron-right caret-icon" data-v-0009425e></span></div></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-2 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/actions/introduction.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Introduction</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/actions/getting-started.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Getting started</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/actions/actions-library.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Actions library</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/actions/authentication.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Authentication</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/actions/production.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Production</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/actions/data-retrieval.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Data retrieval</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/actions/sending-files.html" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Sending files</p><!--]--></a><!----></div><!----></div><!--]--></div></section><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><!----><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link vp-external-link-icon link" href="https://cookbook.openai.com/" target="_blank" rel="noreferrer" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Cookbook</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link vp-external-link-icon link" href="https://community.openai.com/categories" target="_blank" rel="noreferrer" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Forum</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1" data-v-0009425e data-v-0009425e><div class="item" role="button" data-v-0009425e><div class="indicator" data-v-0009425e></div><p class="text" data-v-0009425e>Help</p><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-d8b57b2d data-v-9a6c75ad><div class="VPDoc has-sidebar has-aside" data-v-9a6c75ad data-v-e6f2a212><!--[--><!--]--><div class="container" data-v-e6f2a212><div class="aside" data-v-e6f2a212><div class="aside-curtain" data-v-e6f2a212></div><div class="aside-container" data-v-e6f2a212><div class="aside-content" data-v-e6f2a212><div class="VPDocAside" data-v-e6f2a212 data-v-cb998dce><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-cb998dce data-v-f610f197><div class="content" data-v-f610f197><div class="outline-marker" data-v-f610f197></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-f610f197>On this page</div><ul class="VPDocOutlineItem root" data-v-f610f197 data-v-53c99d69><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-cb998dce></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-e6f2a212><div class="content-container" data-v-e6f2a212><!--[--><!--]--><main class="main" data-v-e6f2a212><div style="position:relative;" class="vp-doc _docs_openai_guides_images-vision" data-v-e6f2a212><div><h1 id="images-and-vision" tabindex="-1">Images and vision <a class="header-anchor" href="#images-and-vision" aria-label="Permalink to &quot;Images and vision&quot;">​</a></h1><p>Learn how to understand or generate images.</p><h2 id="overview" tabindex="-1">Overview <a class="header-anchor" href="#overview" aria-label="Permalink to &quot;Overview&quot;">​</a></h2><p>[</p><p><img src="https://cdn.openai.com/API/docs/images/images.png" alt="Create images"></p><p>Create images</p><p>Use GPT Image or DALL·E to generate or edit images.</p><p>](/docs/openai/guides/image-generation)[</p><p><img src="https://cdn.openai.com/API/docs/images/vision.png" alt="Process image inputs"></p><p>Process image inputs</p><p>Use our models&#39; vision capabilities to analyze images.</p><p>](/docs/openai/guides/images-vision#analyze-images)</p><p>In this guide, you will learn about building applications involving images with the OpenAI API. If you know what you want to build, find your use case below to get started. If you&#39;re not sure where to start, continue reading to get an overview.</p><h3 id="a-tour-of-image-related-use-cases" tabindex="-1">A tour of image-related use cases <a class="header-anchor" href="#a-tour-of-image-related-use-cases" aria-label="Permalink to &quot;A tour of image-related use cases&quot;">​</a></h3><p>Recent language models can process image inputs and analyze them — a capability known as <strong>vision</strong>. With <code>gpt-image-1</code>, they can both analyze visual inputs and create images.</p><p>The OpenAI API offers several endpoints to process images as input or generate them as output, enabling you to build powerful multimodal applications.</p><table tabindex="0"><thead><tr><th>API</th><th>Supported use cases</th></tr></thead><tbody><tr><td>Chat Completions API</td><td>Analyze images and use them as input to generate text or audio</td></tr><tr><td>Responses API</td><td>Analyze images and use them as input to generate text - image generation support coming soon</td></tr><tr><td>Images API</td><td>Generate images as output, optionally using images as input</td></tr></tbody></table><p>To learn more about the input and output modalities supported by our models, refer to our <a href="/docs/models.html">models page</a>.</p><h2 id="generate-or-edit-images" tabindex="-1">Generate or edit images <a class="header-anchor" href="#generate-or-edit-images" aria-label="Permalink to &quot;Generate or edit images&quot;">​</a></h2><p>You can generate or edit images using the Images API (support via the Responses API is coming soon).</p><p>Our latest image generation model, <code>gpt-image-1</code>, is a natively multimodal large language model. It can understand text and images and leverage its broad world knowledge to generate images with better instruction following and contextual awareness.</p><p>In constrast, we also offer specialized image generation models - DALL·E 2 and 3 - which don&#39;t have the same inherent understanding of the world as GPT Image.</p><p>You can learn more about image generation in our <a href="/docs/openai/guides/image-generation.html">Image generation</a> guide.</p><h3 id="using-world-knowledge-for-image-generation" tabindex="-1">Using world knowledge for image generation <a class="header-anchor" href="#using-world-knowledge-for-image-generation" aria-label="Permalink to &quot;Using world knowledge for image generation&quot;">​</a></h3><p>The difference between DALL·E models and GPT Image is that a natively multimodal language model can use its visual understanding of the world to generate lifelike images including real-life details without a reference.</p><p>For example, if you prompt GPT Image to generate an image of a glass cabinet with the most popular semi-precious stones, the model knows enough to select gemstones like amethyst, rose quartz, jade, etc, and depict them in a realistic way.</p><h2 id="analyze-images" tabindex="-1">Analyze images <a class="header-anchor" href="#analyze-images" aria-label="Permalink to &quot;Analyze images&quot;">​</a></h2><p><strong>Vision</strong> is the ability for a model to &quot;see&quot; and understand images. If there is text in an image, the model can also understand the text. It can understand most visual elements, including objects, shapes, colors, and textures, even if there are some <a href="#limitations">limitations</a>.</p><h3 id="giving-a-model-images-as-input" tabindex="-1">Giving a model images as input <a class="header-anchor" href="#giving-a-model-images-as-input" aria-label="Permalink to &quot;Giving a model images as input&quot;">​</a></h3><p>You can provide images as input to generation requests either by providing a fully qualified URL to an image file, or providing an image as a Base64-encoded data URL.</p><p>You can provide multiple images as input in a single request by including multiple images in the <code>content</code> array, but keep in mind that <a href="#calculating-costs">images count as tokens</a> and will be billed accordingly.</p><p>Passing a URL</p><p>Analyze the content of an image</p><div class="language-javascript vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">javascript</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;openai&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> openai</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> new</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> OpenAI</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">();</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> response</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> openai.responses.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">create</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">({</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    model: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;gpt-4.1-mini&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    input: [{</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        role: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;user&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        content: [</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            { type: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;input_text&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, text: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;what&#39;s in this image?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            {</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">                type: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;input_image&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">                image_url: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        ],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    }],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">});</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">console.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">log</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(response.output_text);</span></span></code></pre></div><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> openai </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">client </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">response </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> client.responses.create(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;gpt-4.1-mini&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;user&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            {</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;type&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;input_text&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;text&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;what&#39;s in this image?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">},</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">                &quot;type&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;input_image&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">                &quot;image_url&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        ],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    }],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(response.output_text)</span></span></code></pre></div><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">curl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://api.openai.com/v1/responses</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  -H</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;Content-Type: application/json&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  -H</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;Authorization: Bearer </span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">$OPENAI_API_KEY</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  -d</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;model&quot;: &quot;gpt-4.1-mini&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;input&quot;: [</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">      {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;role&quot;: &quot;user&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        &quot;content&quot;: [</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">          {&quot;type&quot;: &quot;input_text&quot;, &quot;text&quot;: &quot;what is in this image?&quot;},</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">          {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;type&quot;: &quot;input_image&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;image_url&quot;: &quot;https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">          }</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">        ]</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">      }</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    ]</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  }&#39;</span></span></code></pre></div><p>Passing a Base64 encoded image</p><p>Analyze the content of an image</p><div class="language-javascript vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">javascript</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> fs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;fs&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;openai&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> openai</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> new</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> OpenAI</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">();</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> imagePath</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;path_to_your_image.jpg&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> base64Image</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> fs.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">readFileSync</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(imagePath, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;base64&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">);</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> response</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> openai.responses.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">create</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">({</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    model: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;gpt-4.1-mini&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    input: [</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        {</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            role: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;user&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            content: [</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">                { type: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;input_text&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, text: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;what&#39;s in this image?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">                {</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">                    type: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;input_image&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">                    image_url: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">`data:image/jpeg;base64,${</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">base64Image</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">}`</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">                },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            ],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    ],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">});</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">console.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">log</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(response.output_text);</span></span></code></pre></div><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> base64</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> openai </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">client </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Function to encode the image</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> encode_image</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(image_path):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    with</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> open</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(image_path, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;rb&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> image_file:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> base64.b64encode(image_file.read()).decode(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;utf-8&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Path to your image</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">image_path </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;path_to_your_image.jpg&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Getting the Base64 string</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">base64_image </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> encode_image(image_path)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">response </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> client.responses.create(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;gpt-4.1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;user&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">                { </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;type&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;input_text&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;text&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;what&#39;s in this image?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">                {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">                    &quot;type&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;input_image&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">                    &quot;image_url&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;data:image/jpeg;base64,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">base64_image</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">                },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            ],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    ],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(response.output_text)</span></span></code></pre></div><h3 id="image-input-requirements" tabindex="-1">Image input requirements <a class="header-anchor" href="#image-input-requirements" aria-label="Permalink to &quot;Image input requirements&quot;">​</a></h3><p>Input images must meet the following requirements to be used in the API.</p><p>|Supported file types|PNG (.png)JPEG (.jpeg and .jpg)WEBP (.webp)Non-animated GIF (.gif)| |Size limits|Up to 20MB per imageLow-resolution: 512px x 512pxHigh-resolution: 768px (short side) x 2000px (long side)| |Other requirements|No watermarks or logosNo textNo NSFW contentClear enough for a human to understand|</p><h3 id="specify-image-input-detail-level" tabindex="-1">Specify image input detail level <a class="header-anchor" href="#specify-image-input-detail-level" aria-label="Permalink to &quot;Specify image input detail level&quot;">​</a></h3><p>The <code>detail</code> parameter tells the model what level of detail to use when processing and understanding the image (<code>low</code>, <code>high</code>, or <code>auto</code> to let the model decide). If you skip the parameter, the model will use <code>auto</code>.</p><div class="language-plain vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">plain</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>{</span></span>
<span class="line"><span>    &quot;type&quot;: &quot;input_image&quot;,</span></span>
<span class="line"><span>    &quot;image_url&quot;: &quot;https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg&quot;,</span></span>
<span class="line"><span>    &quot;detail&quot;: &quot;high&quot;</span></span>
<span class="line"><span>}</span></span></code></pre></div><p>You can save tokens and speed up responses by using <code>&quot;detail&quot;: &quot;low&quot;</code>. This lets the model process the image with a budget of 85 tokens. The model receives a low-resolution 512px x 512px version of the image. This is fine if your use case doesn&#39;t require the model to see with high-resolution detail (for example, if you&#39;re asking about the dominant shape or color in the image).</p><p>On the other hand, you can use <code>&quot;detail&quot;: &quot;high&quot;</code> if you want the model to have a better understanding of the image.</p><p>Read more about calculating image processing costs in the <a href="#calculating-costs">Calculating costs</a> section below.</p><h2 id="limitations" tabindex="-1">Limitations <a class="header-anchor" href="#limitations" aria-label="Permalink to &quot;Limitations&quot;">​</a></h2><p>While models with vision capabilities are powerful and can be used in many situations, it&#39;s important to understand the limitations of these models. Here are some known limitations:</p><ul><li><strong>Medical images</strong>: The model is not suitable for interpreting specialized medical images like CT scans and shouldn&#39;t be used for medical advice.</li><li><strong>Non-English</strong>: The model may not perform optimally when handling images with text of non-Latin alphabets, such as Japanese or Korean.</li><li><strong>Small text</strong>: Enlarge text within the image to improve readability, but avoid cropping important details.</li><li><strong>Rotation</strong>: The model may misinterpret rotated or upside-down text and images.</li><li><strong>Visual elements</strong>: The model may struggle to understand graphs or text where colors or styles—like solid, dashed, or dotted lines—vary.</li><li><strong>Spatial reasoning</strong>: The model struggles with tasks requiring precise spatial localization, such as identifying chess positions.</li><li><strong>Accuracy</strong>: The model may generate incorrect descriptions or captions in certain scenarios.</li><li><strong>Image shape</strong>: The model struggles with panoramic and fisheye images.</li><li><strong>Metadata and resizing</strong>: The model doesn&#39;t process original file names or metadata, and images are resized before analysis, affecting their original dimensions.</li><li><strong>Counting</strong>: The model may give approximate counts for objects in images.</li><li><strong>CAPTCHAS</strong>: For safety reasons, our system blocks the submission of CAPTCHAs.</li></ul><h2 id="calculating-costs" tabindex="-1">Calculating costs <a class="header-anchor" href="#calculating-costs" aria-label="Permalink to &quot;Calculating costs&quot;">​</a></h2><p>Image inputs are metered and charged in tokens, just as text inputs are. How images are converted to text token inputs varies based on the model. You can find a vision pricing calculator in the FAQ section of the <a href="https://openai.com/api/pricing/" target="_blank" rel="noreferrer">pricing page</a>.</p><h3 id="gpt-4-1-mini-gpt-4-1-nano-o4-mini" tabindex="-1">GPT-4.1-mini, GPT-4.1-nano, o4-mini <a class="header-anchor" href="#gpt-4-1-mini-gpt-4-1-nano-o4-mini" aria-label="Permalink to &quot;GPT-4.1-mini, GPT-4.1-nano, o4-mini&quot;">​</a></h3><p>Image inputs are metered and charged in tokens based on their dimensions. The token cost of an image is determined as follows:</p><ul><li>Calculate the number of 32px x 32px patches that are needed to fully cover the image</li><li>If the number of patches exceeds 1536, we scale the image so that it can be covered by no more than 1536 patches.</li><li>The token cost is the number of patches, capped at a maximum of 1536 tokens</li><li>For <code>gpt-4.1-mini</code> we multiply image tokens by 1.62 to get total tokens, for <code>gpt-4.1-nano</code> we multiply image tokens by 2.46 to get total tokens, and for <code>o4-mini</code> we multiply image tokens by 1.72 to get total tokens, that are then billed at normal text token rates.</li></ul><p><strong>Cost calculation examples</strong></p><ul><li>A 1024 x 1024 image is <strong>1024 tokens</strong><ul><li>Width is 1024, resulting in <code>(1024 + 32 - 1) // 32 = 32</code> patches</li><li>Height is 1024, resulting in <code>(1024 + 32 - 1) // 32 = 32</code> patches</li><li>Tokens calculated as <code>32 * 32 = 1024</code>, below the cap of 1536</li></ul></li><li>A 1800 x 2400 image is <strong>1452 tokens</strong><ul><li>Width is 1800, resulting in <code>(1800 + 32 - 1) // 32 = 57</code> patches</li><li>Height is 2400, resulting in <code>(2400 + 32 - 1) // 32 = 75</code> patches</li><li>We need <code>57 * 75 = 4275</code> patches to cover the full image. Since that exceeds 1536, we need to scale down the image while preserving the aspect ratio.</li><li>We can calculate the shrink factor as <code>sqrt(token_budget × patch_size^2 / (width * height))</code>. In our example, the shrink factor is <code>sqrt(1536 * 32^2 / (1800 * 2400)) = 0.603</code>.</li><li>Width is now 1086, resulting in <code>1086 / 32 = 33.94</code> patches</li><li>Height is now 1448, resulting in <code>1448 / 32 = 45.25</code> patches</li><li>We want to make sure the image fits in a whole number of patches. In this case we scale again by <code>33 / 33.94 = 0.97</code> to fit the width in 33 patches.</li><li>The final width is then <code>1086 * (33 / 33.94) = 1056)</code> and the final height is <code>1448 * (33 / 33.94) = 1408</code></li><li>The image now requires <code>1056 / 32 = 33</code> patches to cover the width and <code>1408 / 32 = 44</code> patches to cover the height</li><li>The total number of tokens is the <code>33 * 44 = 1452</code>, below the cap of 1536</li></ul></li></ul><h3 id="gpt-4o-gpt-4-1-gpt-4o-mini-cua-and-o-series-except-o4-mini" tabindex="-1">GPT 4o, GPT-4.1, GPT-4o-mini, CUA, and o-series (except o4-mini) <a class="header-anchor" href="#gpt-4o-gpt-4-1-gpt-4o-mini-cua-and-o-series-except-o4-mini" aria-label="Permalink to &quot;GPT 4o, GPT-4.1, GPT-4o-mini, CUA, and o-series (except o4-mini)&quot;">​</a></h3><p>The token cost of an image is determined by two factors: size and detail.</p><p>Any image with <code>&quot;detail&quot;: &quot;low&quot;</code> costs a set, base number of tokens. This amount varies by model (see charte below). To calculate the cost of an image with <code>&quot;detail&quot;: &quot;high&quot;</code>, we do the following:</p><ul><li>Scale to fit in a 2048px x 2048px square, maintaining original aspect ratio</li><li>Scale so that the image&#39;s shortest side is 768px long</li><li>Count the number of 512px squares in the image—each square costs a set amount of tokens (see chart below)</li><li>Add the base tokens to the total</li></ul><table tabindex="0"><thead><tr><th>Model</th><th>Base tokens</th><th>Tile tokens</th></tr></thead><tbody><tr><td>4o, 4.1, 4.5</td><td>85</td><td>170</td></tr><tr><td>4o-mini</td><td>2833</td><td>5667</td></tr><tr><td>o1, o1-pro, o3</td><td>75</td><td>150</td></tr><tr><td>computer-use-preview</td><td>65</td><td>129</td></tr></tbody></table><p><strong>Cost calculation examples (for gpt-4o)</strong></p><ul><li>A 1024 x 1024 square image in <code>&quot;detail&quot;: &quot;high&quot;</code> mode costs 765 tokens <ul><li>1024 is less than 2048, so there is no initial resize.</li><li>The shortest side is 1024, so we scale the image down to 768 x 768.</li><li>4 512px square tiles are needed to represent the image, so the final token cost is <code>170 * 4 + 85 = 765</code>.</li></ul></li><li>A 2048 x 4096 image in <code>&quot;detail&quot;: &quot;high&quot;</code> mode costs 1105 tokens <ul><li>We scale down the image to 1024 x 2048 to fit within the 2048 square.</li><li>The shortest side is 1024, so we further scale down to 768 x 1536.</li><li>6 512px tiles are needed, so the final token cost is <code>170 * 6 + 85 = 1105</code>.</li></ul></li><li>A 4096 x 8192 image in <code>&quot;detail&quot;: &quot;low&quot;</code> most costs 85 tokens <ul><li>Regardless of input size, low detail images are a fixed cost.</li></ul></li></ul><h3 id="gpt-image-1" tabindex="-1">GPT Image 1 <a class="header-anchor" href="#gpt-image-1" aria-label="Permalink to &quot;GPT Image 1&quot;">​</a></h3><p>For GPT Image 1, we calculate the cost of an image input the same way as described above, except that we scale down the image so that the shortest side is 512px instead of 768px. There is no detail level configuration for this model, so the price depends on the dimensions of the image.</p><p>The base cost is 65 image tokens, and each tile costs 129 image tokens.</p><p>To see pricing for image input tokens, refer to our <a href="/docs/pricing.html#latest-models">pricing page</a>.</p><hr><p>We process images at the token level, so each image we process counts towards your tokens per minute (TPM) limit.</p><p>For the most precise and up-to-date estimates for image processing, please use our image pricing calculator available <a href="https://openai.com/api/pricing/" target="_blank" rel="noreferrer">here</a>.</p></div></div></main><footer class="VPDocFooter" data-v-e6f2a212 data-v-1bcd8184><!--[--><!--]--><div class="edit-info" data-v-1bcd8184><!----><div class="last-updated" data-v-1bcd8184><p class="VPLastUpdated" data-v-1bcd8184 data-v-1bb0c8a8>Last updated: <time datetime="2025-05-24T07:24:21.000Z" data-v-1bb0c8a8></time></p></div></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-1bcd8184><span class="visually-hidden" id="doc-footer-aria-label" data-v-1bcd8184>Pager</span><div class="pager" data-v-1bcd8184><a class="VPLink link pager-link prev" href="/docs/openai/guides/text-chat.html" data-v-1bcd8184><!--[--><span class="desc" data-v-1bcd8184>Previous page</span><span class="title" data-v-1bcd8184>Text and prompting(chat)</span><!--]--></a></div><div class="pager" data-v-1bcd8184><a class="VPLink link pager-link next" href="/docs/openai/guides/images-vision-chat.html" data-v-1bcd8184><!--[--><span class="desc" data-v-1bcd8184>Next page</span><span class="title" data-v-1bcd8184>Images and vision(chat)</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-d8b57b2d data-v-566314d4><div class="container" data-v-566314d4><p class="message" data-v-566314d4>build with <a href="https://github.com/vuejs/vitepress">vitepress</a></p><p class="copyright" data-v-566314d4>Copyright © 2023-present by NOTHING</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"about_team.md\":\"pRwJN7za\",\"docs_anthropic_index.md\":\"BG_GHy5s\",\"docs_anthropic_prompt_generator_atrhopic.md\":\"C8qQzsPK\",\"docs_examples_vitepress_api-examples.md\":\"Dm37ROcB\",\"docs_examples_vitepress_emoji.md\":\"BY6x7ny1\",\"docs_examples_vitepress_index.md\":\"DwRZ01m2\",\"docs_examples_vitepress_markdown-examples.md\":\"Bf8CSez9\",\"docs_openai_actions_actions-library.md\":\"Dy_1-98e\",\"docs_openai_actions_authentication.md\":\"PteK4cw4\",\"docs_openai_actions_data-retrieval.md\":\"COjnQOg2\",\"docs_openai_actions_getting-started.md\":\"DS2d7s23\",\"docs_openai_actions_introduction.md\":\"CmrWw1gL\",\"docs_openai_actions_production.md\":\"BTrNTbbl\",\"docs_openai_actions_sending-files.md\":\"BvQvsXTq\",\"docs_openai_assistants_deep-dive.md\":\"CgEhOVKF\",\"docs_openai_assistants_overview.md\":\"BIRtnHrL\",\"docs_openai_assistants_quickstart.md\":\"C5nYpAfc\",\"docs_openai_assistants_tools_code-interpreter.md\":\"DMMkwzCF\",\"docs_openai_assistants_tools_file-search.md\":\"Di8dQMdr\",\"docs_openai_assistants_tools_function-calling.md\":\"B2O1o0e-\",\"docs_openai_assistants_whats-new.md\":\"PHTy3paG\",\"docs_openai_deprecations.md\":\"BgHBVnHn\",\"docs_openai_guides_accuracy-optimization.md\":\"ySWgvGhQ\",\"docs_openai_guides_advanced-usage.md\":\"DIFQ8iGw\",\"docs_openai_guides_agents.md\":\"BB83t8wJ\",\"docs_openai_guides_audio.md\":\"38qefc5P\",\"docs_openai_guides_batch.md\":\"ZnNNg9HE\",\"docs_openai_guides_codex.md\":\"Cv_Se84r\",\"docs_openai_guides_conversation-state-chat.md\":\"D41wtFgc\",\"docs_openai_guides_conversation-state.md\":\"C5CmiEny\",\"docs_openai_guides_direct-preference-optimization.md\":\"Df_csJP_\",\"docs_openai_guides_distillation.md\":\"Clp82vMT\",\"docs_openai_guides_embeddings.md\":\"DvT9hRPc\",\"docs_openai_guides_evals-design.md\":\"Zc7C3fzW\",\"docs_openai_guides_evals.md\":\"8BT6XVwl\",\"docs_openai_guides_fine-tuning-best-practices.md\":\"BrdNSrTP\",\"docs_openai_guides_fine-tuning.md\":\"CVoYRa5O\",\"docs_openai_guides_flex-processing.md\":\"DU0Fh4X1\",\"docs_openai_guides_function-calling-chat.md\":\"BtfimyxN\",\"docs_openai_guides_function-calling.md\":\"BtjpvV3H\",\"docs_openai_guides_graders.md\":\"DviATPPg\",\"docs_openai_guides_image-generation.md\":\"CInpsJDA\",\"docs_openai_guides_images-vision-chat.md\":\"DoUccNLd\",\"docs_openai_guides_images-vision.md\":\"kT8cY6sA\",\"docs_openai_guides_index.md\":\"Du75bavh\",\"docs_openai_guides_latency-optimization.md\":\"BUtUqZDU\",\"docs_openai_guides_model-optimization.md\":\"eLGfWnT5\",\"docs_openai_guides_model-selection.md\":\"CGlJuDPV\",\"docs_openai_guides_moderation.md\":\"D2RFR-KH\",\"docs_openai_guides_pdf-files-chat.md\":\"Bo3jizg-\",\"docs_openai_guides_pdf-files.md\":\"DAKQ9CWN\",\"docs_openai_guides_predicted-outputs.md\":\"BD0wCwjF\",\"docs_openai_guides_production-best-practices.md\":\"wv07VYZ3\",\"docs_openai_guides_prompt-caching.md\":\"y_ZpMrKm\",\"docs_openai_guides_prompt-generation.md\":\"DLtdMqm3\",\"docs_openai_guides_rate-limits.md\":\"BW-M7in8\",\"docs_openai_guides_realtime-conversations.md\":\"CEjp5-5Z\",\"docs_openai_guides_realtime-transcription.md\":\"4pwNMyte\",\"docs_openai_guides_realtime-vad.md\":\"CJGYxAZF\",\"docs_openai_guides_realtime.md\":\"B7sKeYr4\",\"docs_openai_guides_reasoning-best-practices.md\":\"B7fqsLcY\",\"docs_openai_guides_reasoning-chat.md\":\"NNoU0ntr\",\"docs_openai_guides_reasoning.md\":\"CaGqwKHi\",\"docs_openai_guides_reinforcement-fine-tuning.md\":\"C2LwTH9k\",\"docs_openai_guides_responses-vs-chat-completions.md\":\"WpIAQgvT\",\"docs_openai_guides_retrieval.md\":\"DsVw2SXI\",\"docs_openai_guides_rft-use-cases.md\":\"nvzkpknx\",\"docs_openai_guides_safety-best-practices.md\":\"DpSK59eH\",\"docs_openai_guides_speech-to-text.md\":\"C7frelap\",\"docs_openai_guides_streaming-responses-chat.md\":\"CI6CWaZL\",\"docs_openai_guides_streaming-responses.md\":\"DSnmIEJ7\",\"docs_openai_guides_structured-outputs-chat.md\":\"TAEvcG4b\",\"docs_openai_guides_structured-outputs.md\":\"BUpRWVJc\",\"docs_openai_guides_supervised-fine-tuning.md\":\"Cf2RT7Uc\",\"docs_openai_guides_text-chat.md\":\"BYUhqvhM\",\"docs_openai_guides_text-to-speech.md\":\"BRmId5_Y\",\"docs_openai_guides_text.md\":\"CX7j2kf7\",\"docs_openai_guides_tools-computer-use.md\":\"B4XASnSh\",\"docs_openai_guides_tools-file-search.md\":\"BKCI8Kd5\",\"docs_openai_guides_tools-local-shell.md\":\"DOAZ9ySC\",\"docs_openai_guides_tools-web-search.md\":\"7D7f3oqc\",\"docs_openai_guides_tools.md\":\"ja1bTVQc\",\"docs_openai_guides_vision-fine-tuning.md\":\"DSZ2Sloh\",\"docs_openai_guides_voice-agents-chained.md\":\"Df5nqa2D\",\"docs_openai_guides_voice-agents.md\":\"CAlTztX6\",\"docs_openai_guides_your-data.md\":\"CcPuC8Tn\",\"docs_openai_libraries.md\":\"CLRlzJ4E\",\"docs_openai_quickstart-chat.md\":\"D1slhSsp\",\"docs_openai_quickstart.md\":\"DA-_M-vX\",\"index.md\":\"CfXNoIs9\",\"readme.md\":\"jy6qJIiQ\",\"tutorial_index.md\":\"DHqSMg3W\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"DaQianAI\",\"description\":\"The ultimate AI\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"search\":{\"provider\":\"local\"},\"nav\":[{\"text\":\"Home\",\"link\":\"/\"},{\"text\":\"APP\",\"items\":[{\"text\":\"deepwiki\",\"link\":\"https://deepwiki.com\"}]},{\"text\":\"Prompt\",\"link\":\"/tutorial\"},{\"text\":\"MCP\",\"items\":[{\"text\":\"Protocol\",\"link\":\"https://modelcontextprotocol.io/introduction\"},{\"text\":\"Python-SDK\",\"link\":\"https://deepwiki.com/modelcontextprotocol/python-sdk/\"}]},{\"text\":\"Tutorial\",\"link\":\"/tutorial\"},{\"text\":\"Docs\",\"items\":[{\"text\":\"OpenAI\",\"link\":\"/docs/openai/guides\"},{\"text\":\"Anthropic\",\"link\":\"/docs/anthropic\"},{\"text\":\"Vitepress\",\"link\":\"/docs/examples/vitepress\"}]},{\"text\":\"About\",\"items\":[{\"text\":\"Team\",\"link\":\"/about/team\"}]}],\"sidebar\":{\"/docs/examples/vitepress\":[{\"text\":\"Examples\",\"items\":[{\"text\":\"Markdown Examples\",\"link\":\"/docs/examples/vitepress/markdown-examples\"},{\"text\":\"Runtime API Examples\",\"link\":\"/docs/examples/vitepress/api-examples\"},{\"text\":\"Emoji\",\"link\":\"/docs/examples/vitepress/emoji\"}]}],\"/docs/openai\":[{\"text\":\"GET STARTED\",\"base\":\"/docs/openai\",\"items\":[{\"text\":\"Quickstart\",\"link\":\"/quickstart\"},{\"text\":\"Quickstart(chat)\",\"link\":\"/quickstart-chat\"},{\"text\":\"Libraries\",\"link\":\"/libraries\"}]},{\"text\":\"CORE CONCEPTS\",\"base\":\"/docs/openai\",\"items\":[{\"text\":\"Text and prompting\",\"link\":\"/guides/text\"},{\"text\":\"Text and prompting(chat)\",\"link\":\"/guides/text-chat\"},{\"text\":\"Images and vision\",\"link\":\"/guides/images-vision\"},{\"text\":\"Images and vision(chat)\",\"link\":\"/guides/images-vision-chat\"},{\"text\":\"Audio and speech\",\"link\":\"/guides/audio\"},{\"text\":\"Structured Outputs\",\"link\":\"/guides/structured-outputs\"},{\"text\":\"Structured Outputs(chat)\",\"link\":\"/guides/structured-outputs-chat\"},{\"text\":\"Function calling\",\"link\":\"/guides/function-calling\"},{\"text\":\"Function calling(chat)\",\"link\":\"/guides/function-calling-chat\"},{\"text\":\"Conversation state\",\"link\":\"/guides/conversation-state\"},{\"text\":\"Conversation state(chat)\",\"link\":\"/guides/conversation-state-chat\"},{\"text\":\"Streaming\",\"link\":\"/guides/streaming-responses\"},{\"text\":\"Streaming(chat)\",\"link\":\"/guides/streaming-responses-chat\"},{\"text\":\"File inputs\",\"link\":\"/guides/pdf-files\"},{\"text\":\"File inputs(chat)\",\"link\":\"/guides/pdf-files-chat\"},{\"text\":\"Reasoning\",\"link\":\"/guides/reasoning\"},{\"text\":\"Reasoning(chat)\",\"link\":\"/guides/reasoning-chat\"}]},{\"text\":\"BUILT-IN TOOLS\",\"base\":\"/docs/openai\",\"items\":[{\"text\":\"Using built-in tools\",\"link\":\"/guides/tools\"},{\"text\":\"Web search\",\"link\":\"/guides/tools-web-search\"},{\"text\":\"File search\",\"link\":\"/guides/tools-file-search\"},{\"text\":\"Computer use\",\"link\":\"/guides/tools-computer-use\"}]},{\"text\":\"AGENTS\",\"items\":[{\"text\":\"Building agents\",\"link\":\"/docs/openai/guides/agents\"},{\"text\":\"Voice agents\",\"link\":\"/docs/openai/guides/voice-agents\"},{\"text\":\"Voice agents(chained)\",\"link\":\"/docs/openai/guides/voice-agents-chained\"},{\"text\":\"Agents SDK\",\"link\":\"https://openai.github.io/openai-agents-python/\"}]},{\"text\":\"REALTIME API\",\"base\":\"/docs/openai\",\"items\":[{\"text\":\"Using the Realtime API\",\"link\":\"/guides/realtime\"},{\"text\":\"Realtime conversations\",\"link\":\"/guides/realtime-conversations\"},{\"text\":\"Realtime transcription\",\"link\":\"/guides/realtime-transcription\"},{\"text\":\"Voice activity detection\",\"link\":\"/guides/realtime-vad\"}]},{\"text\":\"MODEL OPTIMIZATION\",\"base\":\"/docs/openai\",\"items\":[{\"text\":\"Overview\",\"link\":\"/guides/model-optimization\"},{\"text\":\"Evals\",\"link\":\"/guides/evals\"},{\"text\":\"Fine-tuning\",\"link\":\"/guides/fine-tuning\"},{\"text\":\"Supervised fine-tuning\",\"link\":\"/guides/supervised-fine-tuning\"},{\"text\":\"Vision fine-tuning\",\"link\":\"/guides/vision-fine-tuning\"},{\"text\":\"Direct preference optimization\",\"link\":\"/guides/direct-preference-optimization\"},{\"text\":\"Reinforcement fine-tuning\",\"link\":\"/guides/reinforcement-fine-tuning\"},{\"text\":\"Graders\",\"link\":\"/guides/graders\"},{\"text\":\"Distillation\",\"link\":\"/guides/distillation\"}]},{\"text\":\"SPECIALIZED MODELS\",\"base\":\"/docs/openai\",\"items\":[{\"text\":\"Image generation\",\"link\":\"/guides/image-generation\"},{\"text\":\"Text to speech\",\"link\":\"/guides/text-to-speech\"},{\"text\":\"Speech to text\",\"link\":\"/guides/speech-to-text\"},{\"text\":\"Embeddings\",\"link\":\"/guides/embeddings\"},{\"text\":\"Moderation\",\"link\":\"/guides/moderation\"}]},{\"text\":\"OPENAI PLATFORM\",\"base\":\"/docs/openai\",\"items\":[{\"text\":\"Retrieval\",\"link\":\"/guides/retrieval\"},{\"text\":\"Batch\",\"link\":\"/guides/batch\"},{\"text\":\"Prompt generation\",\"link\":\"/guides/prompt-generation\"}]},{\"text\":\"CODEX\",\"items\":[{\"text\":\"Codex\",\"link\":\"/docs/openai/guides/codex\"},{\"text\":\"Local shell tool\",\"link\":\"/docs/openai/guides/tools-local-shell\"},{\"text\":\"Codex CLI\",\"link\":\"https://github.com/openai/codex\"}]},{\"text\":\"BEST PRACTICES\",\"base\":\"/docs/openai\",\"items\":[{\"text\":\"Production best practices\",\"link\":\"/guides/production-best-practices\"},{\"text\":\"Safety best practices\",\"link\":\"/guides/safety-best-practices\"},{\"text\":\"Prompt Caching\",\"link\":\"/guides/prompt-caching\"},{\"text\":\"Predicted Outputs\",\"link\":\"/guides/predicted-outputs\"},{\"text\":\"Reasoning best practices\",\"link\":\"/guides/reasoning-best-practices\"},{\"text\":\"Evals design\",\"link\":\"/guides/evals-design\"},{\"text\":\"Fine-tuning best practices\",\"link\":\"/guides/fine-tuning-best-practices\"},{\"text\":\"Reinforcement fine-tuning use cases\",\"link\":\"/guides/rft-use-cases\"},{\"text\":\"Model selection\",\"link\":\"/guides/model-selection\"},{\"text\":\"Latency optimization\",\"link\":\"/guides/latency-optimization\"},{\"text\":\"Accuracy optimization\",\"link\":\"/guides/accuracy-optimization\"},{\"text\":\"Advanced usage\",\"link\":\"/guides/advanced-usage\"},{\"text\":\"Responses vs. Chat Completions\",\"link\":\"/guides/responses-vs-chat-completions\"},{\"text\":\"Flex processing\",\"link\":\"/guides/flex-processing\"}]},{\"text\":\"ASSISTANTS API\",\"base\":\"/docs/openai\",\"items\":[{\"text\":\"Overview\",\"link\":\"/assistants/overview\"},{\"text\":\"Quickstart\",\"link\":\"/assistants/quickstart\"},{\"text\":\"Deep dive\",\"link\":\"/assistants/deep-dive\"},{\"text\":\"Tools\",\"collapsed\":true,\"items\":[{\"text\":\"File Search\",\"link\":\"/assistants/tools/file-search\"},{\"text\":\"Code interpreter\",\"link\":\"/assistants/tools/code-interpreter\"},{\"text\":\"Function calling\",\"link\":\"/assistants/tools/function-calling\"}]},{\"text\":\"What's new\",\"link\":\"/assistants/whats-new\"}]},{\"text\":\"RESOURCES\",\"items\":[{\"text\":\"Terms and policies\",\"link\":\"https://openai.com/policies/\"},{\"text\":\"Changelog\",\"link\":\"https://platform.openai.com/docs/changelog\"},{\"text\":\"Your data\",\"link\":\"/docs/openai/guides/your-data\"},{\"text\":\"Rate limits\",\"link\":\"/docs/openai/guides/rate-limits\"},{\"text\":\"Deprecations\",\"link\":\"/docs/openai/deprecations\"},{\"text\":\"ChatGPT Actions\",\"base\":\"/docs/openai\",\"collapsed\":true,\"items\":[{\"text\":\"Introduction\",\"link\":\"/actions/introduction\"},{\"text\":\"Getting started\",\"link\":\"/actions/getting-started\"},{\"text\":\"Actions library\",\"link\":\"/actions/actions-library\"},{\"text\":\"Authentication\",\"link\":\"/actions/authentication\"},{\"text\":\"Production\",\"link\":\"/actions/production\"},{\"text\":\"Data retrieval\",\"link\":\"/actions/data-retrieval\"},{\"text\":\"Sending files\",\"link\":\"/actions/sending-files\"}]}]},{\"text\":\"\",\"items\":[{\"text\":\"Cookbook\",\"link\":\"https://cookbook.openai.com/\"},{\"text\":\"Forum\",\"link\":\"https://community.openai.com/categories\"},{\"text\":\"Help\",\"link\":\"\"}]}]},\"footer\":{\"message\":\"build with <a href=\\\"https://github.com/vuejs/vitepress\\\">vitepress</a>\",\"copyright\":\"Copyright © 2023-present by NOTHING\"},\"socialLinks\":[{\"icon\":\"wechat\",\"link\":\"/about/team\"}]},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>