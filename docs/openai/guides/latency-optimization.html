<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Latency optimization | DaQianAI</title>
    <meta name="description" content="The ultimate AI">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/assets/style.qp7qGaqN.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.CfT-s13H.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/framework.DDIT__tB.js">
    <link rel="modulepreload" href="/assets/chunks/theme.NwwqACZM.js">
    <link rel="modulepreload" href="/assets/chunks/katex.ChWnQ-fc.js">
    <link rel="modulepreload" href="/assets/chunks/dagre-OKDRZEBW.DYwXk_cA.js">
    <link rel="modulepreload" href="/assets/chunks/c4Diagram-VJAJSXHY.Nv5X7mpF.js">
    <link rel="modulepreload" href="/assets/chunks/flowDiagram-4HSFHLVR.CUtQxFRt.js">
    <link rel="modulepreload" href="/assets/chunks/erDiagram-Q7BY3M3F.-n7eiTzr.js">
    <link rel="modulepreload" href="/assets/chunks/gitGraphDiagram-7IBYFJ6S.BRyiGL-A.js">
    <link rel="modulepreload" href="/assets/chunks/ganttDiagram-APWFNJXF.Cl9oi4-0.js">
    <link rel="modulepreload" href="/assets/chunks/infoDiagram-PH2N3AL5.XSMLMO8n.js">
    <link rel="modulepreload" href="/assets/chunks/pieDiagram-IB7DONF6.BiPqMNIk.js">
    <link rel="modulepreload" href="/assets/chunks/quadrantDiagram-7GDLP6J5.ByHgky16.js">
    <link rel="modulepreload" href="/assets/chunks/xychartDiagram-VJFVF3MP.CzAC9QUw.js">
    <link rel="modulepreload" href="/assets/chunks/requirementDiagram-KVF5MWMF.Das3iX-B.js">
    <link rel="modulepreload" href="/assets/chunks/sequenceDiagram-X6HHIX6F.CqPDGsIC.js">
    <link rel="modulepreload" href="/assets/chunks/classDiagram-GIVACNV2.AlX6dug0.js">
    <link rel="modulepreload" href="/assets/chunks/classDiagram-v2-COTLJTTW.AlX6dug0.js">
    <link rel="modulepreload" href="/assets/chunks/stateDiagram-DGXRK772.PvbhjBF3.js">
    <link rel="modulepreload" href="/assets/chunks/stateDiagram-v2-YXO3MK2T.DSUDpyUZ.js">
    <link rel="modulepreload" href="/assets/chunks/journeyDiagram-U35MCT3I.DT8Yu3-x.js">
    <link rel="modulepreload" href="/assets/chunks/timeline-definition-BDJGKUSR.OTNtnIIf.js">
    <link rel="modulepreload" href="/assets/chunks/mindmap-definition-ALO5MXBD.D0SGlVsR.js">
    <link rel="modulepreload" href="/assets/chunks/kanban-definition-NDS4AKOZ.CQHhVqoI.js">
    <link rel="modulepreload" href="/assets/chunks/sankeyDiagram-QLVOVGJD.DGqml8DN.js">
    <link rel="modulepreload" href="/assets/chunks/diagram-VNBRO52H.CxzgmhNk.js">
    <link rel="modulepreload" href="/assets/chunks/diagram-SSKATNLV.BgpcdZGN.js">
    <link rel="modulepreload" href="/assets/chunks/blockDiagram-JOT3LUYC.tZq8j7GF.js">
    <link rel="modulepreload" href="/assets/chunks/architectureDiagram-IEHRJDOE.BzQxSAKT.js">
    <link rel="modulepreload" href="/assets/chunks/virtual_mermaid-config.DY5NpnxB.js">
    <link rel="modulepreload" href="/assets/docs_openai_guides_latency-optimization.md.CGO_h0Il.lean.js">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-d8b57b2d><!--[--><!--]--><!--[--><span tabindex="-1" data-v-fcbfc0e0></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-fcbfc0e0>Skip to content</a><!--]--><!----><header class="VPNav" data-v-d8b57b2d data-v-7ad780c2><div class="VPNavBar" data-v-7ad780c2 data-v-9fd4d1dd><div class="wrapper" data-v-9fd4d1dd><div class="container" data-v-9fd4d1dd><div class="title" data-v-9fd4d1dd><div class="VPNavBarTitle has-sidebar" data-v-9fd4d1dd data-v-9f43907a><a class="title" href="/" data-v-9f43907a><!--[--><!--]--><!--[--><img class="VPImage logo" src="/logo.png" alt data-v-ab19afbb><!--]--><span data-v-9f43907a>DaQianAI</span><!--[--><!--]--></a></div></div><div class="content" data-v-9fd4d1dd><div class="content-body" data-v-9fd4d1dd><!--[--><!--]--><div class="VPNavBarSearch search" data-v-9fd4d1dd><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-9fd4d1dd data-v-afb2845e><span id="main-nav-aria-label" class="visually-hidden" data-v-afb2845e> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/" tabindex="0" data-v-afb2845e data-v-815115f5><!--[--><span data-v-815115f5>Home</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-afb2845e data-v-bfe7971f><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-bfe7971f><span class="text" data-v-bfe7971f><!----><span data-v-bfe7971f>APP</span><span class="vpi-chevron-down text-icon" data-v-bfe7971f></span></span></button><div class="menu" data-v-bfe7971f><div class="VPMenu" data-v-bfe7971f data-v-20ed86d6><div class="items" data-v-20ed86d6><!--[--><!--[--><div class="VPMenuLink" data-v-20ed86d6 data-v-7eeeb2dc><a class="VPLink link vp-external-link-icon" href="https://deepwiki.com" target="_blank" rel="noreferrer" data-v-7eeeb2dc><!--[--><span data-v-7eeeb2dc>deepwiki</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/llm" tabindex="0" data-v-afb2845e data-v-815115f5><!--[--><span data-v-815115f5>LLM</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/agent" tabindex="0" data-v-afb2845e data-v-815115f5><!--[--><span data-v-815115f5>Agent</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/eval" tabindex="0" data-v-afb2845e data-v-815115f5><!--[--><span data-v-815115f5>Eval</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/tutorial" tabindex="0" data-v-afb2845e data-v-815115f5><!--[--><span data-v-815115f5>Prompt</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-afb2845e data-v-bfe7971f><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-bfe7971f><span class="text" data-v-bfe7971f><!----><span data-v-bfe7971f>MCP</span><span class="vpi-chevron-down text-icon" data-v-bfe7971f></span></span></button><div class="menu" data-v-bfe7971f><div class="VPMenu" data-v-bfe7971f data-v-20ed86d6><div class="items" data-v-20ed86d6><!--[--><!--[--><div class="VPMenuLink" data-v-20ed86d6 data-v-7eeeb2dc><a class="VPLink link vp-external-link-icon" href="https://modelcontextprotocol.io/introduction" target="_blank" rel="noreferrer" data-v-7eeeb2dc><!--[--><span data-v-7eeeb2dc>Protocol</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-20ed86d6 data-v-7eeeb2dc><a class="VPLink link vp-external-link-icon" href="https://deepwiki.com/modelcontextprotocol/python-sdk/" target="_blank" rel="noreferrer" data-v-7eeeb2dc><!--[--><span data-v-7eeeb2dc>Python-SDK</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/tutorial" tabindex="0" data-v-afb2845e data-v-815115f5><!--[--><span data-v-815115f5>Tutorial</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-afb2845e data-v-bfe7971f><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-bfe7971f><span class="text" data-v-bfe7971f><!----><span data-v-bfe7971f>Docs</span><span class="vpi-chevron-down text-icon" data-v-bfe7971f></span></span></button><div class="menu" data-v-bfe7971f><div class="VPMenu" data-v-bfe7971f data-v-20ed86d6><div class="items" data-v-20ed86d6><!--[--><!--[--><div class="VPMenuLink" data-v-20ed86d6 data-v-7eeeb2dc><a class="VPLink link" href="/docs/openai/guides" data-v-7eeeb2dc><!--[--><span data-v-7eeeb2dc>OpenAI</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-20ed86d6 data-v-7eeeb2dc><a class="VPLink link" href="/docs/anthropic" data-v-7eeeb2dc><!--[--><span data-v-7eeeb2dc>Anthropic</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-20ed86d6 data-v-7eeeb2dc><a class="VPLink link" href="/docs/examples/vitepress" data-v-7eeeb2dc><!--[--><span data-v-7eeeb2dc>Vitepress</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/projects" tabindex="0" data-v-afb2845e data-v-815115f5><!--[--><span data-v-815115f5>Projects</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-afb2845e data-v-bfe7971f><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-bfe7971f><span class="text" data-v-bfe7971f><!----><span data-v-bfe7971f>About</span><span class="vpi-chevron-down text-icon" data-v-bfe7971f></span></span></button><div class="menu" data-v-bfe7971f><div class="VPMenu" data-v-bfe7971f data-v-20ed86d6><div class="items" data-v-20ed86d6><!--[--><!--[--><div class="VPMenuLink" data-v-20ed86d6 data-v-7eeeb2dc><a class="VPLink link" href="/about/team" data-v-7eeeb2dc><!--[--><span data-v-7eeeb2dc>Team</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-20ed86d6 data-v-7eeeb2dc><a class="VPLink link" href="/blog/" data-v-7eeeb2dc><!--[--><span data-v-7eeeb2dc>blog</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-9fd4d1dd data-v-3f90c1a5><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-3f90c1a5 data-v-be9742d9 data-v-b4ccac88><span class="check" data-v-b4ccac88><span class="icon" data-v-b4ccac88><!--[--><span class="vpi-sun sun" data-v-be9742d9></span><span class="vpi-moon moon" data-v-be9742d9></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-9fd4d1dd data-v-ef6192dc data-v-e71e869c><!--[--><a class="VPSocialLink no-icon" href="/about/team" aria-label="wechat" target="_blank" rel="noopener" data-v-e71e869c data-v-60a9a2d3><span class="vpi-social-wechat"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-9fd4d1dd data-v-f953d92f data-v-bfe7971f><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-bfe7971f><span class="vpi-more-horizontal icon" data-v-bfe7971f></span></button><div class="menu" data-v-bfe7971f><div class="VPMenu" data-v-bfe7971f data-v-20ed86d6><!----><!--[--><!--[--><!----><div class="group" data-v-f953d92f><div class="item appearance" data-v-f953d92f><p class="label" data-v-f953d92f>Appearance</p><div class="appearance-action" data-v-f953d92f><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-f953d92f data-v-be9742d9 data-v-b4ccac88><span class="check" data-v-b4ccac88><span class="icon" data-v-b4ccac88><!--[--><span class="vpi-sun sun" data-v-be9742d9></span><span class="vpi-moon moon" data-v-be9742d9></span><!--]--></span></span></button></div></div></div><div class="group" data-v-f953d92f><div class="item social-links" data-v-f953d92f><div class="VPSocialLinks social-links-list" data-v-f953d92f data-v-e71e869c><!--[--><a class="VPSocialLink no-icon" href="/about/team" aria-label="wechat" target="_blank" rel="noopener" data-v-e71e869c data-v-60a9a2d3><span class="vpi-social-wechat"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-9fd4d1dd data-v-6bee1efd><span class="container" data-v-6bee1efd><span class="top" data-v-6bee1efd></span><span class="middle" data-v-6bee1efd></span><span class="bottom" data-v-6bee1efd></span></span></button></div></div></div></div><div class="divider" data-v-9fd4d1dd><div class="divider-line" data-v-9fd4d1dd></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-d8b57b2d data-v-2488c25a><div class="container" data-v-2488c25a><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-2488c25a><span class="vpi-align-left menu-icon" data-v-2488c25a></span><span class="menu-text" data-v-2488c25a>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-2488c25a data-v-6b867909><button data-v-6b867909>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-d8b57b2d data-v-42c4c606><div class="curtain" data-v-42c4c606></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-42c4c606><span class="visually-hidden" id="sidebar-aria-label" data-v-42c4c606> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>GET STARTED</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/quickstart" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Quickstart</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/quickstart-chat" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Quickstart(chat)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/libraries" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Libraries</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>CORE CONCEPTS</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/text" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Text and prompting</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/text-chat" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Text and prompting(chat)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/images-vision" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Images and vision</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/images-vision-chat" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Images and vision(chat)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/audio" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Audio and speech</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/structured-outputs" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Structured Outputs</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/structured-outputs-chat" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Structured Outputs(chat)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/function-calling" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Function calling</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/function-calling-chat" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Function calling(chat)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/conversation-state" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Conversation state</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/conversation-state-chat" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Conversation state(chat)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/streaming-responses" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Streaming</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/streaming-responses-chat" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Streaming(chat)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/pdf-files" data-v-0009425e><!--[--><p class="text" data-v-0009425e>File inputs</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/pdf-files-chat" data-v-0009425e><!--[--><p class="text" data-v-0009425e>File inputs(chat)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/reasoning" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Reasoning</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/reasoning-chat" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Reasoning(chat)</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>BUILT-IN TOOLS</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/tools" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Using built-in tools</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/tools-web-search" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Web search</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/tools-file-search" data-v-0009425e><!--[--><p class="text" data-v-0009425e>File search</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/tools-computer-use" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Computer use</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>AGENTS</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/agents" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Building agents</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/voice-agents" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Voice agents</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/voice-agents-chained" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Voice agents(chained)</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link vp-external-link-icon link" href="https://openai.github.io/openai-agents-python/" target="_blank" rel="noreferrer" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Agents SDK</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>REALTIME API</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/realtime" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Using the Realtime API</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/realtime-conversations" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Realtime conversations</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/realtime-transcription" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Realtime transcription</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/realtime-vad" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Voice activity detection</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>MODEL OPTIMIZATION</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/model-optimization" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Overview</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/evals" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Evals</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/fine-tuning" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Fine-tuning</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/supervised-fine-tuning" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Supervised fine-tuning</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/vision-fine-tuning" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Vision fine-tuning</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/direct-preference-optimization" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Direct preference optimization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/reinforcement-fine-tuning" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Reinforcement fine-tuning</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/graders" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Graders</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/distillation" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Distillation</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>SPECIALIZED MODELS</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/image-generation" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Image generation</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/text-to-speech" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Text to speech</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/speech-to-text" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Speech to text</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/embeddings" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Embeddings</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/moderation" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Moderation</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>OPENAI PLATFORM</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/retrieval" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Retrieval</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/batch" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Batch</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/prompt-generation" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Prompt generation</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>CODEX</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/codex" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Codex</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/tools-local-shell" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Local shell tool</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link vp-external-link-icon link" href="https://github.com/openai/codex" target="_blank" rel="noreferrer" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Codex CLI</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0 has-active" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>BEST PRACTICES</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/production-best-practices" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Production best practices</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/safety-best-practices" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Safety best practices</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/prompt-caching" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Prompt Caching</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/predicted-outputs" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Predicted Outputs</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/reasoning-best-practices" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Reasoning best practices</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/evals-design" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Evals design</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/fine-tuning-best-practices" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Fine-tuning best practices</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/rft-use-cases" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Reinforcement fine-tuning use cases</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/model-selection" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Model selection</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/latency-optimization" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Latency optimization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/accuracy-optimization" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Accuracy optimization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/advanced-usage" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Advanced usage</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/responses-vs-chat-completions" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Responses vs. Chat Completions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/flex-processing" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Flex processing</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>ASSISTANTS API</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/assistants/overview" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Overview</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/assistants/quickstart" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Quickstart</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/assistants/deep-dive" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Deep dive</p><!--]--></a><!----></div><!----></div><section class="VPSidebarItem level-1 collapsible collapsed" data-v-0009425e data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h3 class="text" data-v-0009425e>Tools</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-0009425e><span class="vpi-chevron-right caret-icon" data-v-0009425e></span></div></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-2 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/assistants/tools/file-search" data-v-0009425e><!--[--><p class="text" data-v-0009425e>File Search</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/assistants/tools/code-interpreter" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Code interpreter</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/assistants/tools/function-calling" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Function calling</p><!--]--></a><!----></div><!----></div><!--]--></div></section><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/assistants/whats-new" data-v-0009425e><!--[--><p class="text" data-v-0009425e>What's new</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h2 class="text" data-v-0009425e>RESOURCES</h2><!----></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link vp-external-link-icon link" href="https://openai.com/policies/" target="_blank" rel="noreferrer" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Terms and policies</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link vp-external-link-icon link" href="https://platform.openai.com/docs/changelog" target="_blank" rel="noreferrer" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Changelog</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/your-data" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Your data</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/guides/rate-limits" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Rate limits</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/deprecations" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Deprecations</p><!--]--></a><!----></div><!----></div><section class="VPSidebarItem level-1 collapsible collapsed" data-v-0009425e data-v-0009425e><div class="item" role="button" tabindex="0" data-v-0009425e><div class="indicator" data-v-0009425e></div><h3 class="text" data-v-0009425e>ChatGPT Actions</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-0009425e><span class="vpi-chevron-right caret-icon" data-v-0009425e></span></div></div><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-2 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/actions/introduction" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Introduction</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/actions/getting-started" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Getting started</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/actions/actions-library" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Actions library</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/actions/authentication" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Authentication</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/actions/production" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Production</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/actions/data-retrieval" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Data retrieval</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link link" href="/docs/openai/actions/sending-files" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Sending files</p><!--]--></a><!----></div><!----></div><!--]--></div></section><!--]--></div></section></div><div class="no-transition group" data-v-51288d80><section class="VPSidebarItem level-0" data-v-51288d80 data-v-0009425e><!----><div class="items" data-v-0009425e><!--[--><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link vp-external-link-icon link" href="https://cookbook.openai.com/" target="_blank" rel="noreferrer" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Cookbook</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-0009425e data-v-0009425e><div class="item" data-v-0009425e><div class="indicator" data-v-0009425e></div><a class="VPLink link vp-external-link-icon link" href="https://community.openai.com/categories" target="_blank" rel="noreferrer" data-v-0009425e><!--[--><p class="text" data-v-0009425e>Forum</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1" data-v-0009425e data-v-0009425e><div class="item" role="button" data-v-0009425e><div class="indicator" data-v-0009425e></div><p class="text" data-v-0009425e>Help</p><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-d8b57b2d data-v-9a6c75ad><div class="VPDoc has-sidebar has-aside" data-v-9a6c75ad data-v-e6f2a212><!--[--><!--]--><div class="container" data-v-e6f2a212><div class="aside" data-v-e6f2a212><div class="aside-curtain" data-v-e6f2a212></div><div class="aside-container" data-v-e6f2a212><div class="aside-content" data-v-e6f2a212><div class="VPDocAside" data-v-e6f2a212 data-v-cb998dce><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-cb998dce data-v-f610f197><div class="content" data-v-f610f197><div class="outline-marker" data-v-f610f197></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-f610f197>On this page</div><ul class="VPDocOutlineItem root" data-v-f610f197 data-v-53c99d69><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-cb998dce></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-e6f2a212><div class="content-container" data-v-e6f2a212><!--[--><!--]--><main class="main" data-v-e6f2a212><div style="position:relative;" class="vp-doc _docs_openai_guides_latency-optimization" data-v-e6f2a212><div><h1 id="latency-optimization" tabindex="-1">Latency optimization <a class="header-anchor" href="#latency-optimization" aria-label="Permalink to &quot;Latency optimization&quot;">​</a></h1><p>Improve latency across a wide variety of LLM-related use cases.</p><p>This guide covers the core set of principles you can apply to improve latency across a wide variety of LLM-related use cases. These techniques come from working with a wide range of customers and developers on production applications, so they should apply regardless of what you&#39;re building – from a granular workflow to an end-to-end chatbot.</p><p>While there&#39;s many individual techniques, we&#39;ll be grouping them into <strong>seven principles</strong> meant to represent a high-level taxonomy of approaches for improving latency.</p><p>At the end, we&#39;ll walk through an <a href="#example">example</a> to see how they can be applied.</p><h3 id="seven-principles" tabindex="-1">Seven principles <a class="header-anchor" href="#seven-principles" aria-label="Permalink to &quot;Seven principles&quot;">​</a></h3><ol><li><a href="#process-tokens-faster">Process tokens faster.</a></li><li><a href="#generate-fewer-tokens">Generate fewer tokens.</a></li><li><a href="#use-fewer-input-tokens">Use fewer input tokens.</a></li><li><a href="#make-fewer-requests">Make fewer requests.</a></li><li><a href="#parallelize">Parallelize.</a></li><li><a href="#make-your-users-wait-less">Make your users wait less.</a></li><li><a href="#don-t-default-to-an-llm">Don&#39;t default to an LLM.</a></li></ol><h2 id="process-tokens-faster" tabindex="-1">Process tokens faster <a class="header-anchor" href="#process-tokens-faster" aria-label="Permalink to &quot;Process tokens faster&quot;">​</a></h2><p><strong>Inference speed</strong> is probably the first thing that comes to mind when addressing latency (but as you&#39;ll see soon, it&#39;s far from the only one). This refers to the actual <strong>rate at which the LLM processes tokens</strong>, and is often measured in TPM (tokens per minute) or TPS (tokens per second).</p><p>The main factor that influences inference speed is <strong>model size</strong> – smaller models usually run faster (and cheaper), and when used correctly can even outperform larger models. To maintain high quality performance with smaller models you can explore:</p><ul><li>using a longer, <a href="/docs/openai/guides/prompt-engineering#tactic-specify-the-steps-required-to-complete-a-task">more detailed prompt</a>,</li><li>adding (more) <a href="/docs/openai/guides/prompt-engineering#tactic-provide-examples">few-shot examples</a>, or</li><li><a href="/docs/openai/guides/fine-tuning">fine-tuning</a> / distillation.</li></ul><p>You can also employ inference optimizations like our <a href="/docs/openai/guides/predicted-outputs"><strong>Predicted outputs</strong></a> feature. Predicted outputs let you significantly reduce latency of a generation when you know most of the output ahead of time, such as code editing tasks. By giving the model a prediction, the LLM can focus more on the actual changes, and less on the content that will remain the same.</p><p>Deep dive</p><p>Compute capacity &amp; additional inference optimizations</p><h2 id="generate-fewer-tokens" tabindex="-1">Generate fewer tokens <a class="header-anchor" href="#generate-fewer-tokens" aria-label="Permalink to &quot;Generate fewer tokens&quot;">​</a></h2><p>Generating tokens is almost always the highest latency step when using an LLM: as a general heuristic, <strong>cutting 50% of your output tokens may cut ~50% your latency</strong>. The way you reduce your output size will depend on output type:</p><p>If you&#39;re generating <strong>natural language</strong>, simply <strong>asking the model to be more concise</strong> (&quot;under 20 words&quot; or &quot;be very brief&quot;) may help. You can also use few shot examples and/or fine-tuning to teach the model shorter responses.</p><p>If you&#39;re generating <strong>structured output</strong>, try to <strong>minimize your output syntax</strong> where possible: shorten function names, omit named arguments, coalesce parameters, etc.</p><p>Finally, while not common, you can also use <code>max_tokens</code> or <code>stop_tokens</code> to end your generation early.</p><p>Always remember: an output token cut is a (milli)second earned!</p><h2 id="use-fewer-input-tokens" tabindex="-1">Use fewer input tokens <a class="header-anchor" href="#use-fewer-input-tokens" aria-label="Permalink to &quot;Use fewer input tokens&quot;">​</a></h2><p>While reducing the number of input tokens does result in lower latency, this is not usually a significant factor – <strong>cutting 50% of your prompt may only result in a 1-5% latency improvement</strong>. Unless you&#39;re working with truly massive context sizes (documents, images), you may want to spend your efforts elsewhere.</p><p>That being said, if you <em>are</em> working with massive contexts (or you&#39;re set on squeezing every last bit of performance <em>and</em> you&#39;ve exhausted all other options) you can use the following techniques to reduce your input tokens:</p><ul><li><strong>Fine-tuning the model</strong>, to replace the need for lengthy instructions / examples.</li><li><strong>Filtering context input</strong>, like pruning RAG results, cleaning HTML, etc.</li><li><strong>Maximize shared prompt prefix</strong>, by putting dynamic portions (e.g. RAG results, history, etc) later in the prompt. This makes your request more <a href="https://medium.com/@joaolages/kv-caching-explained-276520203249" target="_blank" rel="noreferrer">KV cache</a>-friendly (which most LLM providers use) and means fewer input tokens are processed on each request.</li></ul><p>Check out our docs to learn more about how <a href="/docs/openai/guides/prompt-engineering#prompt-caching">prompt caching</a> works.</p><h2 id="make-fewer-requests" tabindex="-1">Make fewer requests <a class="header-anchor" href="#make-fewer-requests" aria-label="Permalink to &quot;Make fewer requests&quot;">​</a></h2><p>Each time you make a request you incur some round-trip latency – this can start to add up.</p><p>If you have sequential steps for the LLM to perform, instead of firing off one request per step consider <strong>putting them in a single prompt and getting them all in a single response</strong>. You&#39;ll avoid the additional round-trip latency, and potentially also reduce complexity of processing multiple responses.</p><p>An approach to doing this is by collecting your steps in an enumerated list in the combined prompt, and then requesting the model to return the results in named fields in a JSON. This way you can easily parse out and reference each result!</p><h2 id="parallelize" tabindex="-1">Parallelize <a class="header-anchor" href="#parallelize" aria-label="Permalink to &quot;Parallelize&quot;">​</a></h2><p>Parallelization can be very powerful when performing multiple steps with an LLM.</p><p>If the steps <strong>are <em>not</em> strictly sequential</strong>, you can <strong>split them out into parallel calls</strong>. Two shirts take just as long to dry as one.</p><p>If the steps <strong><em>are</em> strictly sequential</strong>, however, you might still be able to <strong>leverage speculative execution</strong>. This is particularly effective for classification steps where one outcome is more likely than the others (e.g. moderation).</p><ol><li>Start step 1 &amp; step 2 simultaneously (e.g. input moderation &amp; story generation)</li><li>Verify the result of step 1</li><li>If result was not the expected, cancel step 2 (and retry if necessary)</li></ol><p>If your guess for step 1 is right, then you essentially got to run it with zero added latency!</p><h2 id="make-your-users-wait-less" tabindex="-1">Make your users wait less <a class="header-anchor" href="#make-your-users-wait-less" aria-label="Permalink to &quot;Make your users wait less&quot;">​</a></h2><p>There&#39;s a huge difference between <strong>waiting</strong> and <strong>watching progress happen</strong> – make sure your users experience the latter. Here are a few techniques:</p><ul><li><strong>Streaming</strong>: The single most effective approach, as it cuts the <em>waiting</em> time to a second or less. (ChatGPT would feel pretty different if you saw nothing until each response was done.)</li><li><strong>Chunking</strong>: If your output needs further processing before being shown to the user (moderation, translation) consider <strong>processing it in chunks</strong> instead of all at once. Do this by streaming to your backend, then sending processed chunks to your frontend.</li><li><strong>Show your steps</strong>: If you&#39;re taking multiple steps or using tools, surface this to the user. The more real progress you can show, the better.</li><li><strong>Loading states</strong>: Spinners and progress bars go a long way.</li></ul><p>Note that while <strong>showing your steps &amp; having loading states</strong> have a mostly psychological effect, <strong>streaming &amp; chunking</strong> genuinely do reduce overall latency once you consider the app + user system: the user will finish reading a response sooner.</p><h2 id="don-t-default-to-an-llm" tabindex="-1">Don&#39;t default to an LLM <a class="header-anchor" href="#don-t-default-to-an-llm" aria-label="Permalink to &quot;Don&#39;t default to an LLM&quot;">​</a></h2><p>LLMs are extremely powerful and versatile, and are therefore sometimes used in cases where a <strong>faster classical method</strong> would be more appropriate. Identifying such cases may allow you to cut your latency significantly. Consider the following examples:</p><ul><li><strong>Hard-coding:</strong> If your <strong>output</strong> is highly constrained, you may not need an LLM to generate it. Action confirmations, refusal messages, and requests for standard input are all great candidates to be hard-coded. (You can even use the age-old method of coming up with a few variations for each.)</li><li><strong>Pre-computing:</strong> If your <strong>input</strong> is constrained (e.g. category selection) you can generate multiple responses in advance, and just make sure you never show the same one to a user twice.</li><li><strong>Leveraging UI:</strong> Summarized metrics, reports, or search results are sometimes better conveyed with classical, bespoke UI components rather than LLM-generated text.</li><li><strong>Traditional optimization techniques:</strong> An LLM application is still an application; binary search, caching, hash maps, and runtime complexity are all <em>still</em> useful in a world of LLMs.</li></ul><h2 id="example" tabindex="-1">Example <a class="header-anchor" href="#example" aria-label="Permalink to &quot;Example&quot;">​</a></h2><p>Let&#39;s now look at a sample application, identify potential latency optimizations, and propose some solutions!</p><p>We&#39;ll be analyzing the architecture and prompts of a hypothetical customer service bot inspired by real production applications. The <a href="#architecture-and-prompts">architecture and prompts</a> section sets the stage, and the <a href="#analysis-and-optimizations">analysis and optimizations</a> section will walk through the latency optimization process.</p><p>You&#39;ll notice this example doesn&#39;t cover every single principle, much like real-world use cases don&#39;t require applying every technique.</p><h3 id="architecture-and-prompts" tabindex="-1">Architecture and prompts <a class="header-anchor" href="#architecture-and-prompts" aria-label="Permalink to &quot;Architecture and prompts&quot;">​</a></h3><p>The following is the <strong>initial architecture</strong> for a hypothetical <strong>customer service bot</strong>. This is what we&#39;ll be making changes to.</p><p><img src="https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-0.png" alt="Assistants object architecture diagram"></p><p>At a high level, the diagram flow describes the following process:</p><ol><li>A user sends a message as part of an ongoing conversation.</li><li>The last message is turned into a <strong>self-contained query</strong> (see examples in prompt).</li><li>We determine whether or not <strong>additional (retrieved) information is required</strong> to respond to that query.</li><li><strong>Retrieval</strong> is performed, producing search results.</li><li>The assistant <strong>reasons</strong> about the user&#39;s query and search results, and <strong>produces a response</strong>.</li><li>The response is sent back to the user.</li></ol><p>Below are the prompts used in each part of the diagram. While they are still only hypothetical and simplified, they are written with the same structure and wording that you would find in a production application.</p><p>Places where you see placeholders like &quot;<strong>[user input here]</strong>&quot; represent dynamic portions, that would be replaced by actual data at runtime.</p><p>Query contextualization prompt</p><p>Re-writes user query to be a self-contained search query.</p><p>SYSTEM</p><p>Given the previous conversation, re-write the last user query so it contains all necessary context. # Example History: [{user: &quot;What is your return policy?&quot;},{assistant: &quot;...&quot;}] User Query: &quot;How long does it cover?&quot; Response: &quot;How long does the return policy cover?&quot; # Conversation [last 3 messages of conversation] # User Query [last user query]</p><p>USER</p><p>[JSON-formatted input conversation here]</p><p>Retrieval check prompt</p><p>Determines whether a query requires performing retrieval to respond.</p><p>SYSTEM</p><p>Given a user query, determine whether it requires doing a realtime lookup to respond to. # Examples User Query: &quot;How can I return this item after 30 days?&quot; Response: &quot;true&quot; User Query: &quot;Thank you!&quot; Response: &quot;false&quot;</p><p>USER</p><p>[input user query here]</p><p>Assistant prompt</p><p>Fills the fields of a JSON to reason through a pre-defined set of steps to produce a final response given a user conversation and relevant retrieved information.</p><p>SYSTEM</p><p>You are a helpful customer service bot. Use the result JSON to reason about each user query - use the retrieved context. # Example User: &quot;My computer screen is cracked! I want it fixed now!!!&quot; Assistant Response: { &quot;message_is_conversation_continuation&quot;: &quot;True&quot;, &quot;number_of_messages_in_conversation_so_far&quot;: &quot;1&quot;, &quot;user_sentiment&quot;: &quot;Aggravated&quot;, &quot;query_type&quot;: &quot;Hardware Issue&quot;, &quot;response_tone&quot;: &quot;Validating and solution-oriented&quot;, &quot;response_requirements&quot;: &quot;Propose options for repair or replacement.&quot;, &quot;user_requesting_to_talk_to_human&quot;: &quot;False&quot;, &quot;enough_information_in_context&quot;: &quot;True&quot;, &quot;response&quot;: &quot;...&quot; }</p><p>USER</p><p># Relevant Information ` ` ` [retrieved context] ` ` `</p><p>USER</p><p>[input user query here]</p><h3 id="analysis-and-optimizations" tabindex="-1">Analysis and optimizations <a class="header-anchor" href="#analysis-and-optimizations" aria-label="Permalink to &quot;Analysis and optimizations&quot;">​</a></h3><h4 id="part-1-looking-at-retrieval-prompts" tabindex="-1">Part 1: Looking at retrieval prompts <a class="header-anchor" href="#part-1-looking-at-retrieval-prompts" aria-label="Permalink to &quot;Part 1: Looking at retrieval prompts&quot;">​</a></h4><p>Looking at the architecture, the first thing that stands out is the <strong>consecutive GPT-4 calls</strong> - these hint at a potential inefficiency, and can often be replaced by a single call or parallel calls.</p><p><img src="https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-2.png" alt="Assistants object architecture diagram"></p><p>In this case, since the check for retrieval requires the contextualized query, let&#39;s <strong>combine them into a single prompt</strong> to <a href="#make-fewer-requests">make fewer requests</a>.</p><p><img src="https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-3.png" alt="Assistants object architecture diagram"></p><p>Combined query contextualization and retrieval check prompt</p><p><strong>What changed?</strong> Before, we had one prompt to re-write the query and one to determine whether this requires doing a retrieval lookup. Now, this combined prompt does both. Specifically, notice the updated instruction in the first line of the prompt, and the updated output JSON:</p><div class="language-jsx vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">jsx</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">{</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">  query</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;[contextualized query]&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">  retrieval</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;[true/false - whether retrieval is required]&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div><p>SYSTEM</p><p>Given the previous conversation, re-write the last user query so it contains all necessary context. Then, determine whether the full request requires doing a realtime lookup to respond to. Respond in the following form: { query:&quot;[contextualized query]&quot;, retrieval:&quot;[true/false - whether retrieval is required]&quot; } # Examples History: [{user: &quot;What is your return policy?&quot;},{assistant: &quot;...&quot;}] User Query: &quot;How long does it cover?&quot; Response: {query: &quot;How long does the return policy cover?&quot;, retrieval: &quot;true&quot;} History: [{user: &quot;How can I return this item after 30 days?&quot;},{assistant: &quot;...&quot;}] User Query: &quot;Thank you!&quot; Response: {query: &quot;Thank you!&quot;, retrieval: &quot;false&quot;} # Conversation [last 3 messages of conversation] # User Query [last user query]</p><p>USER</p><p>[JSON-formatted input conversation here]</p><p>Actually, adding context and determining whether to retrieve are very straightforward and well defined tasks, so we can likely use a <strong>smaller, fine-tuned model</strong> instead. Switching to GPT-3.5 will let us <a href="#process-tokens-faster">process tokens faster</a>.</p><p><img src="https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-4.png" alt="Assistants object architecture diagram"></p><h4 id="part-2-analyzing-the-assistant-prompt" tabindex="-1">Part 2: Analyzing the assistant prompt <a class="header-anchor" href="#part-2-analyzing-the-assistant-prompt" aria-label="Permalink to &quot;Part 2: Analyzing the assistant prompt&quot;">​</a></h4><p>Let&#39;s now direct our attention to the Assistant prompt. There seem to be many distinct steps happening as it fills the JSON fields – this could indicate an opportunity to <a href="#parallelize">parallelize</a>.</p><p><img src="https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-5.png" alt="Assistants object architecture diagram"></p><p>However, let&#39;s pretend we have run some tests and discovered that splitting the reasoning steps in the JSON produces worse responses, so we need to explore different solutions.</p><p><strong>Could we use a fine-tuned GPT-3.5 instead of GPT-4?</strong> Maybe – but in general, open-ended responses from assistants are best left to GPT-4 so it can better handle a greater range of cases. That being said, looking at the reasoning steps themselves, they may not all require GPT-4 level reasoning to produce. The well defined, limited scope nature makes them and <strong>good potential candidates for fine-tuning</strong>.</p><div class="language-jsx vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">jsx</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;message_is_conversation_continuation&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;True&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// &lt;-</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;number_of_messages_in_conversation_so_far&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// &lt;-</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;user_sentiment&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Aggravated&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// &lt;-</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;query_type&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Hardware Issue&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// &lt;-</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;response_tone&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Validating and solution-oriented&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// &lt;-</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;response_requirements&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Propose options for repair or replacement.&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// &lt;-</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;user_requesting_to_talk_to_human&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;False&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// &lt;-</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;enough_information_in_context&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;True&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// &lt;-</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;response&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;...&quot;</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"> // X -- benefits from GPT-4</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div><p>This opens up the possibility of a trade-off. Do we keep this as a <strong>single request entirely generated by GPT-4</strong>, or <strong>split it into two sequential requests</strong> and use GPT-3.5 for all but the final response? We have a case of conflicting principles: the first option lets us <a href="#make-fewer-requests">make fewer requests</a>, but the second may let us <a href="#1-process-tokens-faster">process tokens faster</a>.</p><p>As with many optimization tradeoffs, the answer will depend on the details. For example:</p><ul><li>The proportion of tokens in the <code>response</code> vs the other fields.</li><li>The average latency decrease from processing most fields faster.</li><li>The average latency <em>increase</em> from doing two requests instead of one.</li></ul><p>The conclusion will vary by case, and the best way to make the determiation is by testing this with production examples. In this case let&#39;s pretend the tests indicated it&#39;s favorable to split the prompt in two to <a href="#process-tokens-faster">process tokens faster</a>.</p><p><img src="https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-6.png" alt="Assistants object architecture diagram"></p><p><strong>Note:</strong> We&#39;ll be grouping <code>response</code> and <code>enough_information_in_context</code> together in the second prompt to avoid passing the retrieved context to both new prompts.</p><p>Assistants prompt - reasoning</p><p>This prompt will be passed to GPT-3.5 and can be fine-tuned on curated examples.</p><p><strong>What changed?</strong> The &quot;enough_information_in_context&quot; and &quot;response&quot; fields were removed, and the retrieval results are no longer loaded into this prompt.</p><p>SYSTEM</p><p>You are a helpful customer service bot. Based on the previous conversation, respond in a JSON to determine the required fields. # Example User: &quot;My freaking computer screen is cracked!&quot; Assistant Response: { &quot;message_is_conversation_continuation&quot;: &quot;True&quot;, &quot;number_of_messages_in_conversation_so_far&quot;: &quot;1&quot;, &quot;user_sentiment&quot;: &quot;Aggravated&quot;, &quot;query_type&quot;: &quot;Hardware Issue&quot;, &quot;response_tone&quot;: &quot;Validating and solution-oriented&quot;, &quot;response_requirements&quot;: &quot;Propose options for repair or replacement.&quot;, &quot;user_requesting_to_talk_to_human&quot;: &quot;False&quot;, }</p><p>Assistants prompt - response</p><p>This prompt will be processed by GPT-4 and will receive the reasoning steps determined in the prior prompt, as well as the results from retrieval.</p><p><strong>What changed?</strong> All steps were removed except for &quot;enough_information_in_context&quot; and &quot;response&quot;. Additionally, the JSON we were previously filling in as output will be passed in to this prompt.</p><p>SYSTEM</p><p>You are a helpful customer service bot. Use the retrieved context, as well as these pre-classified fields, to respond to the user&#39;s query. # Reasoning Fields ` ` ` [reasoning json determined in previous GPT-3.5 call] ` ` ` # Example User: &quot;My freaking computer screen is cracked!&quot; Assistant Response: { &quot;enough_information_in_context&quot;: &quot;True&quot;, &quot;response&quot;: &quot;...&quot; }</p><p>USER</p><p># Relevant Information ` ` ` [retrieved context] ` ` `</p><p>In fact, now that the reasoning prompt does not depend on the retrieved context we can <a href="#parallelize">parallelize</a> and fire it off at the same time as the retrieval prompts.</p><p><img src="https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-6b.png" alt="Assistants object architecture diagram"></p><h4 id="part-3-optimizing-the-structured-output" tabindex="-1">Part 3: Optimizing the structured output <a class="header-anchor" href="#part-3-optimizing-the-structured-output" aria-label="Permalink to &quot;Part 3: Optimizing the structured output&quot;">​</a></h4><p>Let&#39;s take another look at the reasoning prompt.</p><p><img src="https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-7b.png" alt="Assistants object architecture diagram"></p><p>Taking a closer look at the reasoning JSON you may notice the field names themselves are quite long.</p><div class="language-jsx vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">jsx</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;message_is_conversation_continuation&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;True&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// &lt;-</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;number_of_messages_in_conversation_so_far&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// &lt;-</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;user_sentiment&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Aggravated&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// &lt;-</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;query_type&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Hardware Issue&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// &lt;-</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;response_tone&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Validating and solution-oriented&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// &lt;-</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;response_requirements&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Propose options for repair or replacement.&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// &lt;-</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;user_requesting_to_talk_to_human&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;False&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// &lt;-</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div><p>By making them shorter and moving explanations to the comments we can <a href="#generate-fewer-tokens">generate fewer tokens</a>.</p><div class="language-jsx vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">jsx</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;cont&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;True&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// whether last message is a continuation</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;n_msg&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// number of messages in the continued conversation</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;tone_in&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Aggravated&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// sentiment of user query</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;type&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Hardware Issue&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// type of the user query</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;tone_out&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Validating and solution-oriented&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// desired tone for response</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;reqs&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Propose options for repair or replacement.&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// response requirements</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;human&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;False&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// whether user is expressing want to talk to human</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div><p><img src="https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-8b.png" alt="Assistants object architecture diagram"></p><p>This small change removed 19 output tokens. While with GPT-3.5 this may only result in a few millisecond improvement, with GPT-4 this could shave off up to a second.</p><p><img src="https://cdn.openai.com/API/docs/images/token-counts-latency-customer-service-large.png" alt="Assistants object architecture diagram"></p><p>You might imagine, however, how this can have quite a significant impact for larger model outputs.</p><p>We could go further and use single chatacters for the JSON fields, or put everything in an array, but this may start to hurt our response quality. The best way to know, once again, is through testing.</p><h4 id="example-wrap-up" tabindex="-1">Example wrap-up <a class="header-anchor" href="#example-wrap-up" aria-label="Permalink to &quot;Example wrap-up&quot;">​</a></h4><p>Let&#39;s review the optimizations we implemented for the customer service bot example:</p><p><img src="https://cdn.openai.com/API/docs/images/diagram-latency-customer-service-11b.png" alt="Assistants object architecture diagram"></p><ol><li><strong>Combined</strong> query contextualization and retrieval check steps to <a href="#make-fewer-requests">make fewer requests</a>.</li><li>For the new prompt, <strong>switched to a smaller, fine-tuned GPT-3.5</strong> to <a href="./process-tokens-faster">process tokens faster</a>.</li><li>Split the assistant prompt in two, <strong>switching to a smaller, fine-tuned GPT-3.5</strong> for the reasoning, again to <a href="#process-tokens-faster">process tokens faster</a>.</li><li><a href="#parallelize">Parallelized</a> the retrieval checks and the reasoning steps.</li><li><strong>Shortened reasoning field names</strong> and moved comments into the prompt, to <a href="#generate-fewer-tokens">generate fewer tokens</a>.</li></ol></div></div></main><footer class="VPDocFooter" data-v-e6f2a212 data-v-1bcd8184><!--[--><!--]--><div class="edit-info" data-v-1bcd8184><!----><div class="last-updated" data-v-1bcd8184><p class="VPLastUpdated" data-v-1bcd8184 data-v-1bb0c8a8>Last updated: <time datetime="2025-05-24T07:24:21.000Z" data-v-1bb0c8a8></time></p></div></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-1bcd8184><span class="visually-hidden" id="doc-footer-aria-label" data-v-1bcd8184>Pager</span><div class="pager" data-v-1bcd8184><a class="VPLink link pager-link prev" href="/docs/openai/guides/model-selection" data-v-1bcd8184><!--[--><span class="desc" data-v-1bcd8184>Previous page</span><span class="title" data-v-1bcd8184>Model selection</span><!--]--></a></div><div class="pager" data-v-1bcd8184><a class="VPLink link pager-link next" href="/docs/openai/guides/accuracy-optimization" data-v-1bcd8184><!--[--><span class="desc" data-v-1bcd8184>Next page</span><span class="title" data-v-1bcd8184>Accuracy optimization</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-d8b57b2d data-v-566314d4><div class="container" data-v-566314d4><p class="message" data-v-566314d4>build with <a href="https://github.com/vuejs/vitepress">vitepress</a></p><p class="copyright" data-v-566314d4>Copyright © 2023-present by NOTHING</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"about_team.md\":\"KUNdrXdK\",\"agent_index.md\":\"W2MR-bfb\",\"blog_create-a-chatbot-step-by-step.md\":\"B-10A9lz\",\"blog_hands-on-deepseek-create-a-autoclicker.md\":\"DuvPIrj1\",\"blog_in-depth-llm-output.md\":\"CHVr12XE\",\"blog_in-depth-prompt-template.md\":\"C92Ji9QK\",\"blog_index.md\":\"BELbpHOe\",\"blog_introduction-template.md\":\"DjZ0MuNn\",\"blog_langchain-core-introduction.md\":\"C90b70JR\",\"blog_langchain-introduction.md\":\"CMyoqhSy\",\"blog_news-deepseek-r1-0528-improvement.md\":\"D4N_axJ_\",\"blog_useful-tools-share.md\":\"D0Sjlb4H\",\"blog_what-is-few-shot-prompt-template.md\":\"-wPRnp55\",\"blog_what-is-function-calling.md\":\"BtxyPLzz\",\"blog_what-is-prompt-template.md\":\"BKY2XWlf\",\"docs_anthropic_index.md\":\"CIme9r0w\",\"docs_anthropic_prompt_generator_atrhopic.md\":\"B-wKwJqz\",\"docs_examples_vitepress_api-examples.md\":\"BuWK0pJ7\",\"docs_examples_vitepress_emoji.md\":\"zgHGBtLc\",\"docs_examples_vitepress_index.md\":\"DXXZVeAO\",\"docs_examples_vitepress_markdown-examples.md\":\"Dke--KkA\",\"docs_openai_actions_actions-library.md\":\"PVZYe8Vx\",\"docs_openai_actions_authentication.md\":\"Cd88sp7W\",\"docs_openai_actions_data-retrieval.md\":\"BNfPzXxy\",\"docs_openai_actions_getting-started.md\":\"C62A4Kq1\",\"docs_openai_actions_introduction.md\":\"B7c4L_zW\",\"docs_openai_actions_production.md\":\"BPZ2VQa_\",\"docs_openai_actions_sending-files.md\":\"BFcgnUnH\",\"docs_openai_assistants_deep-dive.md\":\"Jbznq2ZG\",\"docs_openai_assistants_overview.md\":\"B-t7I8dc\",\"docs_openai_assistants_quickstart.md\":\"Dooroex_\",\"docs_openai_assistants_tools_code-interpreter.md\":\"wutDZ2J8\",\"docs_openai_assistants_tools_file-search.md\":\"Bza1v9-m\",\"docs_openai_assistants_tools_function-calling.md\":\"Jj1HE6yE\",\"docs_openai_assistants_whats-new.md\":\"CEfbpr_b\",\"docs_openai_deprecations.md\":\"B4gN8sAv\",\"docs_openai_guides_accuracy-optimization.md\":\"BmBw872B\",\"docs_openai_guides_advanced-usage.md\":\"DW-XXSh2\",\"docs_openai_guides_agents.md\":\"CU5-4cPg\",\"docs_openai_guides_audio.md\":\"B_gKpMWE\",\"docs_openai_guides_batch.md\":\"BdiicXiw\",\"docs_openai_guides_codex.md\":\"BDV2ORlY\",\"docs_openai_guides_conversation-state-chat.md\":\"Cr8DkJM4\",\"docs_openai_guides_conversation-state.md\":\"CeUzgrK_\",\"docs_openai_guides_direct-preference-optimization.md\":\"DD1DSXP7\",\"docs_openai_guides_distillation.md\":\"BTYkbr2j\",\"docs_openai_guides_embeddings.md\":\"Bndo8_ao\",\"docs_openai_guides_evals-design.md\":\"Ccc1YLgk\",\"docs_openai_guides_evals.md\":\"CbMCJhVr\",\"docs_openai_guides_fine-tuning-best-practices.md\":\"BBxp75Gn\",\"docs_openai_guides_fine-tuning.md\":\"BKtZAXF4\",\"docs_openai_guides_flex-processing.md\":\"DlzXL9t5\",\"docs_openai_guides_function-calling-chat.md\":\"CRoflGRM\",\"docs_openai_guides_function-calling.md\":\"DRNpeiaa\",\"docs_openai_guides_graders.md\":\"Bp1hJz3K\",\"docs_openai_guides_image-generation.md\":\"BhHNSTgt\",\"docs_openai_guides_images-vision-chat.md\":\"CwYdx2lD\",\"docs_openai_guides_images-vision.md\":\"DR0NvDFs\",\"docs_openai_guides_index.md\":\"CzQ0qR0X\",\"docs_openai_guides_latency-optimization.md\":\"CGO_h0Il\",\"docs_openai_guides_model-optimization.md\":\"B9GhfMWE\",\"docs_openai_guides_model-selection.md\":\"B7e5n7rQ\",\"docs_openai_guides_moderation.md\":\"BOsjZVLW\",\"docs_openai_guides_pdf-files-chat.md\":\"CV2tvxOS\",\"docs_openai_guides_pdf-files.md\":\"Blln9hSd\",\"docs_openai_guides_predicted-outputs.md\":\"FrixN8zc\",\"docs_openai_guides_production-best-practices.md\":\"CSr-im75\",\"docs_openai_guides_prompt-caching.md\":\"yV7kgHTr\",\"docs_openai_guides_prompt-generation.md\":\"U_4YnnPn\",\"docs_openai_guides_rate-limits.md\":\"CDlhWLoi\",\"docs_openai_guides_realtime-conversations.md\":\"aXn5T03B\",\"docs_openai_guides_realtime-transcription.md\":\"DJanc7sU\",\"docs_openai_guides_realtime-vad.md\":\"Bx48zozG\",\"docs_openai_guides_realtime.md\":\"xjiBZIa9\",\"docs_openai_guides_reasoning-best-practices.md\":\"CbfiDfK1\",\"docs_openai_guides_reasoning-chat.md\":\"D-f-5z2w\",\"docs_openai_guides_reasoning.md\":\"D08FXiF0\",\"docs_openai_guides_reinforcement-fine-tuning.md\":\"KN9OCvdY\",\"docs_openai_guides_responses-vs-chat-completions.md\":\"DAl5ZYQy\",\"docs_openai_guides_retrieval.md\":\"DItao1o7\",\"docs_openai_guides_rft-use-cases.md\":\"D3JbnD6-\",\"docs_openai_guides_safety-best-practices.md\":\"BObafObt\",\"docs_openai_guides_speech-to-text.md\":\"BIj2pccE\",\"docs_openai_guides_streaming-responses-chat.md\":\"D1UT1ehD\",\"docs_openai_guides_streaming-responses.md\":\"i8CNXIRy\",\"docs_openai_guides_structured-outputs-chat.md\":\"2eCw5pS8\",\"docs_openai_guides_structured-outputs.md\":\"Ao52JLDs\",\"docs_openai_guides_supervised-fine-tuning.md\":\"BlF6S4cW\",\"docs_openai_guides_text-chat.md\":\"CLsBpuv_\",\"docs_openai_guides_text-to-speech.md\":\"BYsD-pcf\",\"docs_openai_guides_text.md\":\"BKqobsP7\",\"docs_openai_guides_tools-computer-use.md\":\"BC65AAN5\",\"docs_openai_guides_tools-file-search.md\":\"CChbn8EZ\",\"docs_openai_guides_tools-local-shell.md\":\"1WGr0pVi\",\"docs_openai_guides_tools-web-search.md\":\"CEvrFZKa\",\"docs_openai_guides_tools.md\":\"DOPnaI4y\",\"docs_openai_guides_vision-fine-tuning.md\":\"B7J3kG3-\",\"docs_openai_guides_voice-agents-chained.md\":\"DqbJD8Ez\",\"docs_openai_guides_voice-agents.md\":\"BCUcgMm6\",\"docs_openai_guides_your-data.md\":\"DEBnZEqA\",\"docs_openai_libraries.md\":\"DZbLdlkk\",\"docs_openai_quickstart-chat.md\":\"CB0eml7j\",\"docs_openai_quickstart.md\":\"3wvFLD-K\",\"eval_index.md\":\"BRD5fqNa\",\"index.md\":\"DVoAsohY\",\"llm_index.md\":\"C13_21CY\",\"projects_index.md\":\"B83Hyjx3\",\"readme.md\":\"BzXVbSQr\",\"tutorial_index.md\":\"BZTzoi-0\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"DaQianAI\",\"description\":\"The ultimate AI\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"logo\":\"/logo.png\",\"search\":{\"provider\":\"local\"},\"nav\":[{\"text\":\"Home\",\"link\":\"/\"},{\"text\":\"APP\",\"items\":[{\"text\":\"deepwiki\",\"link\":\"https://deepwiki.com\"}]},{\"text\":\"LLM\",\"link\":\"/llm\"},{\"text\":\"Agent\",\"link\":\"/agent\"},{\"text\":\"Eval\",\"link\":\"/eval\"},{\"text\":\"Prompt\",\"link\":\"/tutorial\"},{\"text\":\"MCP\",\"items\":[{\"text\":\"Protocol\",\"link\":\"https://modelcontextprotocol.io/introduction\"},{\"text\":\"Python-SDK\",\"link\":\"https://deepwiki.com/modelcontextprotocol/python-sdk/\"}]},{\"text\":\"Tutorial\",\"link\":\"/tutorial\"},{\"text\":\"Docs\",\"items\":[{\"text\":\"OpenAI\",\"link\":\"/docs/openai/guides\"},{\"text\":\"Anthropic\",\"link\":\"/docs/anthropic\"},{\"text\":\"Vitepress\",\"link\":\"/docs/examples/vitepress\"}]},{\"text\":\"Projects\",\"link\":\"/projects\"},{\"text\":\"About\",\"items\":[{\"text\":\"Team\",\"link\":\"/about/team\"},{\"text\":\"blog\",\"link\":\"/blog/\"}]}],\"sidebar\":{\"/docs/examples/vitepress\":[{\"text\":\"Examples\",\"items\":[{\"text\":\"Markdown Examples\",\"link\":\"/docs/examples/vitepress/markdown-examples\"},{\"text\":\"Runtime API Examples\",\"link\":\"/docs/examples/vitepress/api-examples\"},{\"text\":\"Emoji\",\"link\":\"/docs/examples/vitepress/emoji\"}]}],\"/docs/openai\":[{\"text\":\"GET STARTED\",\"base\":\"/docs/openai\",\"items\":[{\"text\":\"Quickstart\",\"link\":\"/quickstart\"},{\"text\":\"Quickstart(chat)\",\"link\":\"/quickstart-chat\"},{\"text\":\"Libraries\",\"link\":\"/libraries\"}]},{\"text\":\"CORE CONCEPTS\",\"base\":\"/docs/openai\",\"items\":[{\"text\":\"Text and prompting\",\"link\":\"/guides/text\"},{\"text\":\"Text and prompting(chat)\",\"link\":\"/guides/text-chat\"},{\"text\":\"Images and vision\",\"link\":\"/guides/images-vision\"},{\"text\":\"Images and vision(chat)\",\"link\":\"/guides/images-vision-chat\"},{\"text\":\"Audio and speech\",\"link\":\"/guides/audio\"},{\"text\":\"Structured Outputs\",\"link\":\"/guides/structured-outputs\"},{\"text\":\"Structured Outputs(chat)\",\"link\":\"/guides/structured-outputs-chat\"},{\"text\":\"Function calling\",\"link\":\"/guides/function-calling\"},{\"text\":\"Function calling(chat)\",\"link\":\"/guides/function-calling-chat\"},{\"text\":\"Conversation state\",\"link\":\"/guides/conversation-state\"},{\"text\":\"Conversation state(chat)\",\"link\":\"/guides/conversation-state-chat\"},{\"text\":\"Streaming\",\"link\":\"/guides/streaming-responses\"},{\"text\":\"Streaming(chat)\",\"link\":\"/guides/streaming-responses-chat\"},{\"text\":\"File inputs\",\"link\":\"/guides/pdf-files\"},{\"text\":\"File inputs(chat)\",\"link\":\"/guides/pdf-files-chat\"},{\"text\":\"Reasoning\",\"link\":\"/guides/reasoning\"},{\"text\":\"Reasoning(chat)\",\"link\":\"/guides/reasoning-chat\"}]},{\"text\":\"BUILT-IN TOOLS\",\"base\":\"/docs/openai\",\"items\":[{\"text\":\"Using built-in tools\",\"link\":\"/guides/tools\"},{\"text\":\"Web search\",\"link\":\"/guides/tools-web-search\"},{\"text\":\"File search\",\"link\":\"/guides/tools-file-search\"},{\"text\":\"Computer use\",\"link\":\"/guides/tools-computer-use\"}]},{\"text\":\"AGENTS\",\"items\":[{\"text\":\"Building agents\",\"link\":\"/docs/openai/guides/agents\"},{\"text\":\"Voice agents\",\"link\":\"/docs/openai/guides/voice-agents\"},{\"text\":\"Voice agents(chained)\",\"link\":\"/docs/openai/guides/voice-agents-chained\"},{\"text\":\"Agents SDK\",\"link\":\"https://openai.github.io/openai-agents-python/\"}]},{\"text\":\"REALTIME API\",\"base\":\"/docs/openai\",\"items\":[{\"text\":\"Using the Realtime API\",\"link\":\"/guides/realtime\"},{\"text\":\"Realtime conversations\",\"link\":\"/guides/realtime-conversations\"},{\"text\":\"Realtime transcription\",\"link\":\"/guides/realtime-transcription\"},{\"text\":\"Voice activity detection\",\"link\":\"/guides/realtime-vad\"}]},{\"text\":\"MODEL OPTIMIZATION\",\"base\":\"/docs/openai\",\"items\":[{\"text\":\"Overview\",\"link\":\"/guides/model-optimization\"},{\"text\":\"Evals\",\"link\":\"/guides/evals\"},{\"text\":\"Fine-tuning\",\"link\":\"/guides/fine-tuning\"},{\"text\":\"Supervised fine-tuning\",\"link\":\"/guides/supervised-fine-tuning\"},{\"text\":\"Vision fine-tuning\",\"link\":\"/guides/vision-fine-tuning\"},{\"text\":\"Direct preference optimization\",\"link\":\"/guides/direct-preference-optimization\"},{\"text\":\"Reinforcement fine-tuning\",\"link\":\"/guides/reinforcement-fine-tuning\"},{\"text\":\"Graders\",\"link\":\"/guides/graders\"},{\"text\":\"Distillation\",\"link\":\"/guides/distillation\"}]},{\"text\":\"SPECIALIZED MODELS\",\"base\":\"/docs/openai\",\"items\":[{\"text\":\"Image generation\",\"link\":\"/guides/image-generation\"},{\"text\":\"Text to speech\",\"link\":\"/guides/text-to-speech\"},{\"text\":\"Speech to text\",\"link\":\"/guides/speech-to-text\"},{\"text\":\"Embeddings\",\"link\":\"/guides/embeddings\"},{\"text\":\"Moderation\",\"link\":\"/guides/moderation\"}]},{\"text\":\"OPENAI PLATFORM\",\"base\":\"/docs/openai\",\"items\":[{\"text\":\"Retrieval\",\"link\":\"/guides/retrieval\"},{\"text\":\"Batch\",\"link\":\"/guides/batch\"},{\"text\":\"Prompt generation\",\"link\":\"/guides/prompt-generation\"}]},{\"text\":\"CODEX\",\"items\":[{\"text\":\"Codex\",\"link\":\"/docs/openai/guides/codex\"},{\"text\":\"Local shell tool\",\"link\":\"/docs/openai/guides/tools-local-shell\"},{\"text\":\"Codex CLI\",\"link\":\"https://github.com/openai/codex\"}]},{\"text\":\"BEST PRACTICES\",\"base\":\"/docs/openai\",\"items\":[{\"text\":\"Production best practices\",\"link\":\"/guides/production-best-practices\"},{\"text\":\"Safety best practices\",\"link\":\"/guides/safety-best-practices\"},{\"text\":\"Prompt Caching\",\"link\":\"/guides/prompt-caching\"},{\"text\":\"Predicted Outputs\",\"link\":\"/guides/predicted-outputs\"},{\"text\":\"Reasoning best practices\",\"link\":\"/guides/reasoning-best-practices\"},{\"text\":\"Evals design\",\"link\":\"/guides/evals-design\"},{\"text\":\"Fine-tuning best practices\",\"link\":\"/guides/fine-tuning-best-practices\"},{\"text\":\"Reinforcement fine-tuning use cases\",\"link\":\"/guides/rft-use-cases\"},{\"text\":\"Model selection\",\"link\":\"/guides/model-selection\"},{\"text\":\"Latency optimization\",\"link\":\"/guides/latency-optimization\"},{\"text\":\"Accuracy optimization\",\"link\":\"/guides/accuracy-optimization\"},{\"text\":\"Advanced usage\",\"link\":\"/guides/advanced-usage\"},{\"text\":\"Responses vs. Chat Completions\",\"link\":\"/guides/responses-vs-chat-completions\"},{\"text\":\"Flex processing\",\"link\":\"/guides/flex-processing\"}]},{\"text\":\"ASSISTANTS API\",\"base\":\"/docs/openai\",\"items\":[{\"text\":\"Overview\",\"link\":\"/assistants/overview\"},{\"text\":\"Quickstart\",\"link\":\"/assistants/quickstart\"},{\"text\":\"Deep dive\",\"link\":\"/assistants/deep-dive\"},{\"text\":\"Tools\",\"collapsed\":true,\"items\":[{\"text\":\"File Search\",\"link\":\"/assistants/tools/file-search\"},{\"text\":\"Code interpreter\",\"link\":\"/assistants/tools/code-interpreter\"},{\"text\":\"Function calling\",\"link\":\"/assistants/tools/function-calling\"}]},{\"text\":\"What's new\",\"link\":\"/assistants/whats-new\"}]},{\"text\":\"RESOURCES\",\"items\":[{\"text\":\"Terms and policies\",\"link\":\"https://openai.com/policies/\"},{\"text\":\"Changelog\",\"link\":\"https://platform.openai.com/docs/changelog\"},{\"text\":\"Your data\",\"link\":\"/docs/openai/guides/your-data\"},{\"text\":\"Rate limits\",\"link\":\"/docs/openai/guides/rate-limits\"},{\"text\":\"Deprecations\",\"link\":\"/docs/openai/deprecations\"},{\"text\":\"ChatGPT Actions\",\"base\":\"/docs/openai\",\"collapsed\":true,\"items\":[{\"text\":\"Introduction\",\"link\":\"/actions/introduction\"},{\"text\":\"Getting started\",\"link\":\"/actions/getting-started\"},{\"text\":\"Actions library\",\"link\":\"/actions/actions-library\"},{\"text\":\"Authentication\",\"link\":\"/actions/authentication\"},{\"text\":\"Production\",\"link\":\"/actions/production\"},{\"text\":\"Data retrieval\",\"link\":\"/actions/data-retrieval\"},{\"text\":\"Sending files\",\"link\":\"/actions/sending-files\"}]}]},{\"text\":\"\",\"items\":[{\"text\":\"Cookbook\",\"link\":\"https://cookbook.openai.com/\"},{\"text\":\"Forum\",\"link\":\"https://community.openai.com/categories\"},{\"text\":\"Help\",\"link\":\"\"}]}]},\"footer\":{\"message\":\"build with <a href=\\\"https://github.com/vuejs/vitepress\\\">vitepress</a>\",\"copyright\":\"Copyright © 2023-present by NOTHING\"},\"socialLinks\":[{\"icon\":\"wechat\",\"link\":\"/about/team\"}]},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":true}");</script>
    
  </body>
</html>